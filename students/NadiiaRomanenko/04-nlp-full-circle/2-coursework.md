### 1. Які метрики для оцінки якості ви будете використовувати?
Задача пошуку подібних заголовків зводиться до бінарної класифікації: надати алгоритму два речення, і отримати оцінку їх подібності (score або boolean).

Якщо оцінка подібності інтревальна (наприклад, приймає значення від 0 до 1), рішення, чи вважати дві новини подібними приймається на основі порогового значення показника, яке буде балансом між Precision та False Positive Rate. Наприклад, якщо вважати подібними лише новини, для яких алгоритм дає ймовірність подібності >80%, майже не буде помилок першого роду, але багато True Positive залишаться пропущеними. Якщо цей поріг знизити, суттєво зросте частка False Positive. За ROC-кривою можна буде знайти оптимальне значення.

Для оцінки роботи моделі добре підходять класичні метрики оцінки якості бінарної класифікації:
* F1, precision & recall для оцінки часток пропущених подібних та хибно подібних новин
* Confusion matrix для пошуку "перекосу" в бік False Positive чи False Negative

### 2. Як ви бачите побудову бейзлайну?
Примітивне базове рішення для пошуку подібності - редакторська відстань між текстами, максимальне значення з двох часткових порівнянь, без врахування порядку слів:
Приклад з [fuzzy wuzzy](https://github.com/seatgeek/fuzzywuzzy):
```python
    >>> fuzz.partial_ratio("this is a test", "this is a test!")
        100
    >>> fuzz.ratio("fuzzy wuzzy was a bear", "wuzzy fuzzy was a bear")
        91
    >>> fuzz.token_sort_ratio("fuzzy wuzzy was a bear", "wuzzy fuzzy was a bear")
        100
```

План "В" бейслайну - використати метрику BLEU/ROUGE для визначення подібності.
План "С" - косинусна подібність tf-idf векторів документів.


### 3. Які готові рішення (включно зі state-of-the-art) вже існують та на яких підходах? (Якщо немає рішень для вашої задачі, дослідіть рішення для схожих задач.)
Єдиного SOTA-підходу для вирішення задачі не існує. Переможцям [SemEval2016 task 1 ](http://www.aclweb.org/anthology/S16-1089) вдалось досягти акуратності 0.8 - 0.9, на [Kaggle Quora pairs](https://www.kaggle.com/c/quora-question-pairs/leaderboard) log loss у команди переможців склав 0.11.

Спільним у всіх підходах є використання кількох методів векторизації. Кращі результати дали вектори слів (хоч doc2vec часто виявлялись неефективними), [word alignment](http://www.aclweb.org/anthology/N13-1073). Використання мішка слів або TF-IDF обмежені тим, що не враховують синонімічні зв'язки.

Найвищої акуратності в класифікації на подібні/неподібні пари документів досягали глибоким навчанням (LSTM та інші моделі, про які я майже нічого не знаю).
Отже наявні способи вирішення задачі дозволять суттєво покращити результат, порівняно з базовими рішеннями.
