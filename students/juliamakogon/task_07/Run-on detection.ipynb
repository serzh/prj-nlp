{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data: https://drive.google.com/drive/u/1/folders/1R64LrYygM5-97wNSGyx86tMy5YeswxJ7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 2000\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_json('data\\\\runon_train_swift.json', orient='values', compression=None, typ='series')\n",
    "test = pd.read_json('data\\\\runon_test_swift.json', orient='values', compression=None, typ='series')\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[did, False], [you, False], [see, False], [th...\n",
       "1    [[resurrect, False], [the, False], [passed, Fa...\n",
       "2    [[Hey, False], [guys, True], [thank, False], [...\n",
       "3    [[It, False], ['s, False], [alright, False], [...\n",
       "4    [[when, False], [my, False], [oldest, False], ...\n",
       "5    [[Yes, False], [,, False], [in, False], [2007,...\n",
       "6    [[http://www.hulu.com/tiger-and-bunny, False],...\n",
       "7    [[i, False], [struggled, False], [,, False], [...\n",
       "8    [[um, False], [..., False], [HOLY, False], [SH...\n",
       "9    [[i, False], [ca, False], [nâ€™t, False], [belie...\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class FillingNans(object):\n",
    "    '''\n",
    "    Custom function for assembling into the pipeline object \n",
    "    '''\n",
    "    def transform(self, X):\n",
    "        XX = X.copy()\n",
    "        XX[XX==np.nan] = 0\n",
    "        return XX\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "def default_pipe_params():\n",
    "    vec = DictVectorizer()   \n",
    "    clf = LogisticRegression(solver='sag', max_iter=300, random_state=42, multi_class='multinomial')\n",
    "    return [('vectorize', vec),\n",
    "            ('fill_nans', FillingNans()),\n",
    "    #                                  ('variance', variance),\n",
    "            ('clf', clf)]   \n",
    "     \n",
    "    \n",
    "def make_classifier(train, feature_analyzer, nlp, pipe_default = None):\n",
    "    Fx, y = feature_analyzer(train, nlp)\n",
    "    if pipe_default == None:\n",
    "        pipe = Pipeline(default_pipe_params())\n",
    "    else:\n",
    "        pipe = pipe_default\n",
    "    pipe.fit(Fx, y)\n",
    "    return pipe\n",
    "\n",
    "def classify(test, pipe, feature_analyzer, nlp):\n",
    "    Fx, y = feature_analyzer(test, nlp)\n",
    "    predicted = pipe.predict(Fx)\n",
    "    print(classification_report(y, predicted, pipe.named_steps['clf'].classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Baseline - is the word titlecase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      1.00      0.97     82412\n",
      "        1.0       0.38      0.02      0.04      5391\n",
      "\n",
      "avg / total       0.91      0.94      0.91     87803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_baseline(data, nlp):\n",
    "    dim = 0\n",
    "    for x in data:\n",
    "        dim += len(x)\n",
    "    features = np.ndarray(dim, dtype='object')\n",
    "    y = np.zeros(dim)\n",
    "    i = 0\n",
    "    for sent in data:\n",
    "        for j in range(len(sent)):\n",
    "            y[i] = sent[j][1]\n",
    "            features[i] = {}\n",
    "            features[i]['word'] = sent[j][0]\n",
    "            features[i]['word_lower'] = sent[j][0].lower()\n",
    "            i += 1\n",
    "    return features, y  \n",
    "\n",
    "pipe = make_classifier(train, prepare_baseline, nlp)\n",
    "classify(test, pipe, prepare_baseline, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With spacy features on uningrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['did', False], ['you', False], ['see', False], ['that', True], ['See', False], ['what', False], ['he', False], ['did', False], ['there', True], ['He', False], ['acknowledged', False], ['that', False], ['I', False], ['am', False], ['his', False], ['mom', False], ['and', False], ['that', False], ['I', False], ['am', False], ['supposed', False], ['to', False], ['love', False], ['him', False], ['just', False], ['like', False], ['every', False], ['other', False], ['mom', False], ['in', False], ['the', False], ['world', True]]\n",
      "(32,)\n",
      "[{'word': 'did', 'word_lower': 'did', 'word_lemma': 'do', 'pos': 'VERB'}\n",
      " {'word': 'you', 'word_lower': 'you', 'word_lemma': '-PRON-', 'pos': 'PRON'}\n",
      " {'word': 'see', 'word_lower': 'see', 'word_lemma': 'see', 'pos': 'VERB'}\n",
      " {'word': 'that', 'word_lower': 'that', 'word_lemma': 'that', 'pos': 'DET'}\n",
      " {'word': 'See', 'word_lower': 'see', 'word_lemma': 'see', 'pos': 'VERB'}\n",
      " {'word': 'what', 'word_lower': 'what', 'word_lemma': 'what', 'pos': 'NOUN'}\n",
      " {'word': 'he', 'word_lower': 'he', 'word_lemma': '-PRON-', 'pos': 'PRON'}\n",
      " {'word': 'did', 'word_lower': 'did', 'word_lemma': 'do', 'pos': 'VERB'}\n",
      " {'word': 'there', 'word_lower': 'there', 'word_lemma': 'there', 'pos': 'ADV'}\n",
      " {'word': 'He', 'word_lower': 'he', 'word_lemma': '-PRON-', 'pos': 'PRON'}\n",
      " {'word': 'acknowledged', 'word_lower': 'acknowledged', 'word_lemma': 'acknowledge', 'pos': 'VERB'}\n",
      " {'word': 'that', 'word_lower': 'that', 'word_lemma': 'that', 'pos': 'ADP'}\n",
      " {'word': 'I', 'word_lower': 'i', 'word_lemma': '-PRON-', 'pos': 'PRON'}\n",
      " {'word': 'am', 'word_lower': 'am', 'word_lemma': 'be', 'pos': 'VERB'}\n",
      " {'word': 'his', 'word_lower': 'his', 'word_lemma': '-PRON-', 'pos': 'ADJ'}\n",
      " {'word': 'mom', 'word_lower': 'mom', 'word_lemma': 'mom', 'pos': 'NOUN'}\n",
      " {'word': 'and', 'word_lower': 'and', 'word_lemma': 'and', 'pos': 'CCONJ'}\n",
      " {'word': 'that', 'word_lower': 'that', 'word_lemma': 'that', 'pos': 'ADP'}\n",
      " {'word': 'I', 'word_lower': 'i', 'word_lemma': '-PRON-', 'pos': 'PRON'}\n",
      " {'word': 'am', 'word_lower': 'am', 'word_lemma': 'be', 'pos': 'VERB'}\n",
      " {'word': 'supposed', 'word_lower': 'supposed', 'word_lemma': 'suppose', 'pos': 'VERB'}\n",
      " {'word': 'to', 'word_lower': 'to', 'word_lemma': 'to', 'pos': 'PART'}\n",
      " {'word': 'love', 'word_lower': 'love', 'word_lemma': 'love', 'pos': 'VERB'}\n",
      " {'word': 'him', 'word_lower': 'him', 'word_lemma': '-PRON-', 'pos': 'PRON'}\n",
      " {'word': 'just', 'word_lower': 'just', 'word_lemma': 'just', 'pos': 'ADV'}\n",
      " {'word': 'like', 'word_lower': 'like', 'word_lemma': 'like', 'pos': 'ADP'}\n",
      " {'word': 'every', 'word_lower': 'every', 'word_lemma': 'every', 'pos': 'DET'}\n",
      " {'word': 'other', 'word_lower': 'other', 'word_lemma': 'other', 'pos': 'ADJ'}\n",
      " {'word': 'mom', 'word_lower': 'mom', 'word_lemma': 'mom', 'pos': 'NOUN'}\n",
      " {'word': 'in', 'word_lower': 'in', 'word_lemma': 'in', 'pos': 'ADP'}\n",
      " {'word': 'the', 'word_lower': 'the', 'word_lemma': 'the', 'pos': 'DET'}\n",
      " {'word': 'world', 'word_lower': 'world', 'word_lemma': 'world', 'pos': 'NOUN'}]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      1.00      0.97     82412\n",
      "        1.0       0.30      0.03      0.06      5391\n",
      "\n",
      "avg / total       0.90      0.94      0.91     87803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_features_pos(data, nlp):\n",
    "    dim = 0\n",
    "    for x in data:\n",
    "        dim += len(x)\n",
    "    features = np.ndarray(dim, dtype='object')\n",
    "    y = np.zeros(dim)\n",
    "    i = 0\n",
    "    for sent in data:\n",
    "        sent_tokens = []\n",
    "        for x in sent:\n",
    "            sent_tokens.append(x[0])\n",
    "        doc = Doc(nlp.vocab, words=sent_tokens)\n",
    "        nlp.tagger(doc)\n",
    "        for j in range(len(sent)):\n",
    "            y[i] = sent[j][1]\n",
    "            features[i] = {}\n",
    "            features[i]['word'] = doc[j].text\n",
    "            features[i]['word_lower'] = doc[j].lower_\n",
    "            features[i]['word_lemma'] = doc[j].lemma_\n",
    "#             features[i]['word_is_punct'] = doc[j].is_punct\n",
    "#             features[i]['word_shape'] = doc[j].shape_\n",
    "            features[i]['pos'] = doc[j].pos_\n",
    "            i += 1\n",
    "    return features, y\n",
    "\n",
    "def show_features(train, nlp, analyzer):\n",
    "    for x in train[:1]:\n",
    "        print(x)\n",
    "    xx, yy = analyzer(train[:1], nlp)\n",
    "    print(xx.shape)\n",
    "    print(xx)\n",
    "    print(yy)    \n",
    "    \n",
    "show_features(train, nlp, prepare_features_pos)\n",
    "\n",
    "\n",
    "pipe = make_classifier(train, prepare_features_pos, nlp)\n",
    "classify(test, pipe, prepare_features_pos, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With spacy features and 3-grams; without dependency parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['did', False], ['you', False], ['see', False], ['that', True], ['See', False], ['what', False], ['he', False], ['did', False], ['there', True], ['He', False], ['acknowledged', False], ['that', False], ['I', False], ['am', False], ['his', False], ['mom', False], ['and', False], ['that', False], ['I', False], ['am', False], ['supposed', False], ['to', False], ['love', False], ['him', False], ['just', False], ['like', False], ['every', False], ['other', False], ['mom', False], ['in', False], ['the', False], ['world', True]]\n",
      "(32,)\n",
      "[{'word': 'did', 'word_lower': 'did', 'word_lemma': 'do', 'word_shape': 'xxx', 'pos': 'VERB', 'word_1': 'you', 'word_is_title_1': False, 'word_lemma_1': '-PRON-', 'word_lower_1': 'you', 'pos_1': 'PRON', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': '<S>', 'word_lower_-1': '<S>'}\n",
      " {'word': 'you', 'word_lower': 'you', 'word_lemma': '-PRON-', 'word_shape': 'xxx', 'pos': 'PRON', 'word_1': 'see', 'word_is_title_1': False, 'word_lemma_1': 'see', 'word_lower_1': 'see', 'pos_1': 'VERB', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'did', 'word_is_title_-1': False, 'word_lemma_-1': 'do', 'word_lower_-1': 'did', 'pos_-1': 'VERB', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False}\n",
      " {'word': 'see', 'word_lower': 'see', 'word_lemma': 'see', 'word_shape': 'xxx', 'pos': 'VERB', 'word_1': 'that', 'word_is_title_1': False, 'word_lemma_1': 'that', 'word_lower_1': 'that', 'pos_1': 'DET', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'you', 'word_is_title_-1': False, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'you', 'pos_-1': 'PRON', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False}\n",
      " {'word': 'that', 'word_lower': 'that', 'word_lemma': 'that', 'word_shape': 'xxxx', 'pos': 'DET', 'word_1': 'See', 'word_is_title_1': True, 'word_lemma_1': 'see', 'word_lower_1': 'see', 'pos_1': 'VERB', 'word_shape_1': 'Xxx', 'word_is_punct_1': False, 'word_-1': 'see', 'word_is_title_-1': False, 'word_lemma_-1': 'see', 'word_lower_-1': 'see', 'pos_-1': 'VERB', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False}\n",
      " {'word': 'See', 'word_lower': 'see', 'word_lemma': 'see', 'word_shape': 'Xxx', 'pos': 'VERB', 'word_1': 'what', 'word_is_title_1': False, 'word_lemma_1': 'what', 'word_lower_1': 'what', 'pos_1': 'NOUN', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'that', 'word_is_title_-1': False, 'word_lemma_-1': 'that', 'word_lower_-1': 'that', 'pos_-1': 'DET', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'what', 'word_lower': 'what', 'word_lemma': 'what', 'word_shape': 'xxxx', 'pos': 'NOUN', 'word_1': 'he', 'word_is_title_1': False, 'word_lemma_1': '-PRON-', 'word_lower_1': 'he', 'pos_1': 'PRON', 'word_shape_1': 'xx', 'word_is_punct_1': False, 'word_-1': 'See', 'word_is_title_-1': True, 'word_lemma_-1': 'see', 'word_lower_-1': 'see', 'pos_-1': 'VERB', 'word_shape_-1': 'Xxx', 'word_is_punct_-1': False}\n",
      " {'word': 'he', 'word_lower': 'he', 'word_lemma': '-PRON-', 'word_shape': 'xx', 'pos': 'PRON', 'word_1': 'did', 'word_is_title_1': False, 'word_lemma_1': 'do', 'word_lower_1': 'did', 'pos_1': 'VERB', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'what', 'word_is_title_-1': False, 'word_lemma_-1': 'what', 'word_lower_-1': 'what', 'pos_-1': 'NOUN', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'did', 'word_lower': 'did', 'word_lemma': 'do', 'word_shape': 'xxx', 'pos': 'VERB', 'word_1': 'there', 'word_is_title_1': False, 'word_lemma_1': 'there', 'word_lower_1': 'there', 'pos_1': 'ADV', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'he', 'word_is_title_-1': False, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'he', 'pos_-1': 'PRON', 'word_shape_-1': 'xx', 'word_is_punct_-1': False}\n",
      " {'word': 'there', 'word_lower': 'there', 'word_lemma': 'there', 'word_shape': 'xxxx', 'pos': 'ADV', 'word_1': 'He', 'word_is_title_1': True, 'word_lemma_1': '-PRON-', 'word_lower_1': 'he', 'pos_1': 'PRON', 'word_shape_1': 'Xx', 'word_is_punct_1': False, 'word_-1': 'did', 'word_is_title_-1': False, 'word_lemma_-1': 'do', 'word_lower_-1': 'did', 'pos_-1': 'VERB', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False}\n",
      " {'word': 'He', 'word_lower': 'he', 'word_lemma': '-PRON-', 'word_shape': 'Xx', 'pos': 'PRON', 'word_1': 'acknowledged', 'word_is_title_1': False, 'word_lemma_1': 'acknowledge', 'word_lower_1': 'acknowledged', 'pos_1': 'VERB', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'there', 'word_is_title_-1': False, 'word_lemma_-1': 'there', 'word_lower_-1': 'there', 'pos_-1': 'ADV', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'acknowledged', 'word_lower': 'acknowledged', 'word_lemma': 'acknowledge', 'word_shape': 'xxxx', 'pos': 'VERB', 'word_1': 'that', 'word_is_title_1': False, 'word_lemma_1': 'that', 'word_lower_1': 'that', 'pos_1': 'ADP', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'He', 'word_is_title_-1': True, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'he', 'pos_-1': 'PRON', 'word_shape_-1': 'Xx', 'word_is_punct_-1': False}\n",
      " {'word': 'that', 'word_lower': 'that', 'word_lemma': 'that', 'word_shape': 'xxxx', 'pos': 'ADP', 'word_1': 'I', 'word_is_title_1': True, 'word_lemma_1': '-PRON-', 'word_lower_1': 'i', 'pos_1': 'PRON', 'word_shape_1': 'X', 'word_is_punct_1': False, 'word_-1': 'acknowledged', 'word_is_title_-1': False, 'word_lemma_-1': 'acknowledge', 'word_lower_-1': 'acknowledged', 'pos_-1': 'VERB', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'I', 'word_lower': 'i', 'word_lemma': '-PRON-', 'word_shape': 'X', 'pos': 'PRON', 'word_1': 'am', 'word_is_title_1': False, 'word_lemma_1': 'be', 'word_lower_1': 'am', 'pos_1': 'VERB', 'word_shape_1': 'xx', 'word_is_punct_1': False, 'word_-1': 'that', 'word_is_title_-1': False, 'word_lemma_-1': 'that', 'word_lower_-1': 'that', 'pos_-1': 'ADP', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'am', 'word_lower': 'am', 'word_lemma': 'be', 'word_shape': 'xx', 'pos': 'VERB', 'word_1': 'his', 'word_is_title_1': False, 'word_lemma_1': '-PRON-', 'word_lower_1': 'his', 'pos_1': 'ADJ', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'I', 'word_is_title_-1': True, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'i', 'pos_-1': 'PRON', 'word_shape_-1': 'X', 'word_is_punct_-1': False}\n",
      " {'word': 'his', 'word_lower': 'his', 'word_lemma': '-PRON-', 'word_shape': 'xxx', 'pos': 'ADJ', 'word_1': 'mom', 'word_is_title_1': False, 'word_lemma_1': 'mom', 'word_lower_1': 'mom', 'pos_1': 'NOUN', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'am', 'word_is_title_-1': False, 'word_lemma_-1': 'be', 'word_lower_-1': 'am', 'pos_-1': 'VERB', 'word_shape_-1': 'xx', 'word_is_punct_-1': False}\n",
      " {'word': 'mom', 'word_lower': 'mom', 'word_lemma': 'mom', 'word_shape': 'xxx', 'pos': 'NOUN', 'word_1': 'and', 'word_is_title_1': False, 'word_lemma_1': 'and', 'word_lower_1': 'and', 'pos_1': 'CCONJ', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'his', 'word_is_title_-1': False, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'his', 'pos_-1': 'ADJ', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False}\n",
      " {'word': 'and', 'word_lower': 'and', 'word_lemma': 'and', 'word_shape': 'xxx', 'pos': 'CCONJ', 'word_1': 'that', 'word_is_title_1': False, 'word_lemma_1': 'that', 'word_lower_1': 'that', 'pos_1': 'ADP', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'mom', 'word_is_title_-1': False, 'word_lemma_-1': 'mom', 'word_lower_-1': 'mom', 'pos_-1': 'NOUN', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False}\n",
      " {'word': 'that', 'word_lower': 'that', 'word_lemma': 'that', 'word_shape': 'xxxx', 'pos': 'ADP', 'word_1': 'I', 'word_is_title_1': True, 'word_lemma_1': '-PRON-', 'word_lower_1': 'i', 'pos_1': 'PRON', 'word_shape_1': 'X', 'word_is_punct_1': False, 'word_-1': 'and', 'word_is_title_-1': False, 'word_lemma_-1': 'and', 'word_lower_-1': 'and', 'pos_-1': 'CCONJ', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False}\n",
      " {'word': 'I', 'word_lower': 'i', 'word_lemma': '-PRON-', 'word_shape': 'X', 'pos': 'PRON', 'word_1': 'am', 'word_is_title_1': False, 'word_lemma_1': 'be', 'word_lower_1': 'am', 'pos_1': 'VERB', 'word_shape_1': 'xx', 'word_is_punct_1': False, 'word_-1': 'that', 'word_is_title_-1': False, 'word_lemma_-1': 'that', 'word_lower_-1': 'that', 'pos_-1': 'ADP', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'am', 'word_lower': 'am', 'word_lemma': 'be', 'word_shape': 'xx', 'pos': 'VERB', 'word_1': 'supposed', 'word_is_title_1': False, 'word_lemma_1': 'suppose', 'word_lower_1': 'supposed', 'pos_1': 'VERB', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'I', 'word_is_title_-1': True, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'i', 'pos_-1': 'PRON', 'word_shape_-1': 'X', 'word_is_punct_-1': False}\n",
      " {'word': 'supposed', 'word_lower': 'supposed', 'word_lemma': 'suppose', 'word_shape': 'xxxx', 'pos': 'VERB', 'word_1': 'to', 'word_is_title_1': False, 'word_lemma_1': 'to', 'word_lower_1': 'to', 'pos_1': 'PART', 'word_shape_1': 'xx', 'word_is_punct_1': False, 'word_-1': 'am', 'word_is_title_-1': False, 'word_lemma_-1': 'be', 'word_lower_-1': 'am', 'pos_-1': 'VERB', 'word_shape_-1': 'xx', 'word_is_punct_-1': False}\n",
      " {'word': 'to', 'word_lower': 'to', 'word_lemma': 'to', 'word_shape': 'xx', 'pos': 'PART', 'word_1': 'love', 'word_is_title_1': False, 'word_lemma_1': 'love', 'word_lower_1': 'love', 'pos_1': 'VERB', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'supposed', 'word_is_title_-1': False, 'word_lemma_-1': 'suppose', 'word_lower_-1': 'supposed', 'pos_-1': 'VERB', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'love', 'word_lower': 'love', 'word_lemma': 'love', 'word_shape': 'xxxx', 'pos': 'VERB', 'word_1': 'him', 'word_is_title_1': False, 'word_lemma_1': '-PRON-', 'word_lower_1': 'him', 'pos_1': 'PRON', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'to', 'word_is_title_-1': False, 'word_lemma_-1': 'to', 'word_lower_-1': 'to', 'pos_-1': 'PART', 'word_shape_-1': 'xx', 'word_is_punct_-1': False}\n",
      " {'word': 'him', 'word_lower': 'him', 'word_lemma': '-PRON-', 'word_shape': 'xxx', 'pos': 'PRON', 'word_1': 'just', 'word_is_title_1': False, 'word_lemma_1': 'just', 'word_lower_1': 'just', 'pos_1': 'ADV', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'love', 'word_is_title_-1': False, 'word_lemma_-1': 'love', 'word_lower_-1': 'love', 'pos_-1': 'VERB', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'just', 'word_lower': 'just', 'word_lemma': 'just', 'word_shape': 'xxxx', 'pos': 'ADV', 'word_1': 'like', 'word_is_title_1': False, 'word_lemma_1': 'like', 'word_lower_1': 'like', 'pos_1': 'ADP', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'him', 'word_is_title_-1': False, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'him', 'pos_-1': 'PRON', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False}\n",
      " {'word': 'like', 'word_lower': 'like', 'word_lemma': 'like', 'word_shape': 'xxxx', 'pos': 'ADP', 'word_1': 'every', 'word_is_title_1': False, 'word_lemma_1': 'every', 'word_lower_1': 'every', 'pos_1': 'DET', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'just', 'word_is_title_-1': False, 'word_lemma_-1': 'just', 'word_lower_-1': 'just', 'pos_-1': 'ADV', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'every', 'word_lower': 'every', 'word_lemma': 'every', 'word_shape': 'xxxx', 'pos': 'DET', 'word_1': 'other', 'word_is_title_1': False, 'word_lemma_1': 'other', 'word_lower_1': 'other', 'pos_1': 'ADJ', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'like', 'word_is_title_-1': False, 'word_lemma_-1': 'like', 'word_lower_-1': 'like', 'pos_-1': 'ADP', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'other', 'word_lower': 'other', 'word_lemma': 'other', 'word_shape': 'xxxx', 'pos': 'ADJ', 'word_1': 'mom', 'word_is_title_1': False, 'word_lemma_1': 'mom', 'word_lower_1': 'mom', 'pos_1': 'NOUN', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'every', 'word_is_title_-1': False, 'word_lemma_-1': 'every', 'word_lower_-1': 'every', 'pos_-1': 'DET', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'mom', 'word_lower': 'mom', 'word_lemma': 'mom', 'word_shape': 'xxx', 'pos': 'NOUN', 'word_1': 'in', 'word_is_title_1': False, 'word_lemma_1': 'in', 'word_lower_1': 'in', 'pos_1': 'ADP', 'word_shape_1': 'xx', 'word_is_punct_1': False, 'word_-1': 'other', 'word_is_title_-1': False, 'word_lemma_-1': 'other', 'word_lower_-1': 'other', 'pos_-1': 'ADJ', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False}\n",
      " {'word': 'in', 'word_lower': 'in', 'word_lemma': 'in', 'word_shape': 'xx', 'pos': 'ADP', 'word_1': 'the', 'word_is_title_1': False, 'word_lemma_1': 'the', 'word_lower_1': 'the', 'pos_1': 'DET', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'mom', 'word_is_title_-1': False, 'word_lemma_-1': 'mom', 'word_lower_-1': 'mom', 'pos_-1': 'NOUN', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False}\n",
      " {'word': 'the', 'word_lower': 'the', 'word_lemma': 'the', 'word_shape': 'xxx', 'pos': 'DET', 'word_1': 'world', 'word_is_title_1': False, 'word_lemma_1': 'world', 'word_lower_1': 'world', 'pos_1': 'NOUN', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'in', 'word_is_title_-1': False, 'word_lemma_-1': 'in', 'word_lower_-1': 'in', 'pos_-1': 'ADP', 'word_shape_-1': 'xx', 'word_is_punct_-1': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {'word': 'world', 'word_lower': 'world', 'word_lemma': 'world', 'word_shape': 'xxxx', 'pos': 'NOUN', 'word_1': '</S>', 'word_lower_1': '</S>', 'word_-1': 'the', 'word_is_title_-1': False, 'word_lemma_-1': 'the', 'word_lower_-1': 'the', 'pos_-1': 'DET', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False}]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.99      0.98     82412\n",
      "        1.0       0.75      0.47      0.58      5391\n",
      "\n",
      "avg / total       0.95      0.96      0.95     87803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_features_pos_3gram(data, nlp):\n",
    "    dim = 0\n",
    "    for x in data:\n",
    "        dim += len(x)\n",
    "    features = np.ndarray(dim, dtype='object')\n",
    "    y = np.zeros(dim)\n",
    "    i = 0\n",
    "    for sent in data:\n",
    "        sent_tokens = []\n",
    "        for x in sent:\n",
    "            sent_tokens.append(x[0])\n",
    "        doc = Doc(nlp.vocab, words=sent_tokens)\n",
    "        nlp.tagger(doc)\n",
    "        for j in range(len(sent)):\n",
    "            y[i] = sent[j][1]\n",
    "            features[i] = {}\n",
    "            features[i]['word'] = doc[j].text\n",
    "            features[i]['word_lower'] = doc[j].lower_\n",
    "            features[i]['word_lemma'] = doc[j].lemma_\n",
    "#             features[i]['word_is_punct'] = doc[j].is_punct\n",
    "            features[i]['word_shape'] = doc[j].shape_\n",
    "            features[i]['pos'] = doc[j].pos_\n",
    "            for k in [1, -1]:\n",
    "                prefix = '_'+str(k)\n",
    "                if j+k < 0:\n",
    "                    features[i]['word'+prefix] = '<S>'\n",
    "                    features[i]['word_lower'+prefix] = '<S>'\n",
    "                elif j+k >= len(sent):\n",
    "                    features[i]['word'+prefix] = '</S>'\n",
    "                    features[i]['word_lower'+prefix] = '</S>'\n",
    "                else:\n",
    "                    features[i]['word'+prefix] = doc[j+k].text\n",
    "                    features[i]['word_is_title'+prefix] = doc[j+k].is_title\n",
    "                    features[i]['word_lemma'+prefix] = doc[j+k].lemma_\n",
    "                    features[i]['word_lower'+prefix] = doc[j+k].lower_\n",
    "                    features[i]['pos'+prefix] = doc[j+k].pos_\n",
    "                    features[i]['word_shape'+prefix] = doc[j+k].shape_\n",
    "                    features[i]['word_is_punct'+prefix] = doc[j+k].is_punct\n",
    "                    \n",
    "            i += 1\n",
    "    return features, y\n",
    "\n",
    "show_features(train, nlp, prepare_features_pos_3gram)\n",
    "\n",
    "pipe = make_classifier(train, prepare_features_pos_3gram, nlp)\n",
    "classify(test, pipe, prepare_features_pos_3gram, nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading non case sensitive 3-grams from https://www.ngrams.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, zipfile\n",
    "\n",
    "ngrams_coca = {}\n",
    "with zipfile.ZipFile(\"data\\\\w3_.zip\") as myzip:\n",
    "    with myzip.open(\"w3_.txt\", 'r') as file_:\n",
    "        f = io.TextIOWrapper(file_, encoding='ansi')\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            try:\n",
    "                p = line.split('\\t')\n",
    "                if len(p) == 4:\n",
    "                    p[3] = p[3].strip()\n",
    "                    d1 = {}\n",
    "                    if p[1] in ngrams_coca: \n",
    "                        d1 = ngrams_coca[p[1]]\n",
    "                    else:\n",
    "                        ngrams_coca[p[1]] = d1\n",
    "                    d2 = {}\n",
    "                    if p[2] in d1: \n",
    "                        d2 = d1[p[2]]\n",
    "                    else: \n",
    "                        d1[p[2]] = d2\n",
    "                    d2[p[3]] = int(p[0])\n",
    "            except Exception as e:\n",
    "                print(line, str(e))\n",
    "            line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2452"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_count(ngrams, words):\n",
    "    d = ngrams\n",
    "    i = 0\n",
    "    n = len(words)\n",
    "    while i < n and words[i] in d:\n",
    "        d = d[words[i]]\n",
    "        i += 1\n",
    "    if i >= n: \n",
    "        return d\n",
    "    return 0\n",
    "\n",
    "words = ['did', 'you', 'see']\n",
    "print(get_count(ngrams_coca, words))\n",
    "ngrams_coca['did']['you']['see']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['did', False], ['you', False], ['see', False], ['that', True], ['See', False], ['what', False], ['he', False], ['did', False], ['there', True], ['He', False], ['acknowledged', False], ['that', False], ['I', False], ['am', False], ['his', False], ['mom', False], ['and', False], ['that', False], ['I', False], ['am', False], ['supposed', False], ['to', False], ['love', False], ['him', False], ['just', False], ['like', False], ['every', False], ['other', False], ['mom', False], ['in', False], ['the', False], ['world', True]]\n",
      "(32,)\n",
      "[{'word': 'did', 'word_lower': 'did', 'word_lemma': 'do', 'word_shape': 'xxx', 'pos': 'VERB', 'word_1': 'you', 'word_is_title_1': False, 'word_lemma_1': '-PRON-', 'word_lower_1': 'you', 'pos_1': 'PRON', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': '<S>', 'word_lower_-1': '<S>', 'ngram_freq': 0}\n",
      " {'word': 'you', 'word_lower': 'you', 'word_lemma': '-PRON-', 'word_shape': 'xxx', 'pos': 'PRON', 'word_1': 'see', 'word_is_title_1': False, 'word_lemma_1': 'see', 'word_lower_1': 'see', 'pos_1': 'VERB', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'did', 'word_is_title_-1': False, 'word_lemma_-1': 'do', 'word_lower_-1': 'did', 'pos_-1': 'VERB', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False, 'ngram_freq': 7}\n",
      " {'word': 'see', 'word_lower': 'see', 'word_lemma': 'see', 'word_shape': 'xxx', 'pos': 'VERB', 'word_1': 'that', 'word_is_title_1': False, 'word_lemma_1': 'that', 'word_lower_1': 'that', 'pos_1': 'DET', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'you', 'word_is_title_-1': False, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'you', 'pos_-1': 'PRON', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False, 'ngram_freq': 7}\n",
      " {'word': 'that', 'word_lower': 'that', 'word_lemma': 'that', 'word_shape': 'xxxx', 'pos': 'DET', 'word_1': 'See', 'word_is_title_1': True, 'word_lemma_1': 'see', 'word_lower_1': 'see', 'pos_1': 'VERB', 'word_shape_1': 'Xxx', 'word_is_punct_1': False, 'word_-1': 'see', 'word_is_title_-1': False, 'word_lemma_-1': 'see', 'word_lower_-1': 'see', 'pos_-1': 'VERB', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'See', 'word_lower': 'see', 'word_lemma': 'see', 'word_shape': 'Xxx', 'pos': 'VERB', 'word_1': 'what', 'word_is_title_1': False, 'word_lemma_1': 'what', 'word_lower_1': 'what', 'pos_1': 'NOUN', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'that', 'word_is_title_-1': False, 'word_lemma_-1': 'that', 'word_lower_-1': 'that', 'pos_-1': 'DET', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'what', 'word_lower': 'what', 'word_lemma': 'what', 'word_shape': 'xxxx', 'pos': 'NOUN', 'word_1': 'he', 'word_is_title_1': False, 'word_lemma_1': '-PRON-', 'word_lower_1': 'he', 'pos_1': 'PRON', 'word_shape_1': 'xx', 'word_is_punct_1': False, 'word_-1': 'See', 'word_is_title_-1': True, 'word_lemma_-1': 'see', 'word_lower_-1': 'see', 'pos_-1': 'VERB', 'word_shape_-1': 'Xxx', 'word_is_punct_-1': False, 'ngram_freq': 6}\n",
      " {'word': 'he', 'word_lower': 'he', 'word_lemma': '-PRON-', 'word_shape': 'xx', 'pos': 'PRON', 'word_1': 'did', 'word_is_title_1': False, 'word_lemma_1': 'do', 'word_lower_1': 'did', 'pos_1': 'VERB', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'what', 'word_is_title_-1': False, 'word_lemma_-1': 'what', 'word_lower_-1': 'what', 'pos_-1': 'NOUN', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 8}\n",
      " {'word': 'did', 'word_lower': 'did', 'word_lemma': 'do', 'word_shape': 'xxx', 'pos': 'VERB', 'word_1': 'there', 'word_is_title_1': False, 'word_lemma_1': 'there', 'word_lower_1': 'there', 'pos_1': 'ADV', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'he', 'word_is_title_-1': False, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'he', 'pos_-1': 'PRON', 'word_shape_-1': 'xx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'there', 'word_lower': 'there', 'word_lemma': 'there', 'word_shape': 'xxxx', 'pos': 'ADV', 'word_1': 'He', 'word_is_title_1': True, 'word_lemma_1': '-PRON-', 'word_lower_1': 'he', 'pos_1': 'PRON', 'word_shape_1': 'Xx', 'word_is_punct_1': False, 'word_-1': 'did', 'word_is_title_-1': False, 'word_lemma_-1': 'do', 'word_lower_-1': 'did', 'pos_-1': 'VERB', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'He', 'word_lower': 'he', 'word_lemma': '-PRON-', 'word_shape': 'Xx', 'pos': 'PRON', 'word_1': 'acknowledged', 'word_is_title_1': False, 'word_lemma_1': 'acknowledge', 'word_lower_1': 'acknowledged', 'pos_1': 'VERB', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'there', 'word_is_title_-1': False, 'word_lemma_-1': 'there', 'word_lower_-1': 'there', 'pos_-1': 'ADV', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'acknowledged', 'word_lower': 'acknowledged', 'word_lemma': 'acknowledge', 'word_shape': 'xxxx', 'pos': 'VERB', 'word_1': 'that', 'word_is_title_1': False, 'word_lemma_1': 'that', 'word_lower_1': 'that', 'pos_1': 'ADP', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'He', 'word_is_title_-1': True, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'he', 'pos_-1': 'PRON', 'word_shape_-1': 'Xx', 'word_is_punct_-1': False, 'ngram_freq': 5}\n",
      " {'word': 'that', 'word_lower': 'that', 'word_lemma': 'that', 'word_shape': 'xxxx', 'pos': 'ADP', 'word_1': 'I', 'word_is_title_1': True, 'word_lemma_1': '-PRON-', 'word_lower_1': 'i', 'pos_1': 'PRON', 'word_shape_1': 'X', 'word_is_punct_1': False, 'word_-1': 'acknowledged', 'word_is_title_-1': False, 'word_lemma_-1': 'acknowledge', 'word_lower_-1': 'acknowledged', 'pos_-1': 'VERB', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'I', 'word_lower': 'i', 'word_lemma': '-PRON-', 'word_shape': 'X', 'pos': 'PRON', 'word_1': 'am', 'word_is_title_1': False, 'word_lemma_1': 'be', 'word_lower_1': 'am', 'pos_1': 'VERB', 'word_shape_1': 'xx', 'word_is_punct_1': False, 'word_-1': 'that', 'word_is_title_-1': False, 'word_lemma_-1': 'that', 'word_lower_-1': 'that', 'pos_-1': 'ADP', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 8}\n",
      " {'word': 'am', 'word_lower': 'am', 'word_lemma': 'be', 'word_shape': 'xx', 'pos': 'VERB', 'word_1': 'his', 'word_is_title_1': False, 'word_lemma_1': '-PRON-', 'word_lower_1': 'his', 'pos_1': 'ADJ', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'I', 'word_is_title_-1': True, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'i', 'pos_-1': 'PRON', 'word_shape_-1': 'X', 'word_is_punct_-1': False, 'ngram_freq': 4}\n",
      " {'word': 'his', 'word_lower': 'his', 'word_lemma': '-PRON-', 'word_shape': 'xxx', 'pos': 'ADJ', 'word_1': 'mom', 'word_is_title_1': False, 'word_lemma_1': 'mom', 'word_lower_1': 'mom', 'pos_1': 'NOUN', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'am', 'word_is_title_-1': False, 'word_lemma_-1': 'be', 'word_lower_-1': 'am', 'pos_-1': 'VERB', 'word_shape_-1': 'xx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'mom', 'word_lower': 'mom', 'word_lemma': 'mom', 'word_shape': 'xxx', 'pos': 'NOUN', 'word_1': 'and', 'word_is_title_1': False, 'word_lemma_1': 'and', 'word_lower_1': 'and', 'pos_1': 'CCONJ', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'his', 'word_is_title_-1': False, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'his', 'pos_-1': 'ADJ', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False, 'ngram_freq': 5}\n",
      " {'word': 'and', 'word_lower': 'and', 'word_lemma': 'and', 'word_shape': 'xxx', 'pos': 'CCONJ', 'word_1': 'that', 'word_is_title_1': False, 'word_lemma_1': 'that', 'word_lower_1': 'that', 'pos_1': 'ADP', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'mom', 'word_is_title_-1': False, 'word_lemma_-1': 'mom', 'word_lower_-1': 'mom', 'pos_-1': 'NOUN', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'that', 'word_lower': 'that', 'word_lemma': 'that', 'word_shape': 'xxxx', 'pos': 'ADP', 'word_1': 'I', 'word_is_title_1': True, 'word_lemma_1': '-PRON-', 'word_lower_1': 'i', 'pos_1': 'PRON', 'word_shape_1': 'X', 'word_is_punct_1': False, 'word_-1': 'and', 'word_is_title_-1': False, 'word_lemma_-1': 'and', 'word_lower_-1': 'and', 'pos_-1': 'CCONJ', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False, 'ngram_freq': 7}\n",
      " {'word': 'I', 'word_lower': 'i', 'word_lemma': '-PRON-', 'word_shape': 'X', 'pos': 'PRON', 'word_1': 'am', 'word_is_title_1': False, 'word_lemma_1': 'be', 'word_lower_1': 'am', 'pos_1': 'VERB', 'word_shape_1': 'xx', 'word_is_punct_1': False, 'word_-1': 'that', 'word_is_title_-1': False, 'word_lemma_-1': 'that', 'word_lower_-1': 'that', 'pos_-1': 'ADP', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 8}\n",
      " {'word': 'am', 'word_lower': 'am', 'word_lemma': 'be', 'word_shape': 'xx', 'pos': 'VERB', 'word_1': 'supposed', 'word_is_title_1': False, 'word_lemma_1': 'suppose', 'word_lower_1': 'supposed', 'pos_1': 'VERB', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'I', 'word_is_title_-1': True, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'i', 'pos_-1': 'PRON', 'word_shape_-1': 'X', 'word_is_punct_-1': False, 'ngram_freq': 4}\n",
      " {'word': 'supposed', 'word_lower': 'supposed', 'word_lemma': 'suppose', 'word_shape': 'xxxx', 'pos': 'VERB', 'word_1': 'to', 'word_is_title_1': False, 'word_lemma_1': 'to', 'word_lower_1': 'to', 'pos_1': 'PART', 'word_shape_1': 'xx', 'word_is_punct_1': False, 'word_-1': 'am', 'word_is_title_-1': False, 'word_lemma_-1': 'be', 'word_lower_-1': 'am', 'pos_-1': 'VERB', 'word_shape_-1': 'xx', 'word_is_punct_-1': False, 'ngram_freq': 4}\n",
      " {'word': 'to', 'word_lower': 'to', 'word_lemma': 'to', 'word_shape': 'xx', 'pos': 'PART', 'word_1': 'love', 'word_is_title_1': False, 'word_lemma_1': 'love', 'word_lower_1': 'love', 'pos_1': 'VERB', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'supposed', 'word_is_title_-1': False, 'word_lemma_-1': 'suppose', 'word_lower_-1': 'supposed', 'pos_-1': 'VERB', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 3}\n",
      " {'word': 'love', 'word_lower': 'love', 'word_lemma': 'love', 'word_shape': 'xxxx', 'pos': 'VERB', 'word_1': 'him', 'word_is_title_1': False, 'word_lemma_1': '-PRON-', 'word_lower_1': 'him', 'pos_1': 'PRON', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'to', 'word_is_title_-1': False, 'word_lemma_-1': 'to', 'word_lower_-1': 'to', 'pos_-1': 'PART', 'word_shape_-1': 'xx', 'word_is_punct_-1': False, 'ngram_freq': 5}\n",
      " {'word': 'him', 'word_lower': 'him', 'word_lemma': '-PRON-', 'word_shape': 'xxx', 'pos': 'PRON', 'word_1': 'just', 'word_is_title_1': False, 'word_lemma_1': 'just', 'word_lower_1': 'just', 'pos_1': 'ADV', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'love', 'word_is_title_-1': False, 'word_lemma_-1': 'love', 'word_lower_-1': 'love', 'pos_-1': 'VERB', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'just', 'word_lower': 'just', 'word_lemma': 'just', 'word_shape': 'xxxx', 'pos': 'ADV', 'word_1': 'like', 'word_is_title_1': False, 'word_lemma_1': 'like', 'word_lower_1': 'like', 'pos_1': 'ADP', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'him', 'word_is_title_-1': False, 'word_lemma_-1': '-PRON-', 'word_lower_-1': 'him', 'pos_-1': 'PRON', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False, 'ngram_freq': 3}\n",
      " {'word': 'like', 'word_lower': 'like', 'word_lemma': 'like', 'word_shape': 'xxxx', 'pos': 'ADP', 'word_1': 'every', 'word_is_title_1': False, 'word_lemma_1': 'every', 'word_lower_1': 'every', 'pos_1': 'DET', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'just', 'word_is_title_-1': False, 'word_lemma_-1': 'just', 'word_lower_-1': 'just', 'pos_-1': 'ADV', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 4}\n",
      " {'word': 'every', 'word_lower': 'every', 'word_lemma': 'every', 'word_shape': 'xxxx', 'pos': 'DET', 'word_1': 'other', 'word_is_title_1': False, 'word_lemma_1': 'other', 'word_lower_1': 'other', 'pos_1': 'ADJ', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'like', 'word_is_title_-1': False, 'word_lemma_-1': 'like', 'word_lower_-1': 'like', 'pos_-1': 'ADP', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 6}\n",
      " {'word': 'other', 'word_lower': 'other', 'word_lemma': 'other', 'word_shape': 'xxxx', 'pos': 'ADJ', 'word_1': 'mom', 'word_is_title_1': False, 'word_lemma_1': 'mom', 'word_lower_1': 'mom', 'pos_1': 'NOUN', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'every', 'word_is_title_-1': False, 'word_lemma_-1': 'every', 'word_lower_-1': 'every', 'pos_-1': 'DET', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'mom', 'word_lower': 'mom', 'word_lemma': 'mom', 'word_shape': 'xxx', 'pos': 'NOUN', 'word_1': 'in', 'word_is_title_1': False, 'word_lemma_1': 'in', 'word_lower_1': 'in', 'pos_1': 'ADP', 'word_shape_1': 'xx', 'word_is_punct_1': False, 'word_-1': 'other', 'word_is_title_-1': False, 'word_lemma_-1': 'other', 'word_lower_-1': 'other', 'pos_-1': 'ADJ', 'word_shape_-1': 'xxxx', 'word_is_punct_-1': False, 'ngram_freq': 0}\n",
      " {'word': 'in', 'word_lower': 'in', 'word_lemma': 'in', 'word_shape': 'xx', 'pos': 'ADP', 'word_1': 'the', 'word_is_title_1': False, 'word_lemma_1': 'the', 'word_lower_1': 'the', 'pos_1': 'DET', 'word_shape_1': 'xxx', 'word_is_punct_1': False, 'word_-1': 'mom', 'word_is_title_-1': False, 'word_lemma_-1': 'mom', 'word_lower_-1': 'mom', 'pos_-1': 'NOUN', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False, 'ngram_freq': 4}\n",
      " {'word': 'the', 'word_lower': 'the', 'word_lemma': 'the', 'word_shape': 'xxx', 'pos': 'DET', 'word_1': 'world', 'word_is_title_1': False, 'word_lemma_1': 'world', 'word_lower_1': 'world', 'pos_1': 'NOUN', 'word_shape_1': 'xxxx', 'word_is_punct_1': False, 'word_-1': 'in', 'word_is_title_-1': False, 'word_lemma_-1': 'in', 'word_lower_-1': 'in', 'pos_-1': 'ADP', 'word_shape_-1': 'xx', 'word_is_punct_-1': False, 'ngram_freq': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {'word': 'world', 'word_lower': 'world', 'word_lemma': 'world', 'word_shape': 'xxxx', 'pos': 'NOUN', 'word_1': '</S>', 'word_lower_1': '</S>', 'word_-1': 'the', 'word_is_title_-1': False, 'word_lemma_-1': 'the', 'word_lower_-1': 'the', 'pos_-1': 'DET', 'word_shape_-1': 'xxx', 'word_is_punct_-1': False, 'ngram_freq': 0}]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "def quant(freq):\n",
    "    return int(np.log(freq+1))\n",
    "\n",
    "def prepare_with_ngram(data, nlp):\n",
    "    dim = 0\n",
    "    for x in data:\n",
    "        dim += len(x)\n",
    "    features = np.ndarray(dim, dtype='object')\n",
    "    y = np.zeros(dim)\n",
    "    i = 0\n",
    "    for sent in data:\n",
    "        sent_tokens = []\n",
    "        for x in sent:\n",
    "            sent_tokens.append(x[0])\n",
    "        doc = Doc(nlp.vocab, words=sent_tokens)\n",
    "        nlp.tagger(doc)\n",
    "        for j in range(len(sent)):\n",
    "            y[i] = sent[j][1]\n",
    "            features[i] = {}\n",
    "            features[i]['word'] = doc[j].text\n",
    "            features[i]['word_lower'] = doc[j].lower_\n",
    "            features[i]['word_lemma'] = doc[j].lemma_\n",
    "#             features[i]['word_is_punct'] = doc[j].is_punct\n",
    "            features[i]['word_shape'] = doc[j].shape_\n",
    "            features[i]['pos'] = doc[j].pos_\n",
    "            for k in [1, -1]:\n",
    "                prefix = '_'+str(k)\n",
    "                if j+k < 0:\n",
    "                    features[i]['word'+prefix] = '<S>'\n",
    "                    features[i]['word_lower'+prefix] = '<S>'\n",
    "                elif j+k >= len(sent):\n",
    "                    features[i]['word'+prefix] = '</S>'\n",
    "                    features[i]['word_lower'+prefix] = '</S>'\n",
    "                else:\n",
    "                    features[i]['word'+prefix] = doc[j+k].text\n",
    "                    features[i]['word_is_title'+prefix] = doc[j+k].is_title\n",
    "                    features[i]['word_lemma'+prefix] = doc[j+k].lemma_\n",
    "                    features[i]['word_lower'+prefix] = doc[j+k].lower_\n",
    "                    features[i]['pos'+prefix] = doc[j+k].pos_\n",
    "                    features[i]['word_shape'+prefix] = doc[j+k].shape_\n",
    "                    features[i]['word_is_punct'+prefix] = doc[j+k].is_punct\n",
    "            words = [features[i]['word_lower_-1'], features[i]['word_lower'], features[i]['word_lower_1']]\n",
    "            nfreq = get_count(ngrams_coca, words)\n",
    "            features[i]['ngram_freq'] = quant(nfreq)\n",
    "                    \n",
    "            i += 1\n",
    "    return features, y\n",
    "\n",
    "show_features(train, nlp, prepare_with_ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.99      0.98     82412\n",
      "        1.0       0.75      0.48      0.59      5391\n",
      "\n",
      "avg / total       0.95      0.96      0.95     87803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = make_classifier(train, prepare_with_ngram, nlp)\n",
    "classify(test, pipe, prepare_with_ngram, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipe_swift.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipe, 'pipe_swift.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_features(pipe, n):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    vectorizer =  pipe.named_steps['vectorize']\n",
    "    clf =  pipe.named_steps['clf']\n",
    "    print(clf.coef_.shape)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(clf.classes_):\n",
    "        if i >= clf.coef_.shape[0]:\n",
    "            break\n",
    "        top = np.argsort(clf.coef_[i])\n",
    "        reversed_top = top[::-1]\n",
    "        print(\"%s:\\n%s\" % (class_label,\n",
    "              \"\\n\".join(feature_names[j] for j in reversed_top[:n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 332087)\n",
      "0.0:\n",
      "word_lower_1=</S>\n",
      "word_1=</S>\n",
      "word_1=And\n",
      "word_1=jesus\n",
      "word_1=john\n",
      "word_1=A\n",
      "word_1=At\n",
      "word_shape_1=Xx\n",
      "word_shape=.xx\n",
      "word_1=christmas\n",
      "word_shape_1=xx'xx\n",
      "word_1=With\n",
      "word_lemma_1=i\n",
      "word=WHAT\n",
      "word_1=In\n",
      "word_1=ah\n",
      "word_1=jim\n",
      "word_1=OH\n",
      "word_1=vi\n",
      "word_shape_1=x.\n"
     ]
    }
   ],
   "source": [
    "top_features(pipe, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on run-on-test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.98      0.99      4542\n",
      "        1.0       0.56      0.66      0.61       155\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_data = pd.read_json('data\\\\run-on-test.json', orient='values', compression=None, typ='series')\n",
    "classify(validation_data, pipe, prepare_features_pos_3gram, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
