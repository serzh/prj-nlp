{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сентимент-аналіз із використанням \"мішка слів\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сентимент-аналізу я витягнув користувацькі відгуки з кількох категорій розділу \"Побутова техніка\" на Розетці. Код для витягування міститься у файлі prepare_data.py. Там же здійснено первинну обробку даних (наприклад, видалено слова \"Недоліки\" та \"Переваги\", які по шаблону від \"Розетки\" містяться майже в усіх відгуках) та відокремлено україномовні відгуки, використовуючи бібліотеку langdetect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Більша частина коду виведена в окремі файли в цій директорії, а тут я лише продемонструю, як код працює і які дає результати. Для початку створимо тренувальний та тестувальний датасети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_datasets import df, build_datasets, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for all subsequent random states\n",
    "seed = 505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = build_datasets(df, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для початку трохи про дані. Я загалом практично не дивився на \"нейтральні\" відгуки: більшості людей на розетці продукт або подобається (навіть якщо з деякими недоліками), або не подобається, при чому 3 бали - це вже негатив, попри \"серединність\" цієї оцінки (хто з нас купив би продукт із середнім балом 3 із 5?). Крім того, і це мене здивувало, негативних відгуків (оцінка 3 і нижче) - тільки 11.5% від загальної кількості відгуків:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    7339\n",
       "neg     952\n",
       "Name: sent, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Незбалансованість класів означає проблеми при класифікації, як ми побачимо далі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes v.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найпростіший класифікатор на основі мішка слів з використанням Наївного Баєса, без використання стоп-слів, лем і т.д.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_classifier import prob_dict, priors, baseline_classifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.00      0.00      0.00       281\n",
      "        pos       0.89      1.00      0.94      2206\n",
      "\n",
      "avg / total       0.79      0.89      0.83      2487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test['base_pred'] = test['text'].apply(\n",
    "    lambda text: baseline_classifier(text, prob_dict, priors))\n",
    "\n",
    "print(classification_report(test['sent'], test['base_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жоден відгук не було класифіковано як негативний, що робить цей класифікатор totally useless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тональний класифікатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наступний крок - спробувати тональний словник. Цей класифікатор сумує тональність усіх слів відгуку (беручи леми слів), які мають хоч якусь тональність, і класифікує як негативні ті відгуки, які мають мінусову сумарну тональність. Стоп-слова не враховуються."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.27      0.35      0.31       281\n",
      "        pos       0.91      0.88      0.90      2206\n",
      "\n",
      "avg / total       0.84      0.82      0.83      2487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tonal_classifier import tonal_dict, tonal_classifier\n",
    "\n",
    "test['tonal_pred'] = test['text'].apply(\n",
    "    lambda text: tonal_classifier(text, tonal_dict, stopwords))\n",
    "\n",
    "print(classification_report(test['sent'], test['tonal_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цього разу ми знайшли понад третину негативних відгуків, але recall та F1 для негативних відгуків усе ще дуже низькі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes v.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Наступний крок - Наївний Баєс версія 2, імплементований з такими особливостями:\n",
    "\n",
    "- враховано стоп-слова (перелік знайдено в інтернеті);\n",
    "- беруться леми слів (як їх подає бібліотека pymorphy2);\n",
    "- кожне слово у відгуці рахується лише раз (якщо \"поганий\" згадано 5 разів в одному відгуці, ми рахуємо його тільки раз) - за рекомендацією Джурафскі і Мартіна;\n",
    "- усі слова після \"не\" і до найближчого розділового знака ми перетворюємо на спеціальні токети з \"НЕ\" на початку: \"не дуже хороший\" стає \"не\", \"НЕ\\_дуже\", \"НЕ\\_хороший\" - це теж рекомендація з підручника."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.00      0.00      0.00       281\n",
      "        pos       0.89      1.00      0.94      2206\n",
      "\n",
      "avg / total       0.79      0.89      0.83      2487\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from naive_bayes import build_vocab, get_priors, train_Naive_Bayes, NB_classifier\n",
    "\n",
    "vocab = build_vocab(train, 'sent', 'text', stopwords)\n",
    "priors = get_priors(train, 'sent')\n",
    "prob_dict = train_Naive_Bayes(vocab, ['pos', 'neg'])\n",
    "\n",
    "test['NB_pred'] = test['text'].apply(\n",
    "    lambda text: NB_classifier(text, prob_dict, priors, stopwords))\n",
    "\n",
    "print(classification_report(test['sent'], test['NB_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "І у нас знову жоден відгук не класифікується як негативний! Усе-таки головна проблема, вочевидь, у незбалансованих класах у тренувальній вибірці. \n",
    "\n",
    "Якщо ми зробимо даунсемплінг для позитивних відгуків таким чином, щоб їх у тренувальному сеті лишилась тільки половина, то класифікатор зможе краще \"бачити\" негативні відгуки у тестувальній вибірці (яка, звісно, повинна лишатись незбалансованою, як це є в реальному житті). Мінус - ми втрачаємо дані, яких і так не надто багато. Але ми можемо використати ці дані у тестовій вибірці - вона буде більшою, ніж тренувальна, але це зовсім не проблема, тому що тест-сет має показувати результати в \"реальності\", яка завжди більша за будь-який трейн-сет :) \n",
    "\n",
    "Опція balanced у функції build_datasets будує збалансований тренувальний датасет (порівну обох класів), а потім із усіх інших даних робить тестову вибірку таким чином, щоб відсоток обох класів збігався з відсотком у початкових даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "train, test = build_datasets(df, random_state=seed, balanced=True)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.45      0.76      0.56       486\n",
      "        pos       0.97      0.88      0.92      3746\n",
      "\n",
      "avg / total       0.91      0.86      0.88      4232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(train, 'sent', 'text', stopwords)\n",
    "priors = get_priors(train, 'sent')\n",
    "prob_dict = train_Naive_Bayes(vocab, ['pos', 'neg'])\n",
    "\n",
    "test['NB_pred'] = test['text'].apply(\n",
    "    lambda text: NB_classifier(text, prob_dict, priors, stopwords))\n",
    "\n",
    "print(classification_report(test['sent'], test['NB_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас усе ще низький precision, але непогане покриття і F1 для негативних відгуків, що дозволяє отримати також найвищий поки що загальний F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другий алгоритм класифікації, який я імплементував \"своїми силами\" - перцептрон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptron import initialize_vocab, train_perceptron, perceptron_classifier\n",
    "\n",
    "perc_vocab = initialize_vocab(vocab) # using vocabulary created before\n",
    "trained_vocab = train_perceptron(100, train, perc_vocab, stopwords)\n",
    "# 100 iterations takes quite some time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.30      0.78      0.44       486\n",
      "        pos       0.96      0.77      0.86      3746\n",
      "\n",
      "avg / total       0.89      0.77      0.81      4232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test['perc_pred'] = test['text'].apply(\n",
    "    lambda text: perceptron_classifier(text, trained_vocab, stopwords))\n",
    "\n",
    "print(classification_report(test['sent'], test['perc_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Плюс перцептрона - поки що найкраще покриття для негативних відгуків. У деяких задачах цей плюс може перевершувати мінуси. А мінуси - повільність та гірші результати в усіх інших показниках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класифікація в sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер можна спробувати класифікувати ті самі дані за допомогою класифікаторів \"з коробки\", яких вдосталь у бібліотеці scikit-learn. Для кожного з класифікаторів ми застосуємо підхід \"мішок слів\", із токенізацією від бібліотеки tokenize_uk та тим самим списком стоп-слів, що і раніше. Також використаємо TF-IDF-трансформацію, яку вміє автоматично робити sklearn (вона додає дуже мало покращення до результатів, і я не застосовував цю трансформацію до \"своїх\" класифікаторів)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_uk.tokenize_uk import tokenize_words\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спершу - две версії Наївного Баєса, одна з урахуванням повторюваності слів у документі, інша - без (подібно до того, як я рахував кожне слово в документі тільки один раз)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.44      0.70      0.54       486\n",
      "        pos       0.96      0.89      0.92      3746\n",
      "\n",
      "avg / total       0.90      0.87      0.88      4232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sklearn_NB = Pipeline([('vect', CountVectorizer(tokenizer=tokenize_words,\n",
    "                                             stop_words=stopwords)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "sklearn_NB.fit(train['text'], train['sent'])\n",
    "\n",
    "test['sklearn_NB_pred'] = sklearn_NB.predict(test['text'])\n",
    "\n",
    "print(classification_report(test['sent'], test['sklearn_NB_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.49      0.53      0.51       486\n",
      "        pos       0.94      0.93      0.93      3746\n",
      "\n",
      "avg / total       0.89      0.88      0.88      4232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bernoulli = Pipeline([('vect', CountVectorizer(tokenizer=tokenize_words,\n",
    "                                             stop_words=stopwords)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', BernoulliNB())])\n",
    "bernoulli.fit(train['text'], train['sent'])\n",
    "\n",
    "test['bernoulli_pred'] = bernoulli.predict(test['text'])\n",
    "print(classification_report(test['sent'], test['bernoulli_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Середній F1 близький до мого Наївного Баєса (залежно від параметру seed на початку, інколи мій класифікатор показує трохи кращі результати, інколи навпаки), що логічно.\n",
    "\n",
    "Як щодо логістичної регресії?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.35      0.78      0.48       486\n",
      "        pos       0.97      0.81      0.88      3746\n",
      "\n",
      "avg / total       0.89      0.81      0.83      4232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic = Pipeline([('vect', CountVectorizer(tokenizer=tokenize_words,\n",
    "                                             stop_words=stopwords)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', LogisticRegression(random_state=seed))])\n",
    "\n",
    "logistic.fit(train['text'], train['sent'])\n",
    "\n",
    "test['logistic_pred'] = logistic.predict(test['text'])\n",
    "print(classification_report(test['sent'], test['logistic_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результати погіршились, хоча покриття для негативних відгуків хороше.\n",
    "\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.30      0.85      0.44       486\n",
      "        pos       0.97      0.75      0.84      3746\n",
      "\n",
      "avg / total       0.90      0.76      0.80      4232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = Pipeline([('vect', CountVectorizer(tokenizer=tokenize_words,\n",
    "                                             stop_words=stopwords)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', SGDClassifier(tol=1e-3, max_iter=100,\n",
    "                                           random_state=seed))])\n",
    "\n",
    "svm.fit(train['text'], train['sent'])\n",
    "\n",
    "test['svm_pred'] = svm.predict(test['text'])\n",
    "print(classification_report(test['sent'], test['svm_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результати схожі. Я пробував деякі (далеко не всі можливі) комбінації параметрів, і не схоже що щось суттєво змінилось би. \n",
    "\n",
    "Загалом виглядає так, що дані, які в мене є, і їхня порівняно невелика кількість роблять Наївний Баєс із застосуванням кількох методів обробки фіч (лематизація, стоп-слова, врахування негацій), і зі збалансованим тренувальним датасетом, найкращим методом класифікації відгуків на позитивні та негативні в межах парадигми \"мішка слів\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонус\n",
    "\n",
    "У нашому датасеті є окремо витягнуті тексти переваг і тексти недоліків товарів. Логічно, що в позитивних відгуках буде більше тексту про переваги, а в негативних - навпаки. Це можна перевірити на наших даних, попередньо замінивши відсутні переваги або недоліки на прочерк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_adv</th>\n",
       "      <th>len_disadv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>21.514706</td>\n",
       "      <td>52.358193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>35.413135</td>\n",
       "      <td>23.371440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        len_adv  len_disadv\n",
       "sent                       \n",
       "neg   21.514706   52.358193\n",
       "pos   35.413135   23.371440"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "df2['adv'] = df2['adv'].fillna('-')\n",
    "df2['disadv'] = df2['disadv'].fillna('-')\n",
    "df2['len_adv'] = df2['adv'].apply(len)\n",
    "df2['len_disadv'] = df2['disadv'].apply(len)\n",
    "df2.groupby('sent')[['len_adv', 'len_disadv']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У негативних відгуках різниця в довжині тексту дуже велика, у позитивних - помірна. Цей факт можна використати для примітивного класифікатора, який позначає позитивними всі відгуки, де переваги довші за недоліки, і навпаки для негативних. Я не ділив дані на тренувальну і тестувальну вибірку, тому що тут нема ніякого тренування."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.28      0.66      0.39       952\n",
      "        pos       0.95      0.78      0.85      7339\n",
      "\n",
      "avg / total       0.87      0.76      0.80      8291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classify_by_len(row):\n",
    "    len_adv = row['len_adv']\n",
    "    len_disadv = row['len_disadv']\n",
    "    if len_adv >= len_disadv:\n",
    "        return 'pos'\n",
    "    else:\n",
    "        return 'neg'\n",
    "    \n",
    "df2['len_pred'] = df2.apply(classify_by_len, axis=1)\n",
    "print(classification_report(df2['sent'], df2['len_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результати далеко не найгірші :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Що я не робив\n",
    "\n",
    "- нейтральні відгуки. Було б складніше побудувати збалансований тренувальний + \"великий\" тестувальний датасет, і в будь-якому разі незрозуміло, чи справді відгуки, які мають 3 або 4 бали, можна назвати нейтральними;\n",
    "- текст лише до початку шаблону \"Переваги: ... Недоліки: ...\". Нерідко в недоліках або перевагах просто слова \"нема\" або \"ціна\", що без контексту, по ідеї, тільки додає шум. Але спроби класифікувати без цього тексту дають трохи гірші результати, тому що все-таки частіше в шаблон вписують змістовний текст, або не пишуть ніякий текст перед шаблоном."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
