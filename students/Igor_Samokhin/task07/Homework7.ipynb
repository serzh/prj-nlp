{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection of sentence endings in run-on sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для завдання визначення слів, що закінчують речення в \"run-on sentences\", я спробував пошукати дані серед датасетів Kaggle. Дані з Твіттера відразу показали низький результат, і вони в будь-якому разі погано підходять - у Твіттері багато специфічних символів і скорочень, може оминатись пунктуація тощо. Це ускладнює роботу для парсера і водночас не дуже схоже на реальні run-on sentences.\n",
    "\n",
    "Тексти з блогів підійшли краще, але зрештою я зупинився на транскриптах із TED Talks (https://www.kaggle.com/rounakbanik/ted-talks). Спічі на TED мають одночасно ознаки розмовного і наукового стилів. Речення зазвичай короткі, а саме поєднання коротких речень, як мені здається, найчастіше призводить до \"забування\" про крапки між реченнями.\n",
    "\n",
    "2467 транскриптів містять 277465 речень. Для тренування класифікатора я \"склеїв\" послідовні речення купками по 3-5 (спочатку я робив по 1-3, як у тестувальних даних, але виявилось, що класифікатор трохи краще тренується на більших склеєних реченнях). Склеєні, токенізовані речення з лейблом True/False для кожного слова я зберіг у json, щоб тренувальні дані були тієї ж форми, що і тестувальні. Функції, які обробляють і зберігають тренувальні дані, знаходяться у файлі `prepare_data.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завантажимо тестові дані і подивимось, скільки в ньому закінчень речень."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 145), (0, 50), (2, 5)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = json.load(open('run-on-test.json'))\n",
    "\n",
    "from collections import Counter\n",
    "counts = []\n",
    "for d in test_set:\n",
    "    true_count = 0\n",
    "    for w in d:\n",
    "        if w[1] == True:\n",
    "            true_count += 1\n",
    "    counts.append(true_count)\n",
    "    \n",
    "Counter(counts).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У 50 випадках речення взагалі не склеєні, у 145 - склеєно два речення, у п'яти випадках - три."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оскільки тренувальні та тестувальні речення заздалегідь токенізовані, потрібна функція, яка робитиме spacy Doc із токенів (це потребує, крім самих токенів, визначення наявності пробілів після них)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spaceable(token_list):\n",
    "    \"\"\"\n",
    "    Make possible use of spacy doc\n",
    "    with already tokenized text\n",
    "    \"\"\"\n",
    "    tokens = [t[0] for t in token_list]\n",
    "    spaces = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i == len(tokens)-1:\n",
    "            spaces.append(False)\n",
    "        elif ((tokens[i+1] in '!%),.:;?}')\n",
    "              or (tokens[i+1].startswith(\"'\") and len(tokens[i+1]) > 1)):\n",
    "            spaces.append(False)\n",
    "        else:\n",
    "            spaces.append(True)\n",
    "    assert len(spaces) == len(tokens)\n",
    "    doc = spacy.tokens.doc.Doc(\n",
    "        nlp.vocab, words=tokens, spaces=spaces)\n",
    "    for name, proc in nlp.pipeline:\n",
    "        doc = proc(doc)        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завантажуємо тренувальні дані."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70273\n",
      "5795006\n"
     ]
    }
   ],
   "source": [
    "PATH = '/mnt/hdd/Data/NLP/'\n",
    "ted_set = json.load(open(PATH+'ted_data.json'))\n",
    "print(len(ted_set))\n",
    "print(sum(len(sent) for sent in ted_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас вийшло близько 70 тисяч склеєних речень з майже 5,8 мільйонів токенів. На практиці виявилось, що поєднання такої кількості фіч з такою кількістю токенів якраз майже цілком заповнює мої 16 гб оперативної пам'яті."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функція, яка дістає фічі з кожного слова токенізованого склеєного речення. Окрім самого слова, функція дивиться на контекст - два слова ліворуч і два слова праворуч. Таким чином функція аналізує, крім уніграмів, біграми і триграми.\n",
    "\n",
    "Особливості слів, які мені здались корисними для визначення, чи є слово закінченням речення:\n",
    "- саме слово, його лемма і написання з малої літери;\n",
    "- чи є воно з великої літери;\n",
    "- його довжина, його положення в речення (кількість символів від початку речення до слова);\n",
    "- POS-тег, тип залежності;\n",
    "- чи є пунктуацією;\n",
    "- усе те саме для одного або двох правих і лівих слів (те, що здалось важливішим, для двох правих і лівих);\n",
    "- ліва та права біграми, якщо можливо;\n",
    "- кількість лівих та правих \"дітей\" слова у синтаксичному дереві (для закінчення речення, правих дітей, по ідеї, не повинно бути);\n",
    "- найнижчий спільний предок слова та двох правих і двох лівих слів у синтаксичному дереві: у випадку закінчення речення, спільних предків з правими словами бути не повинно або вони будуть дуже високо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отримання всієї інформації про токени я використав spacy, модель англійської мови середнього розміру (велика модель не давала відчутного покращення)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(token_data):\n",
    "    \"\"\"\n",
    "    Make a list of features and two lists of labels\n",
    "    from our data\n",
    "    \"\"\"\n",
    "    data, labels = [], []\n",
    "    wrong_count = 0\n",
    "    for token_list in tqdm(token_data):\n",
    "        doc = make_spaceable(token_list)\n",
    "        sent_len = len(doc)\n",
    "        lca_matrix = doc.get_lca_matrix()\n",
    "        if not sent_len == len(token_list):\n",
    "            continue\n",
    "        for token in doc:\n",
    "            i = token.i\n",
    "            features = {\n",
    "                'word': token.text,\n",
    "                'word_lower': token.lower_,\n",
    "                'word_lemma': token.lemma_,\n",
    "                'word_pos': token.pos_,\n",
    "                'word_dep': token.dep_,\n",
    "                'word_capitalized': True if token.is_title else False,\n",
    "                'word_ispunct': token.is_punct,\n",
    "                'word_n_lefts': token.n_lefts,\n",
    "                'word_n_rights': token.n_rights,\n",
    "                'word_idx': token.idx,\n",
    "                'length': len(token.text)\n",
    "            }\n",
    "            if i > 0:\n",
    "                features.update({\n",
    "                    'word-1': doc[i-1].lower_,\n",
    "                    'w-1pos': doc[i-1].pos_,\n",
    "                    'w-1dep': doc[i-1].dep_,\n",
    "                    'w-1_lca': lca_matrix[i, i-1],\n",
    "                    'word-1_capitalized': True if doc[i-1].is_title else False,\n",
    "                    'word-1_ispunct': True if doc[-1].is_punct else False\n",
    "                })\n",
    "            if i > 1:\n",
    "                features.update({\n",
    "                    'word-2': doc[i-2].lower_,\n",
    "                    'w-2dep': doc[i-2].dep_,\n",
    "                    'w-2_lca': lca_matrix[i, i-2],\n",
    "                    'left_bigram': doc[i-2].lower_ + '_' + doc[i-1].lower_\n",
    "                })\n",
    "            if i < sent_len - 1:\n",
    "                lca = lca_matrix[i, i+1]\n",
    "                features.update({\n",
    "                    'word+1': doc[i+1].lower_,\n",
    "                    'w+1pos': doc[i+1].pos_,\n",
    "                    'w+1dep': doc[i+1].dep_,\n",
    "                    'w+1_lca': lca if lca > -1 else 100,\n",
    "                    'word+1_capitalized': True if doc[i+1].is_title else False,\n",
    "                    'word+1_ispunct': True if doc[i+1].is_punct else False,\n",
    "                    'word+1_n_rights': doc[i+1].n_rights,\n",
    "                    'word+1_n_lefts': doc[i+1].n_lefts\n",
    "                })\n",
    "            if i < sent_len - 2:\n",
    "                lca = lca_matrix[i, i+2]\n",
    "                features.update({\n",
    "                    'word+2': doc[i+2].lower_,\n",
    "                    'w+2dep': doc[i+2].dep_,\n",
    "                    'w+2_lca': lca if lca > -1 else 100,\n",
    "                    'right_bigram': (doc[i+1].lower_ + '_' + doc[i+2].lower_)\n",
    "                })\n",
    "            last_lab = token_list[i][1]\n",
    "            labels.append(last_lab)\n",
    "            data.append(features)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отримаємо наші фічі, для тренувального і для тестувального датасету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70273/70273 [48:08<00:00, 24.33it/s]\n"
     ]
    }
   ],
   "source": [
    "data, labels = get_features(ted_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably it can free some memory, I'm not sure\n",
    "del ted_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестових сетів у нас два, тому валідаційний, \"зовнішній\" тестсет буде позначатись як test_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 87.46it/s]\n"
     ]
    }
   ],
   "source": [
    "test_test_data, test_test_labels = get_features(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2,\n",
    "                                                                   random_state=505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "train_features = vec.fit_transform(train_data)\n",
    "test_features = vec.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для класифікатора я використав логістичну регресію із пакету sklearn. Параметр penalty='l1' на цих даних працює краще, ніж 'l2', в усьому іншомі параметри дефолтні. Параметр class_weight='balanced' не дає покращення. Класифікатор LogisticRegressionCV (поєднання класифікатора і кросвалідації) я теж пробував, але він на диво дав трохи гірший результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=505, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='l1', random_state=505)\n",
    "clf.fit(X=train_features, y=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результати на тестових даних із транскриптів TED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.991     0.996     0.993   1117930\n",
      "       True      0.871     0.745     0.803     41072\n",
      "\n",
      "avg / total      0.986     0.987     0.987   1159002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = clf.predict(test_features)\n",
    "print(classification_report(test_labels, pred1, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результати на власне тестовому, валідаційному датасеті:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.992     0.990     0.991      4542\n",
      "       True      0.726     0.768     0.746       155\n",
      "\n",
      "avg / total      0.983     0.983     0.983      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_test_features = vec.transform(test_test_data)\n",
    "pred2 = clf.predict(test_test_features)\n",
    "print(classification_report(test_test_labels, pred2, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Звичайно, в усіх результатах нас найбільше цікавить не середній F1 (завжди близький до одиниці), а F1 для лейблу \"True\", а також precision i recall для цього ж лейблу. \n",
    "\n",
    "На жаль, F1 вище ніж 0.74 так і не вдалось досягнути. Могло б допомогти більше часу або оперативної пам'яті, але навряд чи з цими даними та умовами вдалось би витягнути на більш ніж 0.75-0.76.\n",
    "\n",
    "Цікаво, що на тестовому сеті з транскриптів TED precision вищий за recall, а у валідаційному сеті навпаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Інші підходи, які я пробував:\n",
    "\n",
    "- даунсемплінг для лейблів False та апсемплінг для лейблів True, з допомогою функці sklearn.utils.resample: обидва підходи дають незначне погіршення на тестовому сеті з транскриптів та значне погіршення на валідаційному сеті - важко сказати, чому;\n",
    "\n",
    "- sklearn CRF suite - CRF для текстової класифікації. Працює непогано, але f1 стабільно виходив на 0.01 гірший, ніж з використанням просто логістичної регресії;\n",
    "\n",
    "- sklearn SGDClassifier (loss='hinge', що означає використання лінійної SVM) - досить помітне погіршення результату на обох тестових вибірках;\n",
    "\n",
    "- зменшення кількості фіч (якщо видалити декілька з тих, які здаються менш важливими) дуже-дуже незначно погіршує результат, тому з точки зору швидкості розрахунків є сенс використати менше фіч. Але для максимально можливого результату тут я їх не видаляв.\n",
    "\n",
    "Частково ці підходи можна подивитись в іншому .ipynb файлі `Draft`.\n",
    "\n",
    "В цілому в усіх випадках підхід, який працює гірше для тестової вибірки з транскриптів, так само гірше працює і для валідаційної вибірки, але інколи різниця між показниками для двох вибірок значно більша, ніж для логістичної регресії."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вже готуючись здавати домашнє, я розумію, що можна було спробувати краще зробити фічі про найближчого спільного предка і більше попрацювати на рівні біграм та триграм (я так і не дійшов до біграм з Гугла). Але для початку і це, можливо, нормальний результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Вже після дедлайну, я ще кілька разів прогнав логістичну регресію на вибірці з цих даних. Навіть вибірки з 5000 речень і фіч ЛИШЕ для поточного слова і наступного слова достатньо, щоб отримати F1 0.67 для валідаційних даних. Причому якщо брати фічі тільки для поточного слова, без жодного контексту, F1 близький до нуля. Оскільки у випадку останнього слова у реченні наступне слово має низку особливостей - це завжди не пунктуаційний знак, часто пишеться з великої літери, і парсер може зрозуміти, що це слово має інший рут - то логічно, що саме фічі наступного слова дають найбільший приріст до результату."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
