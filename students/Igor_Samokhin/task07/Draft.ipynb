{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цього тижня ви працюватимете над задачею виправлення помилок.\n",
    "\n",
    "Run-on речення - це речення, склеєне з двох чи більше речень без належної пунктуації. Таку помилку часто допускають механічно, коли швидко друкують текст, проте така помилка виникає і від незнання мови. Особливо часто ця помилка зустрічається в інтернет-спілкуванні.\n",
    "\n",
    "Наприклад:\n",
    "\n",
    ">Thanks for talking to me let's meet again tomorrow Bye.\n",
    "\n",
    "У цьому реченні насправді три склеєні речення. Правильний варіант:\n",
    "\n",
    ">Thanks for talking to me. Let's meet again tomorrow. Bye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дані:\n",
    "\n",
    "    Виберіть будь-який відкритий корпус та згенеруйте тренувальні дані для моделі. Тренувальними даними буде набір слеєних речень. Візьміть до уваги, що склеєних речень може бути кілька (зазвичай 2, але буває і 3-4), а перше слово наступного речення може писатися з великої чи малої літери.\n",
    "    Зберіть чи знайдіть у відкритому доступі базу енграмів. Візьміть до уваги, що енграми на межі речень доведеться збирати власноруч.\n",
    "\n",
    "Тестування:\n",
    "\n",
    "    Напишіть бейзлайн та метрику для тестування якості.\n",
    "    \n",
    "Класифікатор:\n",
    "\n",
    "    Виділіть ознаки, які впливають на те, чи є слово на межі речень. Подумайте про контекст, написання слів, граматичні ознаки, енграми, межі речень тощо.\n",
    "    Побудуйте класифікатор на основі логістичної регресії з використанням виділених ознак, який анотує послідовно слова у реченні на предмет закінчення речення.\n",
    "    Спробуйте покращити якість роботи класифікатора, змінюючи набір чи комбінацію ознак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "test_set = json.load(open('run-on-test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155 75\n"
     ]
    }
   ],
   "source": [
    "overall_count = 0\n",
    "upcount = 0\n",
    "for d in test_set:\n",
    "    for i, w in enumerate(d):\n",
    "        if w[1] == True:\n",
    "            overall_count += 1\n",
    "            if d[i+1][0].istitle():\n",
    "                upcount += 1\n",
    "print(overall_count, upcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 145), (0, 50), (2, 5)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = []\n",
    "for d in test_set:\n",
    "    true_count = 0\n",
    "    for w in d:\n",
    "        if w[1] == True:\n",
    "            true_count += 1\n",
    "    counts.append(true_count)\n",
    "    \n",
    "Counter(counts).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to reconstruct tokenized sentences\n",
    "def reconstruct(token_list):\n",
    "    \"\"\"\n",
    "    Make sentence from a list of tokens\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    tokens = [t[0] for t in token_list]\n",
    "    for i, token in enumerate(tokens):\n",
    "        if ((i != 0) \n",
    "            and (token not in '!%),.:;?}')\n",
    "            and not (token.startswith(\"'\") and len(token) > 1)):\n",
    "            text += ' ' + token\n",
    "        else:\n",
    "            text += token\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spaceable(token_list):\n",
    "    \"\"\"\n",
    "    Make possible use of spacy doc\n",
    "    with already tokenized text\n",
    "    \"\"\"\n",
    "    tokens = [t[0] for t in token_list]\n",
    "    spaces = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i == len(tokens)-1:\n",
    "            spaces.append(False)\n",
    "        elif ((tokens[i+1] in '!%),.:;?}')\n",
    "              or (tokens[i+1].startswith(\"'\") and len(tokens[i+1]) > 1)):\n",
    "            spaces.append(False)\n",
    "        else:\n",
    "            spaces.append(True)\n",
    "    assert len(spaces) == len(tokens)\n",
    "    doc = spacy.tokens.doc.Doc(\n",
    "        nlp.vocab, words=tokens, spaces=spaces)\n",
    "    for name, proc in nlp.pipeline:\n",
    "        doc = proc(doc)        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_firsts(sent_list):\n",
    "    \"\"\"\n",
    "    Add labels for first words in test data\n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    for sent in sent_list:\n",
    "        new_tokens = []\n",
    "        for i, t in enumerate(sent):\n",
    "            if i == 0:\n",
    "                is_first = False\n",
    "            elif sent[i-1][1]:\n",
    "                is_first = True\n",
    "            else:\n",
    "                is_first = False\n",
    "            triple = [t[0], t[1], is_first]\n",
    "            new_tokens.append(triple)\n",
    "        new_sents.append(new_tokens)\n",
    "    return new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set = add_firsts(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/mnt/hdd/Data/NLP/'\n",
    "#ted = pd.read_csv(PATH+'transcripts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blogs = []\n",
    "with open(PATH+'en_US/en_US.blogs.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if len(line) < 300:\n",
    "            continue\n",
    "        else:\n",
    "            blogs.append(line.strip('\\n '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ted['transcript'] = ted['transcript'].str.replace('\\(.*?\\)', ' ').str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "План побудови фіч:\n",
    "\n",
    "- розбиваємо текст на речення\n",
    "- в кожному реченні виділяємо фічі для речення: довжина всіх символів, кількість слів, який токен закінчує речення\n",
    "- переходимо на рівень слів і виділяємо фічі: саме слово, його номер у реченні, його довжина і форма, ПОС-таг, депенденсі-таг, чи є першим у реченні, чи є останнім, контекст - по два слова праворуч і ліворуч, всі їхні теги та інші фічі, ліва та права біграми;\n",
    "- знаючи, який токен закінчує речення, повертаємось на рівень речень, беремо пари речень, видаляємо закінчуючий токен у першого з них, видаляємо фічу \"кінець речення\" в останнього, і розбиваємо все на біграми, кожна з яких міститиме фічі окремих слів (?), фічу \"чи належать до одного чанку\" (одної синтаксичної фрази), фічу \"рівень найближчого батька\", фічу \"чи є один залежним від іншого\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_sents(sents):\n",
    "    \"\"\"\n",
    "    sents is a list of 2 to 3 sentences, most often 2\n",
    "    returns list of tokens together with their first\n",
    "    and last positions\n",
    "    \"\"\"\n",
    "    glued_tokens = []\n",
    "    for j, sent in enumerate(sents):\n",
    "        doc = nlp(sent, disable=['parser', 'ner'])\n",
    "        if len([t for t in doc if not t.is_punct]) == 0:\n",
    "            continue\n",
    "        for token in doc:\n",
    "            if token.i == 0 and j != 0:\n",
    "                # randomly uncapitalize some previously capitalized words\n",
    "                if (random.random() < 0.5\n",
    "                    and token.pos_ != 'PROPN'\n",
    "                    and token.text != 'I'):\n",
    "                    glued_tokens.append([token.text.lower(), False])\n",
    "                else:\n",
    "                    glued_tokens.append([token.text, False])\n",
    "            elif (token.i == [t.i for t in doc \n",
    "                              if not t.is_punct][-1]\n",
    "                  and j != (len(sents)-1)):\n",
    "                glued_tokens.append([token.text, True])\n",
    "            elif (j != (len(sents)-1) \n",
    "                  and token.is_punct\n",
    "                  and token.i == (len(doc)-1)):\n",
    "                continue\n",
    "            else:\n",
    "                glued_tokens.append([token.text, False])\n",
    "    return glued_tokens\n",
    "\n",
    "def glue_text(text):\n",
    "    \"\"\"\n",
    "    Glue sentences in text in chunks of sizes 1, 2 and 3\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    text_len = len(sentences)\n",
    "    current_ind = 0\n",
    "    glued_tokens = []\n",
    "    while current_ind < text_len:\n",
    "        to_glue_ind = current_ind + random.choices([3, 4, 5], \n",
    "                        cum_weights=[35, 70, 100], k=1)[0]\n",
    "        if to_glue_ind >= text_len:\n",
    "            glued_tokens.append(glue_sents(sentences[current_ind:]))\n",
    "        else:\n",
    "            glued_tokens.append(glue_sents(sentences[current_ind:to_glue_ind]))\n",
    "        current_ind = to_glue_ind\n",
    "    return glued_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blog_data = []\n",
    "for text in tqdm(blogs):\n",
    "    blog_data.extend(glue_text(text))\n",
    "    \n",
    "with open('blog_data.json', 'w') as wf:\n",
    "    json.dump(blog_data, wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token_data = []\n",
    "for text in tqdm(ted['transcript']):\n",
    "    token_data.extend(glue_text(text))\n",
    "    \n",
    "with open('token_data.json', 'w') as wf:\n",
    "    json.dump(token_data, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_set = json.load(open(PATH+'ted_data.json'))\n",
    "#blog_set = json.load(open(PATH+'blog_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70273"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ted_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 477\n",
    "ted_set = random.sample(ted_set, 10000)\n",
    "#blog_set = random.sample(blog_set, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(token_data):\n",
    "    \"\"\"\n",
    "    Make a list of features and two lists of labels\n",
    "    from our data\n",
    "    \"\"\"\n",
    "    data, labels = [], []\n",
    "    wrong_count = 0\n",
    "    for token_list in tqdm(token_data):\n",
    "        doc = make_spaceable(token_list)\n",
    "        sent_len = len(doc)\n",
    "        lca_matrix = doc.get_lca_matrix()\n",
    "        if not sent_len == len(token_list):\n",
    "            continue\n",
    "        for token in doc:\n",
    "            i = token.i\n",
    "            features = {\n",
    "                'word': token.text,\n",
    "                'word_lower': token.lower_,\n",
    "                'word_lemma': token.lemma_,\n",
    "                'word_pos': token.pos_,\n",
    "                'word_dep': token.dep_,\n",
    "                'word_capitalized': True if token.is_title else False,\n",
    "                'word_ispunct': token.is_punct,\n",
    "                'word_n_lefts': token.n_lefts,\n",
    "                'word_n_rights': token.n_rights,\n",
    "                'word_idx': token.idx,\n",
    "                'length': len(token.text)\n",
    "            }\n",
    "            if i > 0:\n",
    "                features.update({\n",
    "                    'word-1': doc[i-1].lower_,\n",
    "                    'w-1pos': doc[i-1].pos_,\n",
    "                    'w-1dep': doc[i-1].dep_,\n",
    "                    'word-1_capitalized': True if doc[i-1].is_title else False,\n",
    "                    'word-1_ispunct': True if doc[-1].is_punct else False\n",
    "                })\n",
    "            if i > 1:\n",
    "                features.update({\n",
    "                    'word-2': doc[i-2].lower_,\n",
    "                    'w-2dep': doc[i-2].dep_,\n",
    "                    'left_bigram': doc[i-2].lower_ + '_' + doc[i-1].lower_\n",
    "                })\n",
    "            if i < sent_len - 1:\n",
    "                lca = lca_matrix[i, i+1]\n",
    "                features.update({\n",
    "                    'word+1': doc[i+1].lower_,\n",
    "                    'w+1pos': doc[i+1].pos_,\n",
    "                    'w+1dep': doc[i+1].dep_,\n",
    "                    'w+1_lca': lca if lca > -1 else 100,\n",
    "                    'word+1_capitalized': True if doc[i+1].is_title else False,\n",
    "                    'word+1_ispunct': True if doc[i+1].is_punct else False,\n",
    "                    'word+1_n_rights': token.n_rights\n",
    "                })\n",
    "            if i < sent_len - 2:\n",
    "                lca = lca_matrix[i, i+2]\n",
    "                features.update({\n",
    "                    'word+2': doc[i+2].lower_,\n",
    "                    'w+2dep': doc[i+2].dep_,\n",
    "                    'w+2_lca': lca if lca > -1 else 100,\n",
    "                    'right_bigram': (doc[i+1].lower_ + '_' + doc[i+2].lower_)\n",
    "                })\n",
    "            last_lab = token_list[i][1]\n",
    "            labels.append(last_lab)\n",
    "            data.append(features)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:30<00:00, 25.59it/s]\n"
     ]
    }
   ],
   "source": [
    "data, last_labels = get_features(ted_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 102.00it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set_features, test_lasts = get_features(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, last_labels, test_size=0.2, random_state=477)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "train_features = vec.fit_transform(train_data)\n",
    "test_features = vec.transform(test_data)\n",
    "test_test_features = vec.transform(test_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l1')\n",
    "clf.fit(X=train_features, y=train_labels)\n",
    "pred1 = clf.predict(test_features)\n",
    "pred2 = clf.predict(test_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.989     0.996     0.993    159072\n",
      "       True      0.875     0.708     0.783      5935\n",
      "\n",
      "avg / total      0.985     0.986     0.985    165007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, pred1, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.991     0.992     0.991      4542\n",
      "       True      0.747     0.723     0.734       155\n",
      "\n",
      "avg / total      0.982     0.983     0.983      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_lasts, pred2, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      0.99      0.99     79497\n",
      "       True       0.68      0.47      0.56      2960\n",
      "\n",
      "avg / total       0.97      0.97      0.97     82457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf2 = SGDClassifier(penalty='l2', loss='log', max_iter=1000, tol=1e-3)\n",
    "clf2.fit(X=train_features, y=train_labels)\n",
    "pred3 = clf2.predict(test_features)\n",
    "print(classification_report(test_labels, pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4 = clf2.predict(test_test_features)\n",
    "print(classification_report(test_lasts, pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер спробувати CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_full_data = [get_features([sent]) for sent in ted_set[:5000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_data = [data for (data, labels) in sent_full_data]\n",
    "sent_labels = [labels for (data, labels) in sent_full_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(list_of_bools): return [str(b) for b in list_of_bools]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "sent_labels = [to_str(l) for l in sent_labels]\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(sent_data, sent_labels, test_size=0.2)\n",
    "test_labels = flatten(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.3, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.3,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = crf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      1.00      0.99     77277\n",
      "       True       0.84      0.71      0.77      2931\n",
      "\n",
      "avg / total       0.98      0.98      0.98     80208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred3 = flatten(pred3)\n",
    "print(classification_report(test_labels, pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99      4542\n",
      "       True       0.71      0.69      0.70       155\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_full_features = [get_features([sent]) for sent in test_set]\n",
    "test_test_data = [data for (data, labels) in test_full_features]\n",
    "test_test_labels = [labels for (data, labels) in test_full_features]\n",
    "test_test_labels = [to_str(l) for l in test_test_labels]\n",
    "pred4 = crf.predict(test_test_data)\n",
    "pred4 = flatten(pred4)\n",
    "test_test_labels = flatten(test_test_labels)\n",
    "assert len(pred4) == len(test_test_labels)\n",
    "print(classification_report(test_test_labels, pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут спроби зробити збалансовані дані:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zipped = np.array(list(zip(data, last_labels)))\n",
    "data_t = data_zipped[np.array(last_labels)]\n",
    "data_f = data_zipped[np.invert(last_labels)]\n",
    "data_f_sample = random.sample(list(data_f), k=len(data_t))\n",
    "data_sampled = random.sample(list(data_t) + list(data_f), k=2*len(data_t))\n",
    "data_balanced = [x for (x,y) in data_sampled]\n",
    "labels_balanced = [y for (x,y) in data_sampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data_balanced, labels_balanced, test_size=0.3)\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "train_features = vec.fit_transform(train_data)\n",
    "test_features = vec.transform(test_data)\n",
    "test_test_features = vec.transform(test_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X=train_features, y=train_labels)\n",
    "pred1 = clf.predict(test_features)\n",
    "pred2 = clf.predict(test_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      1.00      0.99      4653\n",
      "       True       0.93      0.39      0.55        99\n",
      "\n",
      "avg / total       0.99      0.99      0.98      4752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      1.00      0.99      4542\n",
      "       True       0.84      0.40      0.54       155\n",
      "\n",
      "avg / total       0.98      0.98      0.97      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_lasts, pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут апсемплінг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "train_true = np.array(train_data)[train_labels]\n",
    "train_false = np.array(train_data)[np.invert(train_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_false_labels = np.array(train_labels)[np.invert(train_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true = resample(train_true, n_samples=600000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true_labels = np.full((600000,), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = resample(list(zip(train_true, train_true_labels)) + list(zip(train_false, train_false_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [x[1] for x in train_data]\n",
    "train_data = [x[0] for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "train_features = vec.fit_transform(train_data)\n",
    "test_features = vec.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_test is our 200 test sentences\n",
    "test_test_features = vec.transform(test_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=477, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "clf = LogisticRegression(penalty='l1', random_state=477)\n",
    "clf.fit(X=train_features, y=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = clf.predict(test_features)\n",
    "pred2 = clf.predict(test_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.99    159254\n",
      "       True       0.63      0.84      0.72      5830\n",
      "\n",
      "avg / total       0.98      0.98      0.98    165084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.97      0.98      4542\n",
      "       True       0.48      0.83      0.61       155\n",
      "\n",
      "avg / total       0.98      0.97      0.97      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_lasts, pred2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
