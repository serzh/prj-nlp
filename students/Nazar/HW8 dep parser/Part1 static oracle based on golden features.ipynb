{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/EmilStenstrom/conllu\n",
    "#https://github.com/UniversalDependencies/UD_English-EWT\n",
    "from conllu import parse, parse_tree\n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../UD_Ukrainian-IU/uk_iu-ud-train.conllu') as f:\n",
    "    c= f.read()\n",
    "    trees= parse(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('id', 1),\n",
       "             ('form', 'У'),\n",
       "             ('lemma', 'у'),\n",
       "             ('upostag', 'ADP'),\n",
       "             ('xpostag', 'Spsl'),\n",
       "             ('feats', OrderedDict([('Case', 'Loc')])),\n",
       "             ('head', 2),\n",
       "             ('deprel', 'case'),\n",
       "             ('deps', None),\n",
       "             ('misc', OrderedDict([('Id', '0003')]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trees[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=trees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Action_type(Enum):\n",
    "    Left = 1\n",
    "    Right = 2\n",
    "    Reduse = 3\n",
    "    Shift = 4\n",
    "    \n",
    "class oracle:\n",
    "    def __init__(self, tree):\n",
    "        self.rel=[]\n",
    "        self.stack=[OrderedDict([('id', 0),('form','ROOT'),(\"head\",0),(\"lemma\",\"root\")])]\n",
    "        self.queue=tree\n",
    "        self.tree_len=len(tree)\n",
    "        self.step_processor={Action_type.Left:self.Left,\n",
    "                             Action_type.Shift:self.Shift,\n",
    "                             Action_type.Right:self.Right,\n",
    "                             Action_type.Reduse:self.Reduse}\n",
    "        \n",
    "    def Shift(self):\n",
    "        self.stack.append(self.queue.pop(0))\n",
    "        \n",
    "    def Left(self):\n",
    "        self.rel.append((self.stack.pop()['id'],self.queue[0]['id']))\n",
    "        \n",
    "    def Right(self):\n",
    "        self.rel.append((self.queue[0]['id'],self.stack[-1]['id']))\n",
    "        self.stack.append(self.queue.pop(0))\n",
    "        \n",
    "    def Reduse(self):\n",
    "        self.stack.pop()\n",
    "        \n",
    "    def do_steps(self, debug=False):\n",
    "        history_step1=-1\n",
    "        histiry_step2=-1\n",
    "        self.features =[]\n",
    "        self.labels = []\n",
    "        self.dep = []\n",
    "        while self.stack or self.queue:\n",
    "            top_stack = self.stack[-1] if len(self.stack) > 0 else None\n",
    "            first_queue =  self.queue[0] if len(self.queue) > 0 else None\n",
    "            step_type=self.get_action(top_stack,first_queue)\n",
    "            self.labels.append(step_type)\n",
    "            if (step_type==Action_type.Left):\n",
    "                self.dep.append(top_stack['deprel'])\n",
    "            if (step_type==Action_type.Right):\n",
    "                self.dep.append(first_queue['deprel'])\n",
    "            else:\n",
    "                self.dep.append('-')\n",
    "\n",
    "            self.features.append(extract_features(self.stack, self.queue, self.rel,history_step1,histiry_step2,self.tree_len))\n",
    "            histiry_step2=history_step1\n",
    "            history_step1=step_type.value\n",
    "            if debug:\n",
    "                self.info(step_type)\n",
    "            self.step_processor[step_type]()\n",
    "        return self.rel\n",
    "    \n",
    "    def get_action(self, stack,queue):\n",
    "        if stack and not queue:\n",
    "            return Action_type.Reduse\n",
    "        if stack['head']==queue['id']:\n",
    "             return Action_type.Left\n",
    "        if stack['id']==queue['head']:\n",
    "             return Action_type.Right\n",
    "            \n",
    "        if stack['id'] in  [i[0] for i in self.rel] and stack[\"id\"] > queue[\"head\"]:\n",
    "             return Action_type.Reduse\n",
    "        else:\n",
    "            return Action_type.Shift\n",
    "    \n",
    "    def info(self, step_type):\n",
    "        print('step: ', step_type)\n",
    "        print('stack',  self.print_item(self.stack))\n",
    "        print('queue',  self.print_item(self.queue))\n",
    "        print('rel',  self.rel)\n",
    "        print(\"====================================\")\n",
    "\n",
    "    def print_item(self, lst):\n",
    "        return [i['form'] for i in lst]\n",
    "    \n",
    "    def get_relations(tree):\n",
    "        return [(i['id'],i['head']) for i in tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('ubercorpus.lowercased.tokenized.word2vec.300d.bz2', binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate features\n",
    "def extract_features(stack, queue, rel, history_step1, histiry_step2, sentense_len):\n",
    "    #print(\"stack\",stack)\n",
    "    stack1=stack[-1] if len(stack)>0 else None\n",
    "    stack2=stack[-2] if len(stack)>1 else None\n",
    "    stack3=stack[-3] if len(stack)>2 else None\n",
    "    queue1=queue[0] if len(queue)>0 else None\n",
    "    queue2=queue[1] if len(queue)>1 else None\n",
    "    queue3=queue[2] if len(queue)>2 else None\n",
    "    queue4=queue[3] if len(queue)>4 else None\n",
    "    \n",
    "    features = dict()\n",
    "    if len(stack) > 0:\n",
    "        stack_top = stack[-1]\n",
    "        s0_features = get_word_features(stack_top,'s0_',stack2,queue1, True,  )\n",
    "    else:\n",
    "        s0_features = get_word_features(None,'s0_',stack2,queue1 , True, )\n",
    "    features = {**features, **s0_features}\n",
    "    \n",
    "    if len(stack) > 1:\n",
    "        s1_features = get_word_features(stack[-2],'s1_',stack2,queue1 , True )\n",
    "    else:\n",
    "        s1_features = get_word_features(None,'s1_',stack2,queue1, True )\n",
    "    features = {**features, **s1_features}\n",
    "    \n",
    "        \n",
    "    if queue:\n",
    "        queue_top = queue[0]\n",
    "        q0_features = get_word_features(queue_top,'q0_',stack1,queue2, True )\n",
    "    else: \n",
    "        q0_features = get_word_features(None,'q0_',stack1,queue2, True )\n",
    "    features = {**features, **q0_features}\n",
    "    \n",
    "    if len(queue)>1:\n",
    "        queue1 = queue[1]\n",
    "        q1_features = get_word_features(queue1,'q1_',stack1,queue3)\n",
    "    else: \n",
    "        q1_features = get_word_features(None,'q1_',stack1,queue3)\n",
    "    features = {**features, **q1_features}\n",
    "    \n",
    "    if len(queue)>1:\n",
    "        queue1 = queue[1]\n",
    "        q2_features = get_word_features(queue1,'q2_',stack1,queue4)\n",
    "    else: \n",
    "        q2_features = get_word_features(None,'q2_',stack1,queue4)\n",
    "    features = {**features, **q2_features}\n",
    "    \n",
    "    features['history_step1']=history_step1\n",
    "    features['histiry_step2']=histiry_step2\n",
    "\n",
    "    try:\n",
    "        features['is_first_word']=int(queue[0][\"id\"]==1)\n",
    "        features['is_last_word']=int(queue[0][\"id\"]==sentense_len)\n",
    "    except:\n",
    "        features['is_first_word']=0\n",
    "        features['is_last_word']=0\n",
    "    features['stack_len']=len(stack)\n",
    "    features['queue_len']=len(queue)\n",
    "    features['rel_len']=len(rel)\n",
    "    features['parrent_of']= sum([int(i[1]==stack_top['id']) for i in rel])\n",
    "    if stack and queue:\n",
    "        features[\"distance\"] = queue[0][\"id\"] - stack[-1][\"id\"]\n",
    "        features[\"word_index\"] = queue[0][\"id\"]\n",
    "    else:\n",
    "        features[\"distance\"]=-1\n",
    "        features[\"word_index\"]=-1\n",
    "    return features\n",
    "\n",
    "def get_word_features(node, prefix, left, right, add_embeding=False):\n",
    "    try:\n",
    "        left_tag = left['upostag']\n",
    "    except:\n",
    "        left_tag=''\n",
    "    try:\n",
    "        right_tag = right['upostag']\n",
    "    except:\n",
    "        right_tag=''\n",
    "    try:\n",
    "        node_tag = node['upostag']\n",
    "    except:\n",
    "        node_tag=''\n",
    "    try:\n",
    "        for key in node['feats']:\n",
    "            features[prefix+'feats_'+key]=node['feats']['key']\n",
    "    except:\n",
    "        rer=1+1\n",
    "\n",
    "    features = dict()\n",
    "    \n",
    "    try:\n",
    "            word_vector=model.get_vector(word_info.normal_form.lower())\n",
    "    except:\n",
    "            word_vector=[0]*300\n",
    "    for i in range(0,300):\n",
    "        features[prefix+'vector+'+str(i)]=word_vector[i]\n",
    "    if node:\n",
    "        features[prefix+'lemma']=node['lemma'].lower()\n",
    "        #features[prefix+'word']=node['form']\n",
    "    \n",
    "    features[prefix+'POS']=node_tag\n",
    "    features[prefix+'left_item']= '{0}_{1}'.format(left_tag, node_tag)\n",
    "    features[prefix+'item_right']= '{0}_{1}'.format(node_tag,right_tag)\n",
    "    features[prefix+'left_right']= '{0}_{1}'.format(left_tag, right_tag)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collect  features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_df(trees):\n",
    "    from tqdm import tqdm\n",
    "    features =[]\n",
    "    labels = []\n",
    "    dep_labels = []\n",
    "\n",
    "\n",
    "    for index in tqdm(range(0,len(trees))):\n",
    "        tree=trees[index]\n",
    "        o = oracle(tree.copy())\n",
    "        parsed_relations = o.do_steps()\n",
    "        features=features + o.features\n",
    "        labels=labels + o.labels\n",
    "        dep_labels=dep_labels+o.dep\n",
    "    \n",
    "    return features,labels,dep_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = None\n",
    "def encode_feature_df(features,vectorizer):\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=False)\n",
    "        vectorizer.fit(features)\n",
    "    return vectorizer.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4513/4513 [02:56<00:00, 25.64it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../UD_Ukrainian-IU/uk_iu-ud-train.conllu') as f:\n",
    "    c= f.read()\n",
    "    trees= parse(c)\n",
    "\n",
    "features_df,y_train, y_train_dep = get_vectors_df(trees)\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_train=vectorizer.fit_transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [i.value for i in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[col for col in features_df.columns.values if 'vector' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "815"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:32<00:00, 24.17it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../UD_Ukrainian-IU/uk_iu-ud-test.conllu') as f:\n",
    "    c= f.read()\n",
    "    trees= parse(c)\n",
    "\n",
    "features_df,y_test, y_test_dep = get_vectors_df(trees)\n",
    "X_test= vectorizer.transform(features_df)\n",
    "y_test = [i.value for i in y_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trees=trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69837"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=19, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=19, random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Left       0.82      0.89      0.85      7352\n",
      "      Right       0.76      0.78      0.77      7182\n",
      "     Reduse       0.92      0.66      0.77      8370\n",
      "      Shift       0.76      0.91      0.83      7757\n",
      "\n",
      "avg / total       0.82      0.81      0.81     30661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_test, target_names=['Left','Right','Reduse','Shift']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD - LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression( random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Left       0.94      0.97      0.96     38446\n",
      "      Right       0.91      0.92      0.91     34669\n",
      "     Reduse       0.95      0.93      0.94     41165\n",
      "      Shift       0.95      0.94      0.95     40429\n",
      "\n",
      "avg / total       0.94      0.94      0.94    154709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, predicted_test, target_names=['Left','Right','Reduse','Shift']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Left       0.90      0.93      0.92      7352\n",
      "      Right       0.80      0.82      0.81      7182\n",
      "     Reduse       0.87      0.82      0.84      8370\n",
      "      Shift       0.89      0.90      0.90      7757\n",
      "\n",
      "avg / total       0.87      0.87      0.87     30661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_test, target_names=['Left','Right','Reduse','Shift']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "dummy_y = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5649"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5649, input_dim=5649, activation='relu'))\n",
    "    model.add(Dense(2300, input_dim=2300, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=5, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "154709/154709 [==============================] - 555s 4ms/step - loss: 0.4072 - acc: 0.8547\n",
      "Epoch 2/5\n",
      "154709/154709 [==============================] - 561s 4ms/step - loss: 0.2819 - acc: 0.8876\n",
      "Epoch 3/5\n",
      "154709/154709 [==============================] - 580s 4ms/step - loss: 0.2489 - acc: 0.8997\n",
      "Epoch 4/5\n",
      "154709/154709 [==============================] - 591s 4ms/step - loss: 0.2236 - acc: 0.9083\n",
      "Epoch 5/5\n",
      "154709/154709 [==============================] - 584s 4ms/step - loss: 0.2013 - acc: 0.9172\n"
     ]
    }
   ],
   "source": [
    "history = estimator.fit( X_train, dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30661/30661 [==============================] - 21s 693us/step\n"
     ]
    }
   ],
   "source": [
    "predicted = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Left       0.92      0.92      0.92      7352\n",
      "      Right       0.84      0.78      0.81      7182\n",
      "     Reduse       0.84      0.87      0.85      8370\n",
      "      Shift       0.89      0.92      0.90      7757\n",
      "\n",
      "avg / total       0.87      0.87      0.87     30661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predicted, target_names=['Left','Right','Reduse','Shift']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=0.5,\n",
       "         max_samples=0.5, n_estimators=10, n_jobs=8, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging = BaggingClassifier(LogisticRegression(),max_samples=0.5, max_features=0.5, n_jobs=8)\n",
    "bagging.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = bagging.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Left       0.89      0.94      0.92     38446\n",
      "      Right       0.84      0.86      0.85     34669\n",
      "     Reduse       0.92      0.84      0.88     41165\n",
      "      Shift       0.90      0.92      0.91     40429\n",
      "\n",
      "avg / total       0.89      0.89      0.89    154709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, predicted_test, target_names=['Left','Right','Reduse','Shift']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = bagging.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Left       0.88      0.93      0.90      7352\n",
      "      Right       0.79      0.83      0.81      7182\n",
      "     Reduse       0.89      0.77      0.83      8370\n",
      "      Shift       0.87      0.90      0.89      7757\n",
      "\n",
      "avg / total       0.86      0.86      0.86     30661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_test, target_names=['Left','Right','Reduse','Shift']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression beter than SGD on 0.06\n",
    "#LogisticRegression using words cause overfiting  (0.09) score on validation 0.87\n",
    "#LogisticRegression using word emedings overfiting (0.02) but score also decrese on 0.02, score on validation 0.85\n",
    "#NN using word emedings  score on validation 0.87, but\n",
    "#BB on LogisticRegression 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04088878,  0.10514956,  0.64145491, ...,  0.06549302,\n",
       "       -0.30445747,  0.66288475])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s0_vector+22', 's0_left_item=VERB_PROPN', 's0_vector+100',\n",
       "       's1_item_right=SCONJ_PART', 's0_left_item=ADV_PRON', 's1_vector+2',\n",
       "       's1_left_right=VERB_INTJ', 's0_vector+14', 's0_left_item=ADV_ADP',\n",
       "       's0_left_right=_ADP', 'q1_left_item=ADV_ADP',\n",
       "       'q0_left_right=SCONJ_PROPN', 's1_item_right=PART_ADJ',\n",
       "       'q2_left_right=VERB_AUX', 's1_item_right=PART_NUM',\n",
       "       'q1_left_right=ADJ_DET', 's1_vector+3', 'q2_item_right=ADV_',\n",
       "       's0_vector+38', 'q2_left_right=ADJ_ADV', 'q1_left_item=PRON_AUX',\n",
       "       'q2_left_right=X_ADP', 's1_vector+123', 'q1_left_item=INTJ_VERB',\n",
       "       's1_item_right=ADJ_PRON', 's0_vector+23', 'q2_vector+146',\n",
       "       's0_vector+18', 's0_vector+129', 'q0_left_right=VERB_',\n",
       "       's0_vector+7', 'q2_vector+56', 's0_vector+27', 's0_vector+57',\n",
       "       's1_left_right=VERB_', 's0_left_item=AUX_NOUN',\n",
       "       'q1_left_item=PRON_PART', 'q2_vector+265', 's1_item_right=_ADP',\n",
       "       's0_item_right=AUX_PUNCT'], dtype='<U25')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array( list(vectorizer.vocabulary_.keys()))[clf.coef_[0].argsort()[::-1][:40]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the unlabeled attachment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_parse(sentence, model):\n",
    "    stack, queue, relations = [OrderedDict([('id', 0),('form','ROOT'),(\"head\",0),(\"lemma\",\"root\")])], sentence[:], []\n",
    "    history_step1=-1\n",
    "    history_step2=-1\n",
    "    while queue or stack:\n",
    "        if stack and not queue:\n",
    "            stack.pop()\n",
    "        else:\n",
    "            features = extract_features(stack, queue, relations,history_step1, history_step2, len(sentence))\n",
    "            features = vectorizer.transform([features])\n",
    "            action = clf.predict(features)[0]\n",
    "            # actual parsing\n",
    "            if action == 4: #:Action_type.Shift:\n",
    "                stack.append(queue.pop(0))\n",
    "            elif action == 3: #:Action_type.Reduse:\n",
    "                stack.pop()\n",
    "            elif action == 1: #Action_type.Left:\n",
    "                relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "                stack.pop()\n",
    "            elif action == 2: #Action_type.Right:\n",
    "                relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "                stack.append(queue.pop(0))\n",
    "            else:\n",
    "                print(\"Unknown action.\", action)\n",
    "            histiry_step2=history_step1\n",
    "            history_step1=action\n",
    "    return sorted(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Продавши', 'свій', 'шедевр', 'Меценатові', ',', 'еллінський', 'скульптор', 'споневажив', 'саме', 'мистецтво', ':', 'Ти', 'не', 'продався', ',', '—', 'гірше', '!']\n",
      "[(1, 0), (2, 3), (3, 1), (4, 3), (5, 8), (6, 7), (7, 8), (8, 1), (9, 10), (10, 8), (11, 14), (12, 14), (13, 14), (14, 1), (15, 1), (16, 17), (17, 1), (18, 1)]\n",
      "[(1, 8), (2, 3), (3, 1), (4, 1), (5, 1), (6, 7), (7, 8), (8, 0), (9, 10), (10, 8), (11, 14), (12, 14), (13, 14), (14, 8), (15, 17), (16, 17), (17, 14), (18, 14)]\n"
     ]
    }
   ],
   "source": [
    "print([node[\"form\"] for node in test_trees[1]])\n",
    "print(dep_parse(test_trees[1], clf))\n",
    "print([(node[\"id\"], node[\"head\"]) for node in test_trees[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/783 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 2/783 [00:00<00:47, 16.36it/s]\u001b[A\n",
      "  1%|          | 4/783 [00:00<00:57, 13.57it/s]\u001b[A\n",
      "  1%|          | 5/783 [00:00<01:16, 10.11it/s]\u001b[A\n",
      "  1%|          | 6/783 [00:00<01:22,  9.38it/s]\u001b[A\n",
      "  1%|          | 8/783 [00:00<01:22,  9.43it/s]\u001b[A\n",
      "  1%|          | 9/783 [00:01<01:27,  8.82it/s]\u001b[A\n",
      "  1%|▏         | 10/783 [00:01<01:30,  8.53it/s]\u001b[A\n",
      "  1%|▏         | 11/783 [00:01<01:34,  8.16it/s]\u001b[A\n",
      "  2%|▏         | 12/783 [00:01<01:52,  6.86it/s]\u001b[A\n",
      "  2%|▏         | 14/783 [00:01<01:47,  7.15it/s]\u001b[A\n",
      "  2%|▏         | 16/783 [00:02<01:40,  7.63it/s]\u001b[A\n",
      "  2%|▏         | 19/783 [00:02<01:28,  8.63it/s]\u001b[A\n",
      "  3%|▎         | 21/783 [00:02<01:23,  9.11it/s]\u001b[A\n",
      "  3%|▎         | 24/783 [00:02<01:17,  9.85it/s]\u001b[A\n",
      "  3%|▎         | 27/783 [00:02<01:12, 10.37it/s]\u001b[A\n",
      "  4%|▎         | 29/783 [00:02<01:16,  9.86it/s]\u001b[A\n",
      "  4%|▍         | 31/783 [00:03<01:16,  9.86it/s]\u001b[A\n",
      "  4%|▍         | 33/783 [00:03<01:23,  8.95it/s]\u001b[A\n",
      "  4%|▍         | 35/783 [00:04<01:26,  8.62it/s]\u001b[A\n",
      "  5%|▍         | 38/783 [00:04<01:23,  8.94it/s]\u001b[A\n",
      "  5%|▌         | 41/783 [00:04<01:19,  9.31it/s]\u001b[A\n",
      "Exception in thread Thread-122:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 783/783 [01:02<00:00, 12.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 14939\n",
      "Correctly defined: 11055\n",
      "UAS: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "total, tp = 0, 0\n",
    "for index in tqdm(range(0,len(test_trees))):\n",
    "    tree=test_trees[index]\n",
    "    golden = [(node[\"id\"], node[\"head\"]) for node in tree]\n",
    "    predicted = dep_parse(tree, clf)\n",
    "    total += len(tree)\n",
    "    tp += len(set(golden).intersection(set(predicted)))\n",
    "\n",
    "print(\"Total:\", total)\n",
    "print(\"Correctly defined:\", tp)\n",
    "print(\"UAS:\", round(tp/total, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UAS: 0.63 - on NN + VE\n",
    "#UAS: 0.72 - on LR + VE\n",
    "#UAS: 0.74 - on LR + VE + word\n",
    "#UAS: 0.75 - on LR + word "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
