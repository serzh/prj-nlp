{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import bz2\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import random\n",
    "from itertools import groupby, chain, tee\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import nltk.corpus.reader.bnc\n",
    "from spacy.tokens import Doc\n",
    "from time import time\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport config\n",
    "from config import REPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_val = \"run-on-test.json\"\n",
    "with open(file_val) as f:\n",
    "    val_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_true(x):\n",
    "    return sum([el[-1] for el in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_counter(counter, name):\n",
    "    for key, value in counter.most_common():\n",
    "        print(f\"{name}: {key}\\ncount: {value}, percentage: {value*100/sum(counter.values()):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run-ons: 1\n",
      "count: 145, percentage: 72.5%\n",
      "Run-ons: 0\n",
      "count: 50, percentage: 25.0%\n",
      "Run-ons: 2\n",
      "count: 5, percentage: 2.5%\n"
     ]
    }
   ],
   "source": [
    "frq = Counter([sum_true(el) for el in val_data])\n",
    "print_counter(frq, \"Run-ons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(data):\n",
    "    output = []\n",
    "    for k,record in enumerate(data):\n",
    "        temp = [{\"id\": k, \"word\": word, \"label\": label} for word, label in record]\n",
    "        output.extend(temp)\n",
    "    return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>But</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>then</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>120</td>\n",
       "      <td>True</td>\n",
       "      <td>started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>wails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>cries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>audience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865</th>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  label      word\n",
       "2853  120  False       But\n",
       "2854  120  False      then\n",
       "2855  120  False        it\n",
       "2856  120   True   started\n",
       "2857  120  False     there\n",
       "2858  120  False      were\n",
       "2859  120  False     wails\n",
       "2860  120  False       and\n",
       "2861  120  False     cries\n",
       "2862  120  False      from\n",
       "2863  120  False       the\n",
       "2864  120  False  audience\n",
       "2865  120  False         ."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = build_df(val_data)\n",
    "val_df.loc[val_df.id==120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    96.700021\n",
       "True      3.299979\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class imbalance\n",
    "val_df.label.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training data. Use Reddit posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = REPO / \"cmv\" / \"all\"\n",
    "filename = \"heldout_period_data.jsonlist.bz2\"\n",
    "with bz2.open(folder / filename, mode=\"rt\") as f:\n",
    "    lines = f.readlines()\n",
    "    data = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_comment(comment):\n",
    "    text = comment.get(\"body\", \"\")\n",
    "    patt = r\"Confirmed:.*awarded.*|This delta is currently disallowed.*|You cannot award OP a delta as.*\"\n",
    "    match = re.search(patt, text)\n",
    "    if not text or text == \"[deleted]\" or match:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_replacement(m):\n",
    "    if not m.group(1):\n",
    "        return \". \"\n",
    "    else:\n",
    "        return m.group(1)+\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRUBER_URLINTEXT_PAT = re.compile(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEB_URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comment(comment, patt_code=r\"&amp;#(\\d+);\"):\n",
    "    text = comment.get(\"body\", \"\")\n",
    "    text = re.sub(r\"\\[(.*?)\\](\\s*)\\((http(s?):/)?/.*?\\)\", r\"\\1\", text)\n",
    "    text = re.sub(r\"([.!?:])?\\s*[\\n|\\r]+\\s*\", custom_replacement, text)\n",
    "    if re.search(patt_code, text):\n",
    "        text = re.sub(patt_code, lambda x: chr(int(x.group(1), 10)), text)\n",
    "    text = re.sub(r\"&gt;|#(\\d+);(\\.)?|&lt;|&amp;nbsp;(\\.)?\", \"\", text)\n",
    "    text = re.sub(r\"&amp;\", \"and\", text)\n",
    "    text = re.sub(WEB_URL_REGEX, \"\", text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"(\\s)+\", r\"\\1\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "k = 0\n",
    "N = 10000\n",
    "for el in data:\n",
    "    for comment in el[\"comments\"]:\n",
    "        if k > N:\n",
    "            break\n",
    "        if check_comment(comment):\n",
    "            comments.append(process_comment(comment)+\"\\n\")\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\", \"w+\") as f:\n",
    "    f.writelines(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate N grams based on Reddit comments (accross sentence boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\", disable=[\"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to estimate N grams\n",
    "ngr = []\n",
    "k = 0\n",
    "for el in data:\n",
    "    for comment in el[\"comments\"]:\n",
    "        if check_comment(comment):\n",
    "            ngr.append(process_comment(comment)+\"\\n\")\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipngram(doc, n=2):\n",
    "    return zip(*[doc[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891.1918799877167\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "NGRAMS = defaultdict(Counter)\n",
    "docs = nlp.pipe(ngr[:50000], disable=[\"ner\", \"textcat\"], batch_size=10 ** 3, n_threads=4)\n",
    "for doc in docs:\n",
    "    text = [tok.lemma_ if tok.lemma_ != '-PRON-' else tok.lower_ for tok in doc]\n",
    "    NGRAMS[1].update(text)\n",
    "    NGRAMS[2].update(zipngram(text)) \n",
    "    NGRAMS[3].update(zipngram(text, 3)) \n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (REPO / \"ngrams.dill\").open(\"wb+\") as f:\n",
    "    dill.dump(NGRAMS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (REPO / \"ngrams.dill\").open(\"rb\") as f:\n",
    "    NGRAMS = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Run-on sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\", \"r\") as f:\n",
    "    comments = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 3\n",
    "SENT_LIMIT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indices(sentences):\n",
    "    idx = []\n",
    "    k = 0\n",
    "    counter = 0\n",
    "    for i in range(len(sentences)):\n",
    "        if random.random() <= 0.19 and not counter:\n",
    "            k += 1\n",
    "            idx.append(k)\n",
    "        else:\n",
    "            if not counter:\n",
    "                k += 1\n",
    "            idx.append(k)\n",
    "            counter += 1\n",
    "            if random.random() <= 0.04 and counter < LIMIT:\n",
    "                continue\n",
    "            elif counter > 1:\n",
    "                counter = 0\n",
    "                k += 1\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group(group):\n",
    "    output = []\n",
    "    temp = []\n",
    "    for k,sent in enumerate(group):\n",
    "        if k < len(group)-1:\n",
    "            if sent[-1].pos_ == \"PUNCT\":\n",
    "                sent = sent[:-1]           \n",
    "            tokens = [tok.text_with_ws for tok in sent]\n",
    "            if len(sent[-1].text) == len(sent[-1].text_with_ws):\n",
    "                tokens[-1] = tokens[-1] + \" \"\n",
    "            if temp and random.random() <= 0.5:\n",
    "                tokens[0] = tokens[0].lower()\n",
    "            labels = np.zeros(len(tokens), dtype=bool)\n",
    "            labels[-1] = True\n",
    "            temp.extend(zip(tokens, labels))\n",
    "        else:\n",
    "            tokens = [tok.text_with_ws for tok in sent]\n",
    "            if temp and random.random() <= 0.5:\n",
    "                tokens[0] = tokens[0].lower()\n",
    "            labels = np.zeros(len(tokens), dtype=bool)\n",
    "            temp.extend(zip(tokens, labels))\n",
    "            output.append(temp)\n",
    "            temp = []\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_comment(comment, limit=SENT_LIMIT):\n",
    "    doc = nlp(comment)\n",
    "    sentences = [sent for sent in doc.sents if len(sent)>=limit]\n",
    "    examples = []\n",
    "    if len(sentences) < 3:\n",
    "        idx = [0] * len(sentences)\n",
    "    else:\n",
    "        idx = generate_indices(sentences)\n",
    "    for key, g in groupby(zip(idx, sentences), key=lambda x: x[0]):\n",
    "        _, group = map(list, zip(*g))\n",
    "        try:\n",
    "            temp = process_group(group)\n",
    "        except:\n",
    "            continue\n",
    "        examples.extend(temp)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i, c in enumerate(comments):\n",
    "    temp = get_data_from_comment(c)\n",
    "    train_data.extend(temp)\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Finished {i+1} out of {len(comments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run-ons: 1\n",
      "frq: 22656, percentage: 66.5%\n",
      "Run-ons: 0\n",
      "frq: 10604, percentage: 31.1%\n",
      "Run-ons: 2\n",
      "frq: 789, percentage: 2.3%\n"
     ]
    }
   ],
   "source": [
    "frq = Counter([sum_true(el) for el in train_data])\n",
    "print_counter(frq, \"Run-ons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = build_df(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df.id==53];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    97.845464\n",
       "True      2.154536\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class imbalance\n",
    "train_df.label.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subsample train data (remove non run-on sentences, there are too much of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = train_df.groupby(\"id\")[\"label\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = s[s==0].sample(frac=0.25).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = train_df.loc[~train_df.id.isin(exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    97.739146\n",
       "True      2.260854\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class imbalance\n",
    "sample_df.label.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071896, 1124790)"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_df), len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    72.2\n",
       "0.0    25.3\n",
       "2.0     2.5\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 819,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.groupby(\"id\")[\"label\"].sum().value_counts(normalize=True).map(lambda x: round(x, 3) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(REPO / \"train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_test, pred, proba=None, labels=None, print_=True, mode=\"weighted\"):\n",
    "    output = {}\n",
    "    if proba is not None:\n",
    "        roc_auc = metrics.roc_auc_score(y_test, proba)\n",
    "        output[\"AUC\"] = roc_auc\n",
    "    output[\"Recall\"] = metrics.recall_score(y_test, pred, average=mode)\n",
    "    output[\"Precision\"] = metrics.precision_score(y_test, pred, average=mode)\n",
    "    output[\"F1\"] = metrics.f1_score(y_test, pred, average=mode)\n",
    "    output[\"accuracy\"] = metrics.accuracy_score(y_test, pred)\n",
    "    if labels is not None:\n",
    "        index = labels\n",
    "        columns = [\"pred_\" + str(el) for el in index]\n",
    "    else:\n",
    "        columns = None\n",
    "        index = None\n",
    "    output[\"conf_matrix\"] = pd.DataFrame(metrics.confusion_matrix(y_test, pred, labels=labels), \n",
    "                                         columns=columns, index=index)\n",
    "    report = metrics.classification_report(y_true=y_test, y_pred=pred, labels=labels)\n",
    "    if print_:\n",
    "        for key, value in output.items():\n",
    "            if \"matrix\" in key:\n",
    "                print(value)\n",
    "            else:\n",
    "                print(f\"{key}: {value:0.3f}\")\n",
    "        print(report)\n",
    "    return output, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(REPO / \"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[\"word\"] = sample_df[\"word\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross - validation / Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_num = sample_df.groupby(\"id\")[\"label\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "# for train_index, test_index in skf.split(id_num.index, id_num.values):\n",
    "#     train_groups = id_num.index[train_index]\n",
    "#     test_groups = id_num.index[test_index]\n",
    "#     train_df = sample_df.loc[sample_df.id.isin(train_groups)]\n",
    "#     test_df = sample_df.loc[sample_df.id.isin(test_groups)]\n",
    "#     print(train_df.groupby(\"id\")[\"label\"].sum().value_counts(normalize=True))\n",
    "#     print(test_df.groupby(\"id\")[\"label\"].sum().value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    0.721585\n",
      "0.0    0.253299\n",
      "2.0    0.025116\n",
      "Name: label, dtype: float64\n",
      "1.0    0.721550\n",
      "0.0    0.253291\n",
      "2.0    0.025159\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_sentences, test_sentences = train_test_split(id_num.index, train_size=0.7, test_size=0.3, \n",
    "                                                   stratify=id_num.values, random_state=25)\n",
    "train = sample_df.loc[sample_df.id.isin(train_sentences)]\n",
    "test = sample_df.loc[sample_df.id.isin(test_sentences)]\n",
    "print(train.groupby(\"id\")[\"label\"].sum().value_counts(normalize=1))\n",
    "print(test.groupby(\"id\")[\"label\"].sum().value_counts(normalize=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer(object):\n",
    "    \"\"\"\n",
    "    Custom Tokenizer\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab=nlp.vocab, tokenizer=None, return_doc=True):\n",
    "        self.vocab = vocab\n",
    "        self._word_tokenizer = tokenizer\n",
    "        self.return_doc = return_doc\n",
    "\n",
    "    def __call__(self, text):\n",
    "        if self._word_tokenizer:\n",
    "            words = self._word_tokenizer.tokenize(text)\n",
    "        else:\n",
    "            words = text.split(' ')\n",
    "        if self.return_doc:\n",
    "            spaces = [True] * len(words)\n",
    "            return Doc(self.vocab, words=words, spaces=spaces)\n",
    "        else:\n",
    "            return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i, n=2):\n",
    "    word = sent[i]\n",
    "    lemma = word.lemma_\n",
    "    features = {\n",
    "        'word.lemma': lemma,\n",
    "        #'word_frq': word_frq.get(lemma, 1),\n",
    "        'word.pos': word.pos_,\n",
    "        'word.shape': word.shape_,\n",
    "        'word.isupper': word.is_upper,\n",
    "        'word.istitle': word.is_title,\n",
    "        'word.isdigit': word.is_digit,\n",
    "        'word.isalpha': word.is_alpha,\n",
    "        'word.isbracket': word.is_bracket,\n",
    "        'word.isleftpunct': word.is_left_punct,\n",
    "        'word.ispunct': word.is_punct,\n",
    "        'word.isquote': word.is_quote,\n",
    "        'word.isspace': word.is_space,\n",
    "        'word.isstop': word.is_stop\n",
    "    }\n",
    "    #features[\"start_word\"] = start_gram[1].get(lemma, 0) / word_frq.get(lemma, 1)\n",
    "    features[\"word_dot\"] = NGRAMS[2].get((lemma, \".\"), 0) / NGRAMS[1].get(lemma, 1)\n",
    "    if i > 0:\n",
    "        prefix = \"word-1\"\n",
    "        word1 = sent[i-1]\n",
    "        lemma1 = word1.lemma_\n",
    "        #features[f\"{prefix}_frq\"] = word_frq.get(lemma1, 1)\n",
    "        #features[f\"l_{n}_gram\"] = ngrams.get((lemma1, lemma), 0)\n",
    "        features[f\"prob_l_{n}_gram\"] = NGRAMS[2].get((lemma1, lemma), 0) / NGRAMS[1].get(lemma1, 1)#features[f\"{prefix}_frq\"]\n",
    "        features[f\"prob_l_{n}_gram_dot\"] = NGRAMS[3].get((lemma1, lemma, \".\"), 0) / NGRAMS[2].get((lemma1, lemma), 1)\n",
    "        #features[f\"start_l_{n}_gram\"] = start_gram[2].get((lemma1, lemma), 0) / ngrams.get((lemma1, lemma), 1)\n",
    "        #features[f\"end_l_{n}_gram\"] = end_gram[2].get((lemma1, lemma), 0) / ngrams.get((lemma1, lemma), 1)\n",
    "        features.update({\n",
    "            f\"{prefix}.lemma\": lemma1,\n",
    "            f\"{prefix}.pos\": word1.pos_,\n",
    "            f\"{prefix}.shape\": word1.shape_,\n",
    "            f\"{prefix}.istitle\": word1.is_title,\n",
    "            f\"{prefix}.isdigit\": word1.is_digit,\n",
    "            f\"{prefix}.isalpha\": word.is_alpha,\n",
    "            f\"{prefix}.isbracket\": word1.is_bracket,\n",
    "            f\"{prefix}.isleftpunct\": word1.is_left_punct,\n",
    "            f\"{prefix}.ispunct\": word1.is_punct,\n",
    "            f\"{prefix}.isquote\": word1.is_quote,\n",
    "            f\"{prefix}.isspace\": word1.is_space,\n",
    "            f\"{prefix}.isstop\": word1.is_stop\n",
    "        })\n",
    "    else:\n",
    "        features[\"BOS\"] = True\n",
    "    \n",
    "    if i < len(sent) - 1:\n",
    "        prefix = \"word+1\"\n",
    "        word1 = sent[i+1]\n",
    "        lemma1 = word1.lemma_\n",
    "        #features[f\"{prefix}_frq\"] = word_frq.get(lemma1, 1)\n",
    "        #features[f\"r_{n}_gram\"] = ngrams.get((lemma, lemma1), 0)\n",
    "        features[f\"prob_r_{n}_gram\"] = NGRAMS[2].get((lemma, lemma1), 0) / NGRAMS[1].get(lemma, 1)#features[f\"word_frq\"]\n",
    "        #features[f\"start_r_{n}_gram\"] = start_gram[2].get((lemma, lemma1), 0) / ngrams.get((lemma, lemma1), 1)\n",
    "        #features[f\"end_r_{n}_gram\"] = end_gram[2].get((lemma, lemma1), 0) / ngrams.get((lemma, lemma1), 1)\n",
    "        features.update({\n",
    "            f\"{prefix}.lemma\": lemma1,\n",
    "            f\"{prefix}.pos\": word1.pos_,\n",
    "            f\"{prefix}.shape\": word1.shape_,\n",
    "            f\"{prefix}.istitle\": word1.is_title,\n",
    "            f\"{prefix}.isdigit\": word1.is_digit,\n",
    "            f\"{prefix}.isalpha\": word.is_alpha,\n",
    "            f\"{prefix}.isbracket\": word1.is_bracket,\n",
    "            f\"{prefix}.isleftpunct\": word1.is_left_punct,\n",
    "            f\"{prefix}.ispunct\": word1.is_punct,\n",
    "            f\"{prefix}.isquote\": word1.is_quote,\n",
    "            f\"{prefix}.isspace\": word1.is_space,\n",
    "            f\"{prefix}.isstop\": word1.is_stop\n",
    "        })\n",
    "    else:\n",
    "        features[\"EOS\"] = True\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prep'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = \"\".join(test.loc[test.id==13, \"word\"].values)\n",
    "doc = nlp(w)\n",
    "t = doc[0]\n",
    "t.dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2features(doc, 22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(df, n_threads=4, batch_size=1000, is_val=False):\n",
    "    sent_labels = ((key, gr[\"word\"].values, gr[\"label\"].values) for key, gr in df.groupby(\"id\"))\n",
    "    gen1, gen2, gen3 = tee(sent_labels, 3)\n",
    "    ids = (key for (key, _, _) in gen1)\n",
    "    sep = \" \" if is_val else \"\"\n",
    "    sents = (sep.join(words) for (_, words, _) in gen2)\n",
    "    lengths = (len(words) for (_, words, _) in gen3)\n",
    "    docs = nlp.pipe(sents, disable=[\"ner\", \"textcat\"], batch_size=batch_size, n_threads=n_threads)\n",
    "    output = []\n",
    "    exclude_ids = []\n",
    "    for id_, doc, length in zip(ids, docs, lengths):\n",
    "        if len(doc) != length:\n",
    "                exclude_ids.append(id_)\n",
    "                continue\n",
    "        features = sent2features(doc)\n",
    "        output.append(features) \n",
    "    return output, exclude_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, exclude_id_train = build_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, exclude_id_test = build_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = WordTokenizer(nlp.vocab)\n",
    "val_features, exclude_id_val = build_features(val_df, is_val=True)\n",
    "nlp.tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 37, 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exclude_id_test), len(exclude_id_train), len(exclude_id_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = DictVectorizer(sparse=True)\n",
    "train_v = v.fit_transform(chain(*train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v = v.transform(chain(*test_features))\n",
    "val_v = v.transform(chain(*val_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.loc[~train.id.isin(exclude_id_train), \"label\"]\n",
    "y_test = test.loc[~test.id.isin(exclude_id_test), \"label\"]\n",
    "y_val = val_df.loc[~val_df.id.isin(exclude_id_val), \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight=\"balanced\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karimlulu/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.945\n",
      "Precision: 0.980\n",
      "F1: 0.958\n",
      "accuracy: 0.945\n",
      "       pred_False  pred_True\n",
      "False      296619      16755\n",
      "True          960       6303\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.95      0.97    313374\n",
      "       True       0.27      0.87      0.42      7263\n",
      "\n",
      "avg / total       0.98      0.94      0.96    320637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(test_v)\n",
    "labels = clf.classes_\n",
    "output, report = calc_metrics(pred=y_pred, y_test=y_test, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.947\n",
      "Precision: 0.971\n",
      "F1: 0.956\n",
      "accuracy: 0.947\n",
      "       pred_False  pred_True\n",
      "False        4327        215\n",
      "True           35        120\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.95      0.97      4542\n",
      "       True       0.36      0.77      0.49       155\n",
      "\n",
      "avg / total       0.97      0.95      0.96      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_val = clf.predict(val_v)\n",
    "labels = clf.classes_\n",
    "output, report = calc_metrics(pred=y_pred_val, y_test=y_val, labels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
