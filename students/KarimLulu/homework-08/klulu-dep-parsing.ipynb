{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse, parse_tree\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import dill\n",
    "import pymorphy2\n",
    "from tokenize_uk.tokenize_uk import tokenize_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_test, pred, proba=None, labels=None, print_=True, mode=\"weighted\"):\n",
    "    output = {}\n",
    "    if proba is not None:\n",
    "        roc_auc = metrics.roc_auc_score(y_test, proba)\n",
    "        output[\"AUC\"] = roc_auc\n",
    "    output[\"Recall\"] = metrics.recall_score(y_test, pred, average=mode)\n",
    "    output[\"Precision\"] = metrics.precision_score(y_test, pred, average=mode)\n",
    "    output[\"F1\"] = metrics.f1_score(y_test, pred, average=mode)\n",
    "    output[\"Accuracy\"] = metrics.accuracy_score(y_test, pred)\n",
    "    if labels is not None:\n",
    "        index = labels\n",
    "        columns = [\"pred_\" + str(el) for el in index]\n",
    "    else:\n",
    "        columns = None\n",
    "        index = None\n",
    "    conf_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, pred, labels=labels), \n",
    "                               columns=columns, index=index)\n",
    "    report = metrics.classification_report(y_true=y_test, y_pred=pred, labels=labels)\n",
    "    if print_:\n",
    "        for key, value in output.items():\n",
    "            print(f\"{key}: {value:0.3f}\")\n",
    "        print(\"\\nConfusion matrix:\")\n",
    "        print(conf_matrix)\n",
    "        print(\"\\nReport:\")\n",
    "        print(report)\n",
    "    return output, report, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', \"ROOT\"),\n",
    "                    ('xpostag', None), ('feats', None), ('head', None), ('deprel', None),\n",
    "                    ('deps', None), ('misc', None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.home() / \"repos/UD_Ukrainian-IU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with list(data_dir.glob(\"*train*\"))[0].open() as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = parse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('id', 14),\n",
       "             ('form', '.'),\n",
       "             ('lemma', '.'),\n",
       "             ('upostag', 'PUNCT'),\n",
       "             ('xpostag', 'U'),\n",
       "             ('feats', None),\n",
       "             ('head', 6),\n",
       "             ('deprel', 'punct'),\n",
       "             ('deps', None),\n",
       "             ('misc', OrderedDict([('Id', '000g')]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = trees[0]\n",
    "tree[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У <- домі\n",
      "домі <- була\n",
      "римського <- патриція\n",
      "патриція <- домі\n",
      "Руфіна <- патриція\n",
      "була <- root\n",
      "прегарна <- фреска\n",
      "фреска <- була\n",
      ", <- зображення\n",
      "зображення <- фреска\n",
      "Венери <- зображення\n",
      "та <- Адоніса\n",
      "Адоніса <- Венери\n",
      ". <- була\n"
     ]
    }
   ],
   "source": [
    "for node in tree:\n",
    "    head = node[\"head\"]\n",
    "    print(\"{} <- {}\".format(node[\"form\"], tree[head-1][\"form\"] if head>0 else \"root\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parse_context(word, deps, data):\n",
    "    if not word or word == -1:\n",
    "        return 0, \"\", \"\"\n",
    "    deps = deps[word[\"id\"]]\n",
    "    num = len(deps)\n",
    "    if not num:\n",
    "        return num, \"\", \"\"\n",
    "    elif num==1:\n",
    "        return num, data[deps[-1]-1], \"\"\n",
    "    else:\n",
    "        return num, data[deps[-1]-1], data[deps[-1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(stack, queue, tree, parse):\n",
    "    features = {}\n",
    "    stack_depth = len(stack)\n",
    "    s0 = stack[-1] if stack_depth else \"\"\n",
    "    q0 = queue[0] if queue else \"\"\n",
    "    \n",
    "    # Features for stack\n",
    "    if stack:\n",
    "        features[\"s0-form\"] = s0[\"form\"]\n",
    "        features[\"s0-tag\"] = s0[\"upostag\"]\n",
    "        features[\"s0-lemma\"] = s0[\"lemma\"]\n",
    "        features[\"s0-word-tag\"] = s0[\"form\"] + s0[\"upostag\"]\n",
    "        if s0.get(\"feats\"):\n",
    "            for k, v in s0[\"feats\"].items():\n",
    "                features[f\"s0-{k}\"] = v\n",
    "    if stack_depth > 1:\n",
    "        features[\"s1-tag\"] = stack[-2][\"upostag\"]\n",
    "        features[\"s1-word-tag\"] = stack[-2][\"form\"] + stack[-2][\"upostag\"]\n",
    "    \n",
    "    # Features for queue\n",
    "    if queue:\n",
    "        features[\"q0-form\"] = q0[\"form\"]\n",
    "        features[\"q0-tag\"] = q0[\"upostag\"]\n",
    "        features[\"q0-lemma\"] = q0[\"lemma\"]\n",
    "        features[\"q0-word-tag\"] = q0[\"form\"] + q0[\"upostag\"]\n",
    "        if q0.get(\"feats\"):\n",
    "            for k, v in q0[\"feats\"].items():\n",
    "                features[f\"q0-{k}\"] = v \n",
    "    if len(queue) > 1:\n",
    "        features[\"q1-form\"] = queue[1][\"form\"]\n",
    "        features[\"q1-tag\"] = queue[1][\"upostag\"]\n",
    "        features[\"q1-word-tag\"] = queue[1][\"form\"] + queue[1][\"upostag\"]\n",
    "        features[\"q0q1\"] = q0[\"form\"] + queue[1][\"form\"]\n",
    "    if len(queue) > 2:\n",
    "        features[\"q2-tag\"] = queue[2][\"upostag\"]\n",
    "        #features[\"q2-word-tag\"] = queue[2][\"form\"] + queue[2][\"upostag\"]\n",
    "    if len(queue) > 3:\n",
    "        features[\"q3-tag\"] = queue[3][\"upostag\"]\n",
    "        \n",
    "    if queue and stack:\n",
    "        Ds0q0 = q0[\"id\"] - s0[\"id\"]\n",
    "        features[\"distance\"] = Ds0q0\n",
    "        features[\"q0-dist\"] = q0[\"form\"] + \"-{}\".format(Ds0q0)\n",
    "        features[\"s0-dist\"] = s0[\"form\"] + \"-{}\".format(Ds0q0)\n",
    "        features[\"s0q0-dist\"] = s0[\"lemma\"] + q0[\"lemma\"] + \"-{}\".format(Ds0q0)\n",
    "        features[\"s0-tag-dist\"] = s0[\"upostag\"] + \"-{}\".format(Ds0q0)\n",
    "        features[\"q0-tag-dist\"] = q0[\"upostag\"] + \"-{}\".format(Ds0q0)\n",
    "        features[\"s0q0-tag-dist\"] = s0[\"upostag\"] + q0[\"upostag\"] + \"-{}\".format(Ds0q0)\n",
    "        # Add bigrams\n",
    "        features[\"s0q0\"] = s0[\"form\"] + q0[\"form\"]\n",
    "        features[\"s0q0-tag\"] = s0[\"upostag\"] + q0[\"upostag\"]\n",
    "        features[\"q0_q0-tag_s0\"] = q0[\"form\"] + q0[\"upostag\"] + s0[\"form\"]\n",
    "        features[\"q0_q0-tag_s0-tag\"] = q0[\"form\"] + q0[\"upostag\"] + s0[\"upostag\"]\n",
    "        features[\"s0_s0-tag_q0\"] = s0[\"form\"] + s0[\"upostag\"] + q0[\"form\"]\n",
    "        features[\"s0_s0-tag_q0-tag\"] = s0[\"form\"] + s0[\"upostag\"] + q0[\"upostag\"]\n",
    "        features[\"s0_s0-tag_q0_q0-tag\"] = s0[\"form\"] + s0[\"upostag\"] + q0[\"form\"] + q0[\"upostag\"]\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Left two child for top stack\n",
    "    Ns0l, s0l1, s0l2 = get_parse_context(s0, parse.lefts, tree) \n",
    "    if s0l1:\n",
    "        features[\"s0l1\"] = s0l1[\"form\"]\n",
    "        features[\"s0l1-tag\"] = s0l1[\"upostag\"]   \n",
    "    if s0l2:\n",
    "        features[\"s0l2\"] = s0l2[\"form\"]\n",
    "        features[\"s0l2-tag\"] = s0l2[\"upostag\"]\n",
    "    \n",
    "    # Right two child for top stack\n",
    "    Ns0r, s0r1, s0r2 = get_parse_context(s0, parse.rights, tree)\n",
    "    if s0r1:\n",
    "        features[\"s0r1\"] = s0r1[\"form\"]\n",
    "        features[\"s0r1-tag\"] = s0r1[\"upostag\"] \n",
    "    if s0r2:\n",
    "        features[\"s0r2\"] = s0r2[\"form\"]\n",
    "        features[\"s0r2-tag\"] = s0r2[\"upostag\"]\n",
    "    \n",
    "    # Left two child for top queue\n",
    "    Nq0l, q0l1, q0l2 = get_parse_context(q0, parse.lefts, tree)\n",
    "    if q0l1:\n",
    "        features[\"q0l1\"] = q0l1[\"form\"]\n",
    "        features[\"q0l1-tag\"] = q0l1[\"upostag\"]  \n",
    "    if q0l2:\n",
    "        features[\"q0l2\"] = q0l2[\"form\"]\n",
    "        features[\"q0l2-tag\"] = q0l2[\"upostag\"]\n",
    "    \n",
    "    if stack:\n",
    "        features[\"s0l-N\"] = s0[\"form\"] + f\"-{Ns0l}\"\n",
    "        features[\"s0r-N\"] = s0[\"form\"] + f\"-{Ns0r}\"\n",
    "        features[\"s0l-tag-N\"] = s0[\"upostag\"] + f\"-{Ns0l}\"\n",
    "        features[\"s0r-tag-N\"] = s0[\"upostag\"] + f\"-{Ns0r}\"\n",
    "    if queue:\n",
    "        features[\"q0l-N\"] = q0[\"form\"] + f\"-{Nq0l}\"\n",
    "        features[\"q0l-tag-N\"] = q0[\"upostag\"] + f\"-{Nq0l}\"\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parse(object):\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.relations = []\n",
    "        self.lefts = []\n",
    "        self.rights = []\n",
    "        # we need n+1 coz examples in the training data are indexed from 1\n",
    "        for k in range(n+1):\n",
    "            self.lefts.append([])\n",
    "            self.rights.append([])\n",
    "    \n",
    "    def add_relation(self, child, head):\n",
    "        self.relations.append((child, head))\n",
    "        if child < head:\n",
    "            self.lefts[head].append(child)\n",
    "        else:\n",
    "            self.rights[head].append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, stack, q, parse):\n",
    "        if stack and not q:\n",
    "            return \"reduce\"\n",
    "        if stack[-1][\"head\"] == q[0][\"id\"]:\n",
    "            return \"left\"\n",
    "        elif q[0][\"head\"] == stack[-1][\"id\"]:\n",
    "            return \"right\"\n",
    "        elif (stack[-1][\"head\"] in [parent for _, parent in parse.relations] \n",
    "              and q[0][\"head\"] < stack[-1][\"id\"]):\n",
    "            return \"reduce\"\n",
    "        else:\n",
    "            return \"shift\" \n",
    "        \n",
    "    def parse(self, tree, oracle=None, vectorizer=None, log=False):\n",
    "        q = tree.copy()\n",
    "        parse = Parse(len(q))\n",
    "        stack = [ROOT]\n",
    "        labels = []\n",
    "        features = []\n",
    "        while q or stack:\n",
    "            if log:\n",
    "                print(\"Stack:\", [el[\"form\"] for el in stack])\n",
    "                print(\"Q:\", [el[\"form\"] for el in q])\n",
    "            feature_set = extract_features(stack, q, tree, parse)\n",
    "            \n",
    "            if oracle is not None:\n",
    "                v_features = vectorizer.transform(feature_set)\n",
    "                action = oracle.predict(v_features)[0]\n",
    "            else:\n",
    "                action = self.get_action(stack or None, q or None, parse)\n",
    "            \n",
    "            if action == \"left\":\n",
    "                parse.add_relation(stack[-1][\"id\"], q[0][\"id\"])\n",
    "                stack.pop()\n",
    "            elif action == \"right\":\n",
    "                parse.add_relation(q[0][\"id\"], stack[-1][\"id\"])\n",
    "                stack.append(q.pop(0))\n",
    "            elif action == \"reduce\":\n",
    "                stack.pop()\n",
    "            elif action == \"shift\":\n",
    "                stack.append(q.pop(0))              \n",
    "            labels.append(action)\n",
    "            features.append(feature_set)\n",
    "        return labels, features, parse.relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 29\n"
     ]
    }
   ],
   "source": [
    "parser = Parser()\n",
    "labels, features, _ = parser.parse(tree)\n",
    "print(len(labels), len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(trees, parser):\n",
    "    o_labels = []\n",
    "    o_features = []\n",
    "    for tree in trees:\n",
    "        labels, features, _ = parser.parse(tree)\n",
    "        o_labels.extend(labels)\n",
    "        o_features.extend(features)\n",
    "    return o_labels, o_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare train / test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with list(data_dir.glob(\"*test*\"))[0].open() as f:\n",
    "    test_data = f.read()\n",
    "test_trees = parse(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154709 154709\n"
     ]
    }
   ],
   "source": [
    "y_train, features_train = get_data(trees, parser)\n",
    "print(len(features_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30661 30661\n"
     ]
    }
   ],
   "source": [
    "y_test, features_test = get_data(test_trees, parser)\n",
    "print(len(features_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer(sparse=True)\n",
    "v_train = vectorizer.fit_transform(features_train)\n",
    "v_test = vectorizer.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=25, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=25)\n",
    "clf.fit(v_train, y_train)\n",
    "y_pred = clf.predict(v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.897\n",
      "Precision: 0.897\n",
      "F1: 0.896\n",
      "Accuracy: 0.897\n",
      "\n",
      "Confusion matrix:\n",
      "        pred_left  pred_reduce  pred_right  pred_shift\n",
      "left         6987          141          52         186\n",
      "reduce        367         6972         826         191\n",
      "right          63          586        6357         211\n",
      "shift         196          132         214        7180\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       left       0.92      0.95      0.93      7366\n",
      "     reduce       0.89      0.83      0.86      8356\n",
      "      right       0.85      0.88      0.87      7217\n",
      "      shift       0.92      0.93      0.93      7722\n",
      "\n",
      "avg / total       0.90      0.90      0.90     30661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output, report, conf_matrix = calc_metrics(y_test, y_pred, labels=clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add UAS calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UAS(trees, oracle=None, vectorizer=None):\n",
    "    total, tp, failed = 0, 0, 0\n",
    "    for tree in trees:\n",
    "        try:\n",
    "            golden = [(node[\"id\"], node[\"head\"]) for node in tree]\n",
    "            _, _, predicted = parser.parse(tree, oracle=oracle, vectorizer=vectorizer)\n",
    "            total += len(golden)\n",
    "            tp += len(set(golden).intersection(set(predicted))) \n",
    "        except:\n",
    "            failed += 1\n",
    "    return total, tp, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: 4\n",
      "Total: 14781\n",
      "Correctly defined: 11501\n",
      "UAS: 0.78\n"
     ]
    }
   ],
   "source": [
    "total, tp, failed = UAS(test_trees, clf, vectorizer)\n",
    "print(\"Failed:\", failed)\n",
    "print(\"Total:\", total)\n",
    "print(\"Correctly defined:\", tp)\n",
    "print(\"UAS:\", round(tp / total, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.dill\", \"wb+\") as f:\n",
    "    dill.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse sentence using trained parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"ADJF\": \"ADJ\", \"ADJS\": \"ADJ\", \"COMP\": \"ADJ\", \"PRTF\": \"ADJ\",\n",
    "           \"PRTS\": \"ADJ\", \"GRND\": \"VERB\", \"NUMR\": \"NUM\", \"ADVB\": \"ADV\",\n",
    "           \"NPRO\": \"PRON\", \"PRED\": \"ADV\", \"PREP\": \"ADP\", \"PRCL\": \"PART\"}\n",
    "\n",
    "CONJ_COORD = [\"а\", \"або\", \"але\", \"ані\", \"все\", \"все-таки\", \"втім\", \"ж\", \"же\",\n",
    "              \"зате\", \"і\", \"й\", \"ніже\", \"однак\", \"одначе\", \"прецінь\", \"проте\",\n",
    "              \"та\", \"так\", \"також\", \"усе\", \"усе-таки\", \"утім\", \"чи\"]\n",
    "\n",
    "DET = ['інакший', 'його', 'тамтой', 'чий', 'їх', 'інш.', 'деякий', 'ввесь', 'ваш', \n",
    "     'ніякий', 'весь', 'інший', 'чийсь', 'жадний', 'другий', 'кожний', \n",
    "     'такий', 'оцей', 'скілька', 'цей', 'жодний', 'все', 'кілька', 'увесь', \n",
    "     'кожній', 'те', 'сей', 'ін.', 'отакий', 'котрий', 'усякий', 'самий', \n",
    "     'наш', 'усілякий', 'будь-який', 'сам', 'свій', 'всілякий', 'всенький', 'її', \n",
    "     'всякий', 'отой', 'небагато', 'який', 'їхній', 'той', 'якийсь', 'ин.', 'котрийсь', \n",
    "     'твій', 'мій', 'це', 'цей']\n",
    "\n",
    "def normalize_pos(word):\n",
    "    if word.normal_form in DET:\n",
    "            return \"DET\"\n",
    "    if word.tag._POS == \"PNCT\":\n",
    "        return \"PUNCT\"\n",
    "    if word.tag.POS == \"CONJ\":\n",
    "        if word.word in CONJ_COORD:\n",
    "            return \"CCONJ\"\n",
    "        else:\n",
    "            return \"SCONJ\"\n",
    "    else:\n",
    "        return mapping.get(word.tag.POS, word.tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(sentence):\n",
    "    output = []\n",
    "    for k,word in enumerate(sentence):\n",
    "        p = morph.parse(word)[0]\n",
    "        word_dict = {\"id\": k+1, \n",
    "                     \"upostag\": normalize_pos(p) or \"\", \n",
    "                     \"form\": word, \n",
    "                     \"lemma\": p.normal_form or word}\n",
    "        output.append(word_dict)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_deprel(relations, sentence):\n",
    "#     for child, head in relations:\n",
    "#         print(\"{} <- {}\".format(sentence[child-1][\"form\"], sentence[head-1][\"form\"] if head > 0 else \"root\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_deprel(relations, sentence):\n",
    "    for child, head in relations:\n",
    "        sentence[child-1][\"head\"] = head\n",
    "    for word in sentence:\n",
    "        head = word.get('head', 0)\n",
    "        print(\"{} <- {}\".format(word[\"form\"], sentence[head-1][\"form\"] if head > 0 else \"root\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.dill\", \"rb\") as f:\n",
    "    clf = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw away punctuations inside sentences\n",
    "sents = [\"Ці речі були дуже прекрасні але я не міг зосередитися на них через чудовий захід сонця.\",\n",
    "         \"От вони нагодували його й напоїли і під крильця насипали пшона.\",\n",
    "         \"Поселення на острові існували ще за часів енеоліту і бронзового віку про це свідчать знайдені археологами предмети побуту знаряддя праці рештки посуду та прикраси.\"\n",
    "        ]\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Ці речі були дуже прекрасні але я не міг зосередитися на них через чудовий захід сонця.\n",
      "\n",
      "Ці <- речі\n",
      "речі <- були\n",
      "були <- root\n",
      "дуже <- прекрасні\n",
      "прекрасні <- root\n",
      "але <- міг\n",
      "я <- міг\n",
      "не <- міг\n",
      "міг <- були\n",
      "зосередитися <- міг\n",
      "на <- root\n",
      "них <- зосередитися\n",
      "через <- захід\n",
      "чудовий <- захід\n",
      "захід <- зосередитися\n",
      "сонця <- захід\n",
      ". <- були\n",
      "\n",
      "Sentence: От вони нагодували його й напоїли і під крильця насипали пшона.\n",
      "\n",
      "От <- нагодували\n",
      "вони <- нагодували\n",
      "нагодували <- root\n",
      "його <- нагодували\n",
      "й <- напоїли\n",
      "напоїли <- нагодували\n",
      "і <- насипали\n",
      "під <- крильця\n",
      "крильця <- насипали\n",
      "насипали <- нагодували\n",
      "пшона <- насипали\n",
      ". <- нагодували\n",
      "\n",
      "Sentence: Поселення на острові існували ще за часів енеоліту і бронзового віку про це свідчать знайдені археологами предмети побуту знаряддя праці рештки посуду та прикраси.\n",
      "\n",
      "Поселення <- існували\n",
      "на <- root\n",
      "острові <- Поселення\n",
      "існували <- root\n",
      "ще <- root\n",
      "за <- root\n",
      "часів <- існували\n",
      "енеоліту <- часів\n",
      "і <- свідчать\n",
      "бронзового <- віку\n",
      "віку <- свідчать\n",
      "про <- root\n",
      "це <- root\n",
      "свідчать <- існували\n",
      "знайдені <- археологами\n",
      "археологами <- свідчать\n",
      "предмети <- свідчать\n",
      "побуту <- свідчать\n",
      "знаряддя <- свідчать\n",
      "праці <- знаряддя\n",
      "рештки <- існували\n",
      "посуду <- рештки\n",
      "та <- прикраси\n",
      "прикраси <- посуду\n",
      ". <- існували\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    words = tokenize_words(sent)\n",
    "    example = create_example(words)\n",
    "    _, _, relations = parser.parse(example, oracle=clf, vectorizer=vectorizer)#, log=True)\n",
    "    print(\"Sentence: {}\\n\".format(sent))\n",
    "    print_deprel(relations, example)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_list(l):\n",
    "    return np.unique(l, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['learning_rate'] = 0.1\n",
    "params['n_estimators'] = 1000\n",
    "params['max_depth'] = 5\n",
    "params['min_child_weight'] = 100\n",
    "params['gamma'] = 0\n",
    "params['subsample'] = 0.8\n",
    "params['colsample_bytree'] = 0.8\n",
    "params['objective'] = \"multi:softmax\" \n",
    "params['seed'] = 27\n",
    "params['n_jobs'] = -1\n",
    "params[\"eval_metric\"] = [\"merror\", \"mlogloss\"]\n",
    "params[\"early_stopping_rounds\"] = 50\n",
    "params[\"num_class\"] = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_map_train, y_train_num = encode_list(y_train)\n",
    "encode_map_test, y_test_num = encode_list(y_test)\n",
    "dtrain = xgb.DMatrix(v_train, y_train_num)\n",
    "dtest = xgb.DMatrix(v_test, y_test_num)\n",
    "eval_set = [(dtrain, \"train\"), (dtest, \"eval\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.204875\ttrain-mlogloss:1.27807\teval-merror:0.227096\teval-mlogloss:1.28283\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-merror:0.15153\ttrain-mlogloss:0.445531\teval-merror:0.164737\teval-mlogloss:0.475694\n",
      "[100]\ttrain-merror:0.133173\ttrain-mlogloss:0.368257\teval-merror:0.146114\teval-mlogloss:0.394882\n",
      "[150]\ttrain-merror:0.124964\ttrain-mlogloss:0.336257\teval-merror:0.135449\teval-mlogloss:0.362005\n",
      "[200]\ttrain-merror:0.116419\ttrain-mlogloss:0.315891\teval-merror:0.127784\teval-mlogloss:0.341344\n",
      "[250]\ttrain-merror:0.11161\ttrain-mlogloss:0.302556\teval-merror:0.123055\teval-mlogloss:0.32783\n",
      "[300]\ttrain-merror:0.107214\ttrain-mlogloss:0.29202\teval-merror:0.118848\teval-mlogloss:0.317335\n",
      "[350]\ttrain-merror:0.10353\ttrain-mlogloss:0.28343\teval-merror:0.114445\teval-mlogloss:0.308679\n",
      "[400]\ttrain-merror:0.100304\ttrain-mlogloss:0.275616\teval-merror:0.111118\teval-mlogloss:0.301432\n",
      "[450]\ttrain-merror:0.097415\ttrain-mlogloss:0.269457\teval-merror:0.109064\teval-mlogloss:0.295854\n",
      "[500]\ttrain-merror:0.094991\ttrain-mlogloss:0.264131\teval-merror:0.107205\teval-mlogloss:0.290944\n",
      "[550]\ttrain-merror:0.092813\ttrain-mlogloss:0.259113\teval-merror:0.105541\teval-mlogloss:0.286774\n",
      "[600]\ttrain-merror:0.091546\ttrain-mlogloss:0.255102\teval-merror:0.104693\teval-mlogloss:0.283398\n",
      "[650]\ttrain-merror:0.090337\ttrain-mlogloss:0.251371\teval-merror:0.103813\teval-mlogloss:0.280136\n",
      "[700]\ttrain-merror:0.08887\ttrain-mlogloss:0.247908\teval-merror:0.102736\teval-mlogloss:0.277166\n",
      "[750]\ttrain-merror:0.08803\ttrain-mlogloss:0.245189\teval-merror:0.102378\teval-mlogloss:0.274925\n",
      "[800]\ttrain-merror:0.087351\ttrain-mlogloss:0.242529\teval-merror:0.101497\teval-mlogloss:0.272671\n",
      "[850]\ttrain-merror:0.08631\ttrain-mlogloss:0.239832\teval-merror:0.100975\teval-mlogloss:0.270568\n",
      "[900]\ttrain-merror:0.085373\ttrain-mlogloss:0.237391\teval-merror:0.100551\teval-mlogloss:0.26866\n",
      "[950]\ttrain-merror:0.08461\ttrain-mlogloss:0.235132\teval-merror:0.099997\teval-mlogloss:0.266742\n",
      "[999]\ttrain-merror:0.083673\ttrain-mlogloss:0.232917\teval-merror:0.099442\teval-mlogloss:0.265112\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(dtrain=dtrain, num_boost_round=params.get(\"n_estimators\"), \n",
    "                  early_stopping_rounds=params.get(\"early_stopping_rounds\"), \n",
    "                  params=params, evals=eval_set, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.901\n",
      "Precision: 0.900\n",
      "F1: 0.900\n",
      "Accuracy: 0.901\n",
      "\n",
      "Confusion matrix:\n",
      "        pred_left  pred_reduce  pred_right  pred_shift\n",
      "left         6947          162          51         206\n",
      "reduce        325         7240         600         191\n",
      "right          64          596        6252         305\n",
      "shift         231          149         169        7173\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       left       0.92      0.94      0.93      7366\n",
      "     reduce       0.89      0.87      0.88      8356\n",
      "      right       0.88      0.87      0.88      7217\n",
      "      shift       0.91      0.93      0.92      7722\n",
      "\n",
      "avg / total       0.90      0.90      0.90     30661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_xgb = model.predict(dtest)\n",
    "y_pred_xgb = encode_map_test[pred_xgb.astype(np.int)]\n",
    "xgb_metrics = calc_metrics(y_test, y_pred_xgb, labels=encode_map_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKBooster(object):\n",
    "\n",
    "    def __init__(self, booster=xgb.Booster(), params={}, verbose=False, encode_map=None):\n",
    "        self.booster = booster\n",
    "        self.params = params\n",
    "        self.verbose = verbose\n",
    "        self.encode_map = encode_map\n",
    "\n",
    "    def _predict(self, X, feature_names=None):\n",
    "        X = xgb.DMatrix(X, feature_names=feature_names)\n",
    "        return self.booster.predict(X)\n",
    "\n",
    "    def predict(self, X, feature_names=None):\n",
    "        pred = self._predict(X, feature_names=feature_names)\n",
    "        y_pred = encode_map_test[pred.astype(np.int)]\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, deval=None):\n",
    "        dtrain = xgb.DMatrix(X, y)\n",
    "        evals = [(dtrain, 'train')]\n",
    "        if deval is not None:\n",
    "            evals.append((deval, 'eval'))\n",
    "        self.booster = xgb.train(params=self.params,\n",
    "                                 dtrain=dtrain,\n",
    "                                 num_boost_round=self.params.get(\"n_estimators\"),\n",
    "                                 early_stopping_rounds=self.params.get(\"early_stopping_rounds\"),\n",
    "                                 evals=evals,\n",
    "                                 verbose_eval=self.verbose)\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        self.params.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['shift'], dtype='<U6')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skbooster = SKBooster(booster=model, encode_map=encode_map_test)\n",
    "skbooster.predict(v_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total, tp, failed = UAS(test_trees, skbooster, vectorizer)\n",
    "print(\"Failed:\", failed)\n",
    "print(\"Total:\", total)\n",
    "print(\"Correctly defined:\", tp)\n",
    "print(\"UAS:\", round(tp / total, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
