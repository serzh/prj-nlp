{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, recall_score, \n",
    "                             confusion_matrix, accuracy_score, f1_score,\n",
    "                             classification_report)\n",
    "from sklearn.preprocessing import QuantileTransformer, MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import xgboost as xgb\n",
    "from pathlib import Path\n",
    "from tokenize_uk.tokenize_uk import tokenize_words\n",
    "from operator import itemgetter\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport config\n",
    "%aimport target_encoding\n",
    "from config import params\n",
    "from target_encoding import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_test, pred, proba=None, labels=[\"1\", \"2\", \"3\", \"4\", \"5\"], print_=True,\n",
    "                 average=\"macro\", report=True):\n",
    "    output = {}\n",
    "    if proba is not None:\n",
    "        roc_auc = roc_auc_score(y_test, proba)\n",
    "        output[\"AUC\"] = roc_auc\n",
    "    output[\"Recall\"] = recall_score(y_test, pred, average=average)\n",
    "    output[\"Precision\"] = precision_score(y_test, pred, average=average)\n",
    "    output[\"F1\"] = f1_score(y_test, pred, average=average)\n",
    "    output[\"accuracy\"] = accuracy_score(y_test, pred)\n",
    "    if labels is not None:\n",
    "        index = labels\n",
    "        columns = [\"pred_\" + el for el in index]\n",
    "    else:\n",
    "        columns = None\n",
    "        index = None\n",
    "    output[\"conf_matrix\"] = pd.DataFrame(confusion_matrix(y_test, pred), \n",
    "                                         columns=columns, index=index)\n",
    "    if print_:\n",
    "        for key, value in output.items():\n",
    "            if \"matrix\" in key:\n",
    "                print(value)\n",
    "            else:\n",
    "                print(f\"{key}: {value:0.3f}\")\n",
    "    if report:\n",
    "        output[\"report\"] = classification_report(y_test, pred, labels)\n",
    "        print(output[\"report\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stop_words.txt\") as file:\n",
    "    stop = file.readlines()\n",
    "stop = [el.strip() for el in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone = pd.read_csv(\"tone-dict.tsv\", sep='\\t', header=None, names=[\"word\", \"sentiment\"])\n",
    "tone[\"word\"] = tone[\"word\"].str.lower()\n",
    "tone_map = tone.set_index(\"word\").to_dict()[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in Path().glob(\"smart*.json\"):\n",
    "    with open(file, \"r\") as f_in:\n",
    "        data.extend(json.load(f_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = json_normalize(data, record_path=\"reviews\", meta=[\"path\", \"price\", \"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"] = df.apply(lambda x: x['text'] + \" \" + x['pros']+ \" \" + x['cons'], axis=1)\n",
    "df[\"id\"] = df[\"link\"].str.extract(r\"/p(?P<id>[0-9]+)\")\n",
    "df[\"category\"] = df[\"path\"].map(lambda x: x[-2])\n",
    "#frq = df.groupby(\"stars\")[\"text\"].count()\n",
    "#df[\"weights\"] = df[\"stars\"].map({\"1\": 1, \"2\": 1, \"3\": 1, \"4\": 5, \"5\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>cons</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>pros</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>path</th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>балацька лариса</td>\n",
       "      <td>До телефона треба зразу брати і захисник скло ...</td>\n",
       "      <td>2018-3-24</td>\n",
       "      <td>https://rozetka.com.ua/zte_blade_v8_gray/p2573...</td>\n",
       "      <td>Камера і батарея</td>\n",
       "      <td>5</td>\n",
       "      <td>Купили телефон в подарунок дітям, зразу два зо...</td>\n",
       "      <td>[Интернет-супермаркет №, Смартфоны, ТВ и элект...</td>\n",
       "      <td>4499</td>\n",
       "      <td>ZTE Blade V8 Gray</td>\n",
       "      <td>Купили телефон в подарунок дітям, зразу два зо...</td>\n",
       "      <td>25738433</td>\n",
       "      <td>Мобильные телефоны</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                               cons  \\\n",
       "0  балацька лариса  До телефона треба зразу брати і захисник скло ...   \n",
       "\n",
       "        date                                               link  \\\n",
       "0  2018-3-24  https://rozetka.com.ua/zte_blade_v8_gray/p2573...   \n",
       "\n",
       "               pros  stars                                               text  \\\n",
       "0  Камера і батарея      5  Купили телефон в подарунок дітям, зразу два зо...   \n",
       "\n",
       "                                                path  price  \\\n",
       "0  [Интернет-супермаркет №, Смартфоны, ТВ и элект...   4499   \n",
       "\n",
       "               title                                             review  \\\n",
       "0  ZTE Blade V8 Gray  Купили телефон в подарунок дітям, зразу два зо...   \n",
       "\n",
       "         id            category  \n",
       "0  25738433  Мобильные телефоны  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "count    3797.000000\n",
       "mean        4.359494\n",
       "std         1.004769\n",
       "min         1.000000\n",
       "25%         4.000000\n",
       "50%         5.000000\n",
       "75%         5.000000\n",
       "max         5.000000\n",
       "Name: stars, dtype: float64"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)\n",
    "df[\"stars\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars      stars\n",
      "5   2293  60.389781\n",
      "4    988  26.020543\n",
      "3    243   6.399789\n",
      "1    139   3.660785\n",
      "2    134   3.529102\n"
     ]
    }
   ],
   "source": [
    "s = df.stars.value_counts()\n",
    "print(pd.concat([s, s*100 / s.sum()], axis=1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we deal with an imbalanced multiclass classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                     1045\n",
       "Немає                                                                                  82\n",
       "немає                                                                                  40\n",
       "-                                                                                      32\n",
       "Нема                                                                                   20\n",
       "Ціна, батарея,вимикається на морозі,швидко дряпається без захисного скла і чохла.      17\n",
       "Немає.                                                                                 17\n",
       "не виявив                                                                              16\n",
       "Поки що не виявлено недоліків!                                                         16\n",
       "Крім \"серйозної\" ціни не знайшов.                                                      16\n",
       "Name: cons, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.074193548387097"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.63345379452762"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze cons\n",
    "df[\"cons\"].value_counts().head(10)\n",
    "patt = \"не має|не знайш|немає|не вияв|відсутн|нема|--\"\n",
    "cond = ((df[\"cons\"].str.lower().str.contains(patt, regex=True)) | (df[\"cons\"]==\"\"))\n",
    "df.loc[~cond, \"stars\"].mean()\n",
    "df.loc[cond, \"stars\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings differ considerably for reviews with words in section Недостатки  as \"немає\", \"не виявив\", \"не знайшов\", \"відсутні\", тощо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                         1064\n",
       "Ціна                                                                                       44\n",
       "ціна                                                                                       25\n",
       "Батарея                                                                                    19\n",
       "Зв‘язок, камера, чіткість роботи, зручний.                                                 16\n",
       "Це супер-машина, а не телефон                                                              16\n",
       "Прекрасна камера, достатньо пам'яті (я про 128 GB) donating в фонд боротьби зі СНІДом      16\n",
       "Прекрасна камера, зручний в управлінні!                                                    16\n",
       "-                                                                                          14\n",
       "Дизайн                                                                                     10\n",
       "Name: pros, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.311745334796926"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.482142857142857"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze pros\n",
    "df[\"pros\"].value_counts().head(10)\n",
    "patt = \"ціна|якість\"\n",
    "cond1 = (df[\"pros\"].str.lower().str.contains(patt, regex=True)) | (df[\"pros\"].str.len() < 10)\n",
    "cond2 = df[\"pros\"]==\"\"\n",
    "cond = cond2\n",
    "df.loc[~cond, \"stars\"].mean()\n",
    "df.loc[cond, \"stars\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 20% of data for testing and stratify according to the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of train: 3037, Num. of test: 760\n"
     ]
    }
   ],
   "source": [
    "y = df[\"stars\"]\n",
    "X = df.loc[:, ~df.columns.isin([\"stars\"])]\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42,\n",
    "                                                    stratify=y)\n",
    "print(f\"Num. of train: {len(X_train)}, Num. of test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate performance of the very simple baseline prediction algorithm - <b>everything is a 5 star review (just take a majority class)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.604\n",
      "Precision: 0.365\n",
      "F1: 0.455\n",
      "accuracy: 0.604\n",
      "   pred_1  pred_2  pred_3  pred_4  pred_5\n",
      "1       0       0       0       0      28\n",
      "2       0       0       0       0      27\n",
      "3       0       0       0       0      48\n",
      "4       0       0       0       0     198\n",
      "5       0       0       0       0     459\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.00      0.00      0.00        28\n",
      "          2       0.00      0.00      0.00        27\n",
      "          3       0.00      0.00      0.00        48\n",
      "          4       0.00      0.00      0.00       198\n",
      "          5       0.60      1.00      0.75       459\n",
      "\n",
      "avg / total       0.36      0.60      0.45       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_pred = np.full(y_test.shape, 5)\n",
    "base_metrics = calc_metrics(y_test, base_pred, proba=None, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF - IDF & other fesatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_array(x):\n",
    "    if len(x) > 0:\n",
    "        return x\n",
    "    return np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = validate_array(np.array(x))\n",
    "    nonzero = np.nonzero(x)[0].shape[0]\n",
    "    n_pos = (x > 0).sum()\n",
    "    n_neg = (x < 0).sum()\n",
    "    pos = validate_array(x[x > 0])\n",
    "    neg = validate_array(x[x < 0])\n",
    "    max_pos = np.max(pos)\n",
    "    min_pos = np.min(pos)\n",
    "    mean_pos = np.mean(pos)\n",
    "    std_pos = np.std(pos)\n",
    "    sum_pos = np.sum(pos)\n",
    "    max_neg = np.max(neg)\n",
    "    min_neg = np.min(neg)\n",
    "    mean_neg = np.mean(neg)\n",
    "    std_neg = np.std(neg)\n",
    "    sum_neg = np.sum(neg)\n",
    "    return pd.Series({\"nonzero\": nonzero, \n",
    "                      \"nonzero_ratio\": nonzero / len(x) if len(x)>0 else 0,\n",
    "                      \"n_pos\": n_pos,\n",
    "                      \"n_pos_ratio\": n_pos / nonzero if nonzero>0 else 0,\n",
    "                      \"n_neg\": n_neg,\n",
    "                      \"n_neg_ratio\": n_neg / nonzero if nonzero>0 else 0,\n",
    "                      \"mean\": np.mean(x),\n",
    "                      \"max\": np.max(x),\n",
    "                      \"min\": np.min(x),\n",
    "                      \"std\": np.std(x),\n",
    "                      \"sum\": np.sum(x),\n",
    "                      \"max_pos\": max_pos,\n",
    "                      \"min_pos\": min_pos,\n",
    "                      \"mean_pos\": mean_pos,\n",
    "                      \"std_pos\": std_pos,\n",
    "                      \"sum_pos\": sum_pos,\n",
    "                      \"max_neg\": max_neg,\n",
    "                      \"min_neg\": min_neg,\n",
    "                      \"mean_neg\": mean_neg,\n",
    "                      \"std_neg\": std_neg,\n",
    "                      \"sum_neg\": sum_neg \n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_cons(df, var=\"cons\", patt=\"не має|не знайш|немає|не вияв|відсутн|нема|--\"):\n",
    "    cond = ((df[var].str.lower().str.contains(patt, regex=True)) | (df[var]==\"\"))\n",
    "    return cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tone(X, tone_map=tone_map, var=\"review\"):\n",
    "    temp = X[var].map(lambda x: [tone_map.get(word.lower(), 0) for word in tokenize_words(x)])\n",
    "    return temp.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformer(train, test, transformer):\n",
    "    if transformer is not None:\n",
    "        train = transformer.fit_transform(train)\n",
    "        test = transformer.transform(test)\n",
    "        return train, test\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_length_features(df):\n",
    "    df = df[[\"text\", \"pros\", \"cons\"]].applymap(len)\n",
    "    df[\"text_cons\"] = df.apply(lambda x: x[\"text\"]/x[\"cons\"], axis=1)\n",
    "    df[\"text_pros\"] = df.apply(lambda x: x[\"text\"]/x[\"cons\"], axis=1)\n",
    "    df[\"pros_cons\"] = df.apply(lambda x: x[\"pros\"]/x[\"cons\"], axis=1)\n",
    "    df = df.values\n",
    "    df[~np.isfinite(df)] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(X_train, X_test, y_train=None, transformer=None, var=\"review\", features=None,\n",
    "                   vectorizer=None, encoder=None):\n",
    "    f_train = []\n",
    "    f_test = []\n",
    "    for feature in features:\n",
    "        if feature == \"tfidf\":\n",
    "            train = vectorizer.fit_transform(X_train[var]).toarray()\n",
    "            test = vectorizer.transform(X_test[var]).toarray()\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "        elif feature == \"sentiment\":\n",
    "            train = add_tone(X_train, var=var).values\n",
    "            test = add_tone(X_test, var=var).values\n",
    "            train, test = apply_transformer(train, test, transformer)\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "        elif feature == \"len\":\n",
    "            train = build_length_features(X_train)\n",
    "            test = build_length_features(X_test)\n",
    "            train, test = apply_transformer(train, test, transformer)\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "        elif feature == \"is_cons\":\n",
    "            train = condition_cons(X_train).astype(int)[:, np.newaxis]\n",
    "            test = condition_cons(X_test).astype(int)[:, np.newaxis]\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "        elif feature == \"target_encode\":\n",
    "            train = encoder.fit_transform(X_train[[\"id\", \"category\"]], y_train).values\n",
    "            test = encoder.transform(X_test[[\"id\", \"category\"]]).values\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "    return np.concatenate((f_train), axis=1), np.concatenate((f_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nb(train, y_train, test, y_test, alpha=2.5, priors=None):\n",
    "    clf = MultinomialNB(alpha=alpha, class_prior=priors)\n",
    "    clf.fit(train, y_train)\n",
    "    pred = clf.predict(test)\n",
    "    proba = clf.predict_proba(test)\n",
    "    metrics = calc_metrics(y_test, pred, proba=None, average=\"weighted\")\n",
    "    return pred, proba, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = QuantileTransformer(n_quantiles=10, output_distribution=\"uniform\")\n",
    "minmax = MinMaxScaler()\n",
    "std = StandardScaler()\n",
    "maxabs = MaxAbsScaler()\n",
    "transformer = minmax\n",
    "encoder = TargetEncoder(columns=[\"id\", \"category\"], feature_names={\"id\": \"id_mean\",\n",
    "                                                                   \"category\": \"category_mean\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try only TF-IDF (with stop words if we use analyzer=word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 3500\n"
     ]
    }
   ],
   "source": [
    "tf_params = {\"lowercase\": 1,\n",
    "             \"analyzer\": \"word\",\n",
    "             \"stop_words\": stop,\n",
    "             \"ngram_range\": (1, 1),\n",
    "             \"min_df\": 1,\n",
    "             \"max_df\": 1.0,\n",
    "             \"preprocessor\": None,\n",
    "             \"max_features\": 3500*1 or None,\n",
    "             \"norm\": 'l2'*0,\n",
    "             \"use_idf\": 0,\n",
    "             \"smooth_idf\": 0,\n",
    "             \"sublinear_tf\": 0, \n",
    "             \"tokenizer\": None#tokenize_words\n",
    "             }\n",
    "var = \"review\"\n",
    "features = [\n",
    "            \"tfidf\",\n",
    "            #\"target_encode\",\n",
    "            #\"sentiment\", \n",
    "            #\"len\", \n",
    "            #\"is_cons\"\n",
    "            ]\n",
    "vectorizer = TfidfVectorizer(**tf_params)\n",
    "train, test = build_features(X_train, X_test, y_train, features=features, vectorizer=vectorizer,\n",
    "                            transformer=transformer, var=var, encoder=encoder)\n",
    "print(f\"Features: {train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.634\n",
      "Precision: 0.561\n",
      "F1: 0.586\n",
      "accuracy: 0.634\n",
      "   pred_1  pred_2  pred_3  pred_4  pred_5\n",
      "1       2       0       2      13      11\n",
      "2       1       0       3      13      10\n",
      "3       2       0       1      27      18\n",
      "4       0       0       4      73     121\n",
      "5       0       1       1      51     406\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.40      0.07      0.12        28\n",
      "          2       0.00      0.00      0.00        27\n",
      "          3       0.09      0.02      0.03        48\n",
      "          4       0.41      0.37      0.39       198\n",
      "          5       0.72      0.88      0.79       459\n",
      "\n",
      "avg / total       0.56      0.63      0.59       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred, proba, metrics = fit_nb(train, y_train, test, y_test, alpha=2.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 3521\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "            \"tfidf\",\n",
    "            \"sentiment\", \n",
    "            #\"len\", \n",
    "            #\"is_cons\",\n",
    "            #\"target_encode\"\n",
    "            ]\n",
    "train, test = build_features(X_train, X_test, features=features, vectorizer=vectorizer,\n",
    "                            transformer=transformer, var=var)\n",
    "print(f\"Features: {train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.646\n",
      "Precision: 0.558\n",
      "F1: 0.586\n",
      "accuracy: 0.646\n",
      "   pred_1  pred_2  pred_3  pred_4  pred_5\n",
      "1       1       0       2      11      14\n",
      "2       1       0       2      15       9\n",
      "3       0       0       0      30      18\n",
      "4       0       0       0      68     130\n",
      "5       0       0       0      37     422\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.50      0.04      0.07        28\n",
      "          2       0.00      0.00      0.00        27\n",
      "          3       0.00      0.00      0.00        48\n",
      "          4       0.42      0.34      0.38       198\n",
      "          5       0.71      0.92      0.80       459\n",
      "\n",
      "avg / total       0.56      0.65      0.59       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred, proba, metrics = fit_nb(train, y_train, test, y_test, alpha=2.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy as well as other metrics <b>increased</b> after adding sentiment features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Add more features</b> - target encoding of the id field, length for review/pros/cons sections, divisions of lengths, whether cons section match pattern \"не має|не знайш|немає|не вияв|відсутн|нема|--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 3530\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "            \"tfidf\",\n",
    "            \"target_encode\",\n",
    "            \"sentiment\", \n",
    "            \"len\", \n",
    "            \"is_cons\"\n",
    "            ]\n",
    "train, test = build_features(X_train, X_test, y_train, features=features, vectorizer=vectorizer,\n",
    "                            transformer=transformer, var=var, encoder=encoder)\n",
    "print(f\"Features: {train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.646\n",
      "Precision: 0.537\n",
      "F1: 0.580\n",
      "accuracy: 0.646\n",
      "   pred_1  pred_2  pred_3  pred_4  pred_5\n",
      "1       0       0       1      12      15\n",
      "2       0       0       0      17      10\n",
      "3       0       0       0      23      25\n",
      "4       0       0       2      64     132\n",
      "5       0       0       1      31     427\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.00      0.00      0.00        28\n",
      "          2       0.00      0.00      0.00        27\n",
      "          3       0.00      0.00      0.00        48\n",
      "          4       0.44      0.32      0.37       198\n",
      "          5       0.70      0.93      0.80       459\n",
      "\n",
      "avg / total       0.54      0.65      0.58       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred, proba, metrics = fit_nb(train, y_train, test, y_test, alpha=2.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add <b>lemmatization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word, idx=0, analyzer=morph):\n",
    "    p = morph.parse(word)\n",
    "    if p:\n",
    "        return p[idx].normal_form\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text, lower=True, idx=0, analyzer=morph, stop=stop):\n",
    "    words = tokenize_words(text)\n",
    "    if lower:\n",
    "        words = [el.lower() for el in words]\n",
    "    lemmas = [get_lemma(word, idx, analyzer) for word in words]\n",
    "    if stop:\n",
    "        lemmas = [lemma for lemma in lemmas if lemma not in stop]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[var] = df[var].map(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 3530\n"
     ]
    }
   ],
   "source": [
    "tf_params_prep = tf_params.copy()\n",
    "tf_params_prep[\"preprocessor\"] = preprocessor\n",
    "features = [\n",
    "            \"tfidf\",\n",
    "            \"target_encode\",\n",
    "            \"sentiment\", \n",
    "            \"len\", \n",
    "            \"is_cons\"\n",
    "            ]\n",
    "vectorizer = TfidfVectorizer(**tf_params_prep)\n",
    "train, test = build_features(X_train, X_test, y_train, features=features, vectorizer=vectorizer,\n",
    "                            transformer=transformer, var=var, encoder=encoder)\n",
    "print(f\"Features: {train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.643\n",
      "Precision: 0.571\n",
      "F1: 0.581\n",
      "accuracy: 0.643\n",
      "   pred_1  pred_2  pred_3  pred_4  pred_5\n",
      "1       1       0       1      12      14\n",
      "2       0       0       0      18       9\n",
      "3       0       0       0      28      20\n",
      "4       0       0       1      65     132\n",
      "5       0       0       0      36     423\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      0.04      0.07        28\n",
      "          2       0.00      0.00      0.00        27\n",
      "          3       0.00      0.00      0.00        48\n",
      "          4       0.41      0.33      0.36       198\n",
      "          5       0.71      0.92      0.80       459\n",
      "\n",
      "avg / total       0.57      0.64      0.58       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred, proba, metrics = fit_nb(train, y_train, test, y_test, alpha=2.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works a bit worse when lemmatization is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 3530\n"
     ]
    }
   ],
   "source": [
    "tf_params_xgb = tf_params.copy()\n",
    "tf_params_prep[\"use_idf\"] = 1\n",
    "tf_params_xgb[\"norm\"] = \"l2\"\n",
    "tf_params_xgb[\"preprocessor\"] = preprocessor\n",
    "features = [\n",
    "            \"tfidf\",\n",
    "            \"target_encode\",\n",
    "            \"sentiment\", \n",
    "            \"len\", \n",
    "            \"is_cons\"\n",
    "            ]\n",
    "vectorizer = TfidfVectorizer(**tf_params_xgb)\n",
    "train, test = build_features(X_train, X_test, y_train, features=features, vectorizer=vectorizer,\n",
    "                            transformer=transformer, var=var, encoder=encoder)\n",
    "print(f\"Features: {train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['num_class'] = df[\"stars\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train, y_train-1)#, weight=X_train[\"weights\"])\n",
    "dtest = xgb.DMatrix(test, y_test-1)#, weight=X_test[\"weights\"])\n",
    "eval_set = [(dtrain, \"train\"), (dtest, \"eval\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.52437\ttrain-merror:0.382614\teval-mlogloss:1.52473\teval-merror:0.384211\n",
      "Multiple eval metrics have been passed: 'eval-merror' will be used for early stopping.\n",
      "\n",
      "Will train until eval-merror hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mlogloss:0.911149\ttrain-merror:0.362858\teval-mlogloss:0.931587\teval-merror:0.367105\n",
      "[76]\ttrain-mlogloss:0.888813\ttrain-merror:0.351992\teval-mlogloss:0.922265\teval-merror:0.351316\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(dtrain=dtrain, num_boost_round=77,#params.get(\"n_estimators\"), \n",
    "                  early_stopping_rounds=params.get(\"early_stopping_rounds\"), \n",
    "                  params=params, evals=eval_set, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score (accuracy): 0.648684\n",
      "Recall: 0.649\n",
      "Precision: 0.564\n",
      "F1: 0.573\n",
      "accuracy: 0.649\n",
      "   pred_1  pred_2  pred_3  pred_4  pred_5\n",
      "1       0       0       0      11      17\n",
      "2       0       0       1      12      14\n",
      "3       0       0       3      17      28\n",
      "4       0       0       1      50     147\n",
      "5       0       0       1      18     440\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.00      0.00      0.00        28\n",
      "          2       0.00      0.00      0.00        27\n",
      "          3       0.50      0.06      0.11        48\n",
      "          4       0.46      0.25      0.33       198\n",
      "          5       0.68      0.96      0.80       459\n",
      "\n",
      "avg / total       0.56      0.65      0.57       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_xgb = (model.predict(dtest)+1).astype(int)\n",
    "print(f\"Best score (accuracy): {1-model.best_score}\")\n",
    "xgb_metrics = calc_metrics(y_test, pred_xgb, average=\"weighted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
