{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from spacy.lang.xx import MultiLanguage\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "morph = pymorphy2.MorphAnalyzer(lang=\"uk\")\n",
    "nlp = MultiLanguage()\n",
    "tokenizer = MultiLanguage().Defaults.create_tokenizer(nlp)\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, recall_score, \n",
    "                             confusion_matrix, accuracy_score, f1_score)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "with open(\"stop-words.txt\", \"r\") as fl:\n",
    "    stop_words = fl.read().split(\"\\n\")\n",
    "ADVANTANGE_TXT = \"Переваги:\\xa0\"\n",
    "DRAWBACK_TXT = \"Недоліки:\\xa0\"\n",
    "SCRAPE_FILE = \"scrape_feedbacks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    2353\n",
       "4     998\n",
       "3     239\n",
       "1     135\n",
       "2     128\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(SCRAPE_FILE)\n",
    "#remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "df['stars'].value_counts()\n",
    "# df['stars'] = df['stars'].map({1:1,2:1,3:2,4:2,5:3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data looks kinda biased. I tried to cut 5 and 4 but does not really help. Let's split it to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.2\n",
    "x = df[\"text\"] +\" \"+df[\"advantage\"].astype(str)+\" \"+df[\"drawback\"].astype(str)\n",
    "y = df[\"stars\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tone_dict = {}\n",
    "def count_occurences(x_train, y_train):\n",
    "    occurences = defaultdict(Counter)\n",
    "    \n",
    "    for text, cls in zip(x_train, y_train):\n",
    "        for token in tokenizer(text):\n",
    "            normal_form = morph.parse(token.text)[0].normal_form\n",
    "            occurences[cls].update([normal_form])\n",
    "    print(\"Number of words by classes, {}\".format([(cls, len(dc)) for cls, dc in occurences.items()]))\n",
    "    return occurences\n",
    "\n",
    "def get_metrics(y_true, prediction):\n",
    "    print(\"RECALL: {}\".format(recall_score(y_true, prediction, average='weighted')))\n",
    "    print(\"PRECISION: {}\".format(precision_score(y_true, prediction, average='weighted')))\n",
    "    print(\"F1: {}\".format(f1_score(y_true, prediction, average='weighted')))\n",
    "    print(\"ACCURACY: {}\".format(accuracy_score(y_true, prediction)))\n",
    "\n",
    "\n",
    "def predict(vocabulary_pr, texts):\n",
    "    prediction = []\n",
    "    tone_val = [0.0001 for i in range(len(classes))]\n",
    "    for text in texts:\n",
    "        probs_stats = []\n",
    "#         probs_tone = []\n",
    "        for token in tokenizer(text):\n",
    "            normal_form = morph.parse(token.text)[0].normal_form\n",
    "            if not normal_form in stop_words:\n",
    "                if normal_form in vocabulary_pr:\n",
    "                    probs_stats.append(vocabulary_pr[normal_form])\n",
    "                        \n",
    "#                         print(normal_form, vocabulary_pr[normal_form])\n",
    "                if normal_form in tone_dict:\n",
    "                    tone_val_c = tone_val.copy()\n",
    "                    tone_val_c[tone_dict[normal_form]-1] = 0.999\n",
    "                    probs_stats.append(tone_val_c)\n",
    "        probs_stats = np.prod(np.array(probs_stats), axis=0)\n",
    "#         if probs_tone:\n",
    "#             import ipdb; ipdb.set_trace()\n",
    "#             probs_tone = np.prod(np.array(probs_tone), axis=0)\n",
    "            \n",
    "        prediction.append(np.argmax(probs_stats)+1)\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words by classes, [(5, 8715), (4, 5789), (1, 1808), (2, 1954), (3, 2881)]\n"
     ]
    }
   ],
   "source": [
    "occurences = count_occurences(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12567"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(occurences.keys())\n",
    "print(classes)\n",
    "occurences_by_cls = y_train.value_counts()\n",
    "vocabulary = set([item for sublist in [occurences[cls].keys() for cls in classes] for item in sublist])\n",
    "vocabulary_pr = {}\n",
    "def train():\n",
    "    for word in vocabulary:\n",
    "        if word not in stop_words:\n",
    "            occurences_word_by_cls = {cls: occurences[cls][word] for cls in classes}\n",
    "            occurences_word_total = sum(occurences_word_by_cls.values())\n",
    "            probs = []\n",
    "            for cls in classes:\n",
    "#                 print(\"occurences_word_by_cls\", occurences_word_by_cls[cls])\n",
    "#                 print(\"occurences_by_cls\", occurences_by_cls[cls])\n",
    "                pr = ((occurences_word_by_cls[cls]/occurences_by_cls[cls])*(occurences_by_cls[cls]/len(x_train)))/((occurences_word_total)/(len(x_train)))\n",
    "                if pr == 0:\n",
    "                    pr = 0.0001\n",
    "                elif pr >= 1:\n",
    "                    pr = 0.9999\n",
    "                probs.append(pr)\n",
    "            vocabulary_pr[word] = probs\n",
    "#             print(word, probs)  \n",
    "  \n",
    "train()\n",
    "len(vocabulary_pr.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET METRICS:\n",
      "RECALL: 0.7495133030499675\n",
      "PRECISION: 0.7748653466808507\n",
      "F1: 0.7201803576103017\n",
      "ACCURACY: 0.7495133030499675\n",
      "===================\n",
      "TEST SET METRICS:\n",
      "RECALL: 0.5330739299610895\n",
      "PRECISION: 0.43824155181997665\n",
      "F1: 0.4779723303069608\n",
      "ACCURACY: 0.5330739299610895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING SET METRICS:\")\n",
    "get_metrics(y_train, predict(vocabulary_pr, x_train))\n",
    "print(\"===================\")\n",
    "print(\"TEST SET METRICS:\")\n",
    "get_metrics(y_test, predict(vocabulary_pr, x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look very precise, let's try scikit tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SET METRICS:\n",
      "RECALL: 0.6108949416342413\n",
      "PRECISION: 0.37319262971430306\n",
      "F1: 0.4633357769882893\n",
      "ACCURACY: 0.6108949416342413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "tf_params = {\"lowercase\": True,\n",
    "             \"analyzer\": \"char_wb\",\n",
    "             \"stop_words\": stop_words,\n",
    "#              \"ngram_range\": (3, 3),\n",
    "#              \"min_df\": 1,\n",
    "#              \"max_df\": 1.0,\n",
    "#              \"preprocessor\": None,#Preprocessor(),\n",
    "#              \"max_features\": 3500,\n",
    "#              \"norm\": None,\n",
    "             \"use_idf\": True\n",
    "             }\n",
    "priors = y_train.value_counts(normalize=True).values\n",
    "vectorizer = TfidfVectorizer(**tf_params)\n",
    "train = vectorizer.fit_transform(x_train)\n",
    "test = vectorizer.transform(x_test)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train.toarray(), y_train)\n",
    "pred = clf.predict(test.toarray())\n",
    "print(\"TEST SET METRICS:\")\n",
    "get_metrics(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty much the same result. Let'saAdd tones .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_dict = pd.read_csv(\"tone-dict-uk.tsv\", delimiter='\\t', names=[\"word\", \"mark\"])\n",
    "#bad(1), neutral(2) and good(3):\n",
    "tone_mapping = {-2: 1, -1: 2, 0:3, 1:4, 2:5}\n",
    "tone_dict[\"mark\"] = tone_dict[\"mark\"].map(tone_mapping)\n",
    "tone_dict = tone_dict.set_index('word').T.to_dict('int')['mark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET METRICS:\n",
      "RECALL: 0.7495133030499675\n",
      "PRECISION: 0.7748653466808507\n",
      "F1: 0.7201803576103017\n",
      "ACCURACY: 0.7495133030499675\n",
      "===================\n",
      "TEST SET METRICS:\n",
      "RECALL: 0.5330739299610895\n",
      "PRECISION: 0.43824155181997665\n",
      "F1: 0.4779723303069608\n",
      "ACCURACY: 0.5330739299610895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# import ipdb; ipdb.set_trace()\n",
    "print(\"TRAINING SET METRICS:\")\n",
    "get_metrics(y_train, predict(vocabulary_pr, x_train))\n",
    "print(\"===================\")\n",
    "print(\"TEST SET METRICS:\")\n",
    "get_metrics(y_test, predict(vocabulary_pr, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed vocabulary length from 12567 to 12298\n",
      "TRAINING SET METRICS:\n",
      "RECALL: 0.7498377676833226\n",
      "PRECISION: 0.7739448156782516\n",
      "F1: 0.7236799205156947\n",
      "ACCURACY: 0.7498377676833226\n",
      "===================\n",
      "TEST SET METRICS:\n",
      "RECALL: 0.5291828793774319\n",
      "PRECISION: 0.43813585715588804\n",
      "F1: 0.4768450629441803\n",
      "ACCURACY: 0.5291828793774319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#try to make it better\n",
    "prev_len = len(vocabulary_pr)\n",
    "for i in vocabulary:\n",
    "    #numbers\n",
    "    try:\n",
    "        float(i)\n",
    "        vocabulary_pr.pop(i)\n",
    "        continue\n",
    "    except:\n",
    "        continue\n",
    "    #special symbols\n",
    "    if i in (\".\", \"?\", \"!\", \"=\", \"+\", \"/\" ,\";\" \":\", \"*\", \"(\", \")\"):\n",
    "        vocabulary_pr.pop(i)\n",
    "    #short\n",
    "    elif len(i) < 3:\n",
    "        vocabulary_pr.pop(i)\n",
    "        \n",
    "#     elif sum([occurences[cls][i] for cls in classes]) <2:\n",
    "#         print(i)\n",
    "#         vocabulary_pr.pop(i)\n",
    "#     print(sum([occurences[cls][i] for cls in classes]))\n",
    "print(\"changed vocabulary length from {} to {}\".format(prev_len, len(vocabulary_pr)))\n",
    "print(\"TRAINING SET METRICS:\")\n",
    "get_metrics(y_train, predict(vocabulary_pr, x_train))\n",
    "print(\"===================\")\n",
    "print(\"TEST SET METRICS:\")\n",
    "get_metrics(y_test, predict(vocabulary_pr, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
