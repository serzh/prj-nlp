{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "# Read dataset\n",
    "review_data = []\n",
    "\n",
    "f = codecs.open('reviews_rozetka.txt', \"r\", \"utf-8\")\n",
    "text = f.read()\n",
    "lines = text.splitlines()\n",
    "\n",
    "for line in lines:\n",
    "    item = line.split(\"/////\")\n",
    "    if (len(item) != 2):\n",
    "        print 'wrong item: ', item\n",
    "        continue\n",
    "    item[0] = item[0].encode('utf-8')\n",
    "    review_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 4832\n",
      "test set size: 1074\n"
     ]
    }
   ],
   "source": [
    "# Split data to train and test set\n",
    "\n",
    "train_set = sorted(review_data, key=lambda tup: tup[1])\n",
    "  \n",
    "test_set = []\n",
    "for i, item in enumerate(train_set):\n",
    "    if (i % 9 == 0) or (i % 8 == 0):\n",
    "        test_set.append(item)\n",
    "        train_set.remove(item)\n",
    "\n",
    "print 'train set size:', str(len(train_set))\n",
    "print 'test set size:', str(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  1 :  157\n",
      "test  1 :  35\n",
      "train  2 :  180\n",
      "test  2 :  41\n",
      "train  3 :  277\n",
      "test  3 :  61\n",
      "train  4 :  909\n",
      "test  4 :  202\n",
      "train  5 :  3309\n",
      "test  5 :  735\n",
      "{1: 0.032491721854304635, 2: 0.037251655629139076, 3: 0.05732615894039735, 4: 0.18812086092715233, 5: 0.6848096026490066}\n"
     ]
    }
   ],
   "source": [
    "# Check star frequencies\n",
    "\n",
    "star_probability = {}\n",
    "for star in range(1, 6):\n",
    "    train_for_star = len(list(x for x in train_set if x[1] == str(star)))\n",
    "    print 'train ', str(star), ': ', str(train_for_star)\n",
    "    test_for_star = len(list(x for x in test_set if x[1] == str(star)))\n",
    "    print 'test ', str(star), ': ', str(test_for_star)\n",
    "\n",
    "    star_probability[star] = train_for_star / float(len(train_set))\n",
    "\n",
    "print star_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full vocabulary size: 15987\n",
      "final vocabulary size: 4579\n"
     ]
    }
   ],
   "source": [
    "# form vocabulary, skip rare words\n",
    "\n",
    "import polyglot\n",
    "from polyglot.text import Text, Word\n",
    "import numpy as np\n",
    "\n",
    "vocabulary = {}\n",
    "for item in train_set:\n",
    "    text = Text(item[0])\n",
    "    for word in text.words:\n",
    "        word_text = word.lower()\n",
    "        if not vocabulary.has_key(word_text):\n",
    "            vocabulary[word_text] = 0\n",
    "        vocabulary[word_text] += 1\n",
    "\n",
    "print 'full vocabulary size:', len(vocabulary)\n",
    "full_vocabulary = dict(vocabulary)\n",
    "for it, k in full_vocabulary.iteritems():\n",
    "    if int(k) <= 2:\n",
    "        del vocabulary[it]\n",
    "\n",
    "print 'final vocabulary size:', len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð¡onvert text to vectors (zero-one vectors of presence), form train and test vector sets\n",
    "\n",
    "vocab_words = vocabulary.keys()\n",
    "\n",
    "train_vectors = []\n",
    "train_classes = []\n",
    "for item in train_set:\n",
    "    text = Text(item[0])\n",
    "    vector_x = np.zeros(len(vocab_words))\n",
    "    for word in text.words:\n",
    "        word_text = word.lower()\n",
    "        if word_text in vocab_words:\n",
    "            vector_x[vocab_words.index(word_text)] = 1\n",
    "\n",
    "    train_vectors.append(vector_x)\n",
    "    train_classes.append(item[1])\n",
    "\n",
    "\n",
    "test_vectors = []\n",
    "test_classes = []\n",
    "for item in test_set:\n",
    "    text = Text(item[0])\n",
    "    vector_x = np.zeros(len(vocab_words))\n",
    "    for word in text.words:\n",
    "        word_text = word.lower()\n",
    "        if word_text in vocab_words:\n",
    "            vector_x[vocab_words.index(word_text)] = 1\n",
    "\n",
    "    test_vectors.append(vector_x)\n",
    "    star = int(item[1])\n",
    "    test_classes.append(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit SVM model, train one-vs-rest classifiers for multi-class classification\n",
    "# use balanced class weights to handle significant bias (68% of data are 5*)\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "lin_clf = svm.LinearSVC(class_weight='balanced')\n",
    "lin_clf.fit(train_vectors, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  802 / 1074\n",
      "errors (correct -> classified : count)\n",
      "1 -> 4 : 5\n",
      "1 -> 5 : 9\n",
      "1 -> 2 : 6\n",
      "1 -> 3 : 2\n",
      "4 -> 3 : 7\n",
      "4 -> 2 : 4\n",
      "4 -> 1 : 3\n",
      "5 -> 3 : 15\n",
      "5 -> 4 : 74\n",
      "4 -> 5 : 77\n",
      "3 -> 2 : 3\n",
      "3 -> 4 : 10\n",
      "5 -> 1 : 4\n",
      "2 -> 5 : 7\n",
      "2 -> 4 : 3\n",
      "3 -> 1 : 2\n",
      "2 -> 1 : 4\n",
      "5 -> 2 : 13\n",
      "2 -> 3 : 6\n",
      "3 -> 5 : 18\n"
     ]
    }
   ],
   "source": [
    "# run classifier on test set\n",
    "\n",
    "prediced_classes = lin_clf.predict(test_vectors)\n",
    "common = 0\n",
    "errors_freqencies = {}\n",
    "errors = {}\n",
    "for i, clas in enumerate(test_classes):\n",
    "    if (clas == prediced_classes[i]):\n",
    "        common += 1\n",
    "    else:\n",
    "        key = str(clas) + ' -> ' + str(prediced_classes[i])\n",
    "        if not errors_freqencies.has_key(key):\n",
    "            errors_freqencies[key] = 0\n",
    "        errors_freqencies[key] += 1\n",
    "        \n",
    "        if not errors.has_key(key):\n",
    "            errors[key] = []\n",
    "        errors[key].append(test_set[i])\n",
    "        \n",
    "print 'correct: ', str(common), '/', str(len(test_set))\n",
    "print 'errors (correct -> classified : count)'\n",
    "for key, freq in errors_freqencies.iteritems():\n",
    "    print key, ':', freq \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to display wrong classification examples\n",
    "for key, e in errors.iteritems():\n",
    "    print key\n",
    "    for er in e:\n",
    "        print er[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
