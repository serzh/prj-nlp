{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "# Data: newsela.com geography texts, Robert Heinlein \"Stranger in the strange land\"\n",
    "with open('newsela_short_short', 'rb') as file:\n",
    "    lines = file.readlines()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train and test data\n",
    "# Read sequence of sentences, convert it to list of tokens.\n",
    "# When new sentence starts randomly decide if merge it with previous one.\n",
    "# \"Merge\" = Remove last token of previous sentence (.!?) and mark its \n",
    "# last word as end of sentence (1 in train_classes vector) \n",
    "\n",
    "import random\n",
    "\n",
    "train_tokens = []\n",
    "train_classes = []\n",
    "i = 0\n",
    "sent_count = 0\n",
    "for line in lines:\n",
    "    doc = nlp(line.decode('utf-8'))\n",
    "    sent_count += len(list(doc.sents))\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        r = random.random()\n",
    "        sent_end_removed = False\n",
    "        if (r < 0.6) and (i > 1):\n",
    "            i -= 1\n",
    "            train_classes[i - 1] = 1\n",
    "            sent_end_removed = True\n",
    "            \n",
    "        for word in doc:\n",
    "            if (word.pos_ == 'SPACE') or (word.pos_ == 'X'):\n",
    "                continue\n",
    "            if (sent_end_removed == True):\n",
    "                word_text = word.text\n",
    "                # there was an attempt to lowercase first word of some sentences. But it was useless.\n",
    "                train_tokens[i] = word_text\n",
    "                train_classes[i] = 0\n",
    "                sent_end_removed = False\n",
    "            else:\n",
    "                train_tokens.append(word.text)\n",
    "                train_classes.append(0)\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  7514\n",
      "Number of broken sentences:  4473\n",
      "Number of tokens:  582820\n",
      "Number of tokens marked as sentence end:  4473\n"
     ]
    }
   ],
   "source": [
    "print 'Number of sentences: ', sent_count\n",
    "print 'Number of broken sentences: ', sum(train_classes)\n",
    "print 'Number of tokens: ', len(train_tokens)\n",
    "print 'Number of tokens marked as sentence end: ', sum(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature set\n",
    "\n",
    "def extractFeatures(tokens, i):\n",
    "    features = dict()\n",
    "    \n",
    "    # Basic feature set\n",
    "    \n",
    "    features[\"word\"] = tokens[i].lower()\n",
    "    features[\"word-1\"] = tokens[i-1].lower() if i > 1 else \"NONE\"\n",
    "    features[\"word+1\"] = tokens[i+1].lower() if i < (len(tokens) - 1) else \"NONE\" \n",
    "    \n",
    "    features[\"left-bigram\"] = tokens[i-2].lower() + \"_\" + tokens[i-1].lower() \\\n",
    "        if i > 1 else \"NONE\"\n",
    "    features[\"right-bigram\"] = tokens[i+1].lower() + \"_\" + tokens[i+2].lower() \\\n",
    "        if i < (len(tokens) - 2) else \"NONE\"\n",
    "    \n",
    "    features[\"is_capitalized\"] = tokens[i].istitle() \n",
    "    features[\"is_capitalized-1\"] = tokens[i-1].istitle() if i > 0 else \"NONE\"\n",
    "    features[\"is_capitalized-2\"] = tokens[i-2].istitle() if i > 1 else \"NONE\"\n",
    "    features[\"is_capitalized+1\"] = tokens[i+1].istitle() if i < (len(tokens) - 1) else \"NONE\"\n",
    "    features[\"is_capitalized+2\"] = tokens[i+2].istitle() if i < (len(tokens) - 2) else \"NONE\"\n",
    "    \n",
    "    # Improvements\n",
    "    \n",
    "    ind = i\n",
    "    while (tokens[ind] not in [u'!', u'.', u'?']) and (ind >= 0):\n",
    "        ind -= 1\n",
    "    features[\"dist_to_last_punct\"] = i - ind   \n",
    "\n",
    "# No improvement, skip this feature  \n",
    "#     ind = i\n",
    "#     while (tokens[ind] not in [u'!', u'.', u'?']) and (ind < len(tokens)):\n",
    "#         ind += 1\n",
    "#     features[\"dist_to_next_punct\"] = ind - i      \n",
    "\n",
    "    features[\"left-3gram\"] = tokens[i-3].lower() + \"_\" + tokens[i-2].lower() + \"_\" + tokens[i-1].lower() \\\n",
    "        if i > 2 else \"NONE\"\n",
    "    features[\"right-3gram\"] = tokens[i+1].lower() + \"_\" + tokens[i+2].lower() + \"_\" + tokens[i+3].lower() \\\n",
    "        if i < (len(tokens) - 3) else \"NONE\"\n",
    "    features[\"is_capitalized+3\"] = tokens[i+3].istitle() if i < (len(tokens) - 3) else \"NONE\" \n",
    "    features[\"is_capitalized-3\"] = tokens[i-3].istitle() if i > 2 else \"NONE\"\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features, split train and test set\n",
    "# Downsample class of words that do not end sentence\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = [], [], [], []\n",
    "for i in range(len(train_tokens)):\n",
    "    r = random.random()\n",
    "    if (train_classes[i] == 0) and (r > 0.07): #downsampling\n",
    "        continue\n",
    "        \n",
    "    features = extractFeatures(train_tokens, i)\n",
    "\n",
    "    r = random.random()\n",
    "    if r < 0.8:\n",
    "        train_data.append(features)\n",
    "        train_labels.append(train_classes[i])\n",
    "    else:\n",
    "        test_data.append(features)\n",
    "        test_labels.append(train_classes[i])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36075 36075 3604\n",
      "9021 9021 869\n"
     ]
    }
   ],
   "source": [
    "print len(train_data), len(train_labels), sum(train_labels)\n",
    "print len(test_data), len(test_labels), sum(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bsaseline: Each uppercased word starts new sentence\n",
    "import numpy as np\n",
    "predicted_labels_base = np.zeros(len(test_labels))\n",
    "for i in range(len(test_labels) - 1):\n",
    "    if test_data[i][\"is_capitalized+1\"] == True:\n",
    "        predicted_labels_base[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.88      0.93      7933\n",
      "          1       0.46      0.92      0.62       900\n",
      "\n",
      "avg / total       0.94      0.88      0.90      8833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quality for baseline\n",
    "target_names = [u'0', u'1']\n",
    "print(classification_report(test_labels, predicted_labels_base, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features to vectors\n",
    "vec = DictVectorizer()\n",
    "vec.fit(train_data)\n",
    "x_train = vec.transform(train_data).toarray()\n",
    "x_test = vec.transform(test_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(x_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00      8152\n",
      "          1       0.95      0.96      0.95       869\n",
      "\n",
      "avg / total       0.99      0.99      0.99      9021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test model on generated test set\n",
    "predicted_labels = logreg.predict(x_test)\n",
    "target_names = [u'0', u'1']\n",
    "print(classification_report(test_labels, predicted_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Resuls for generated test set (results may be different because of random downsampling)\n",
    "# I have more data to reduce these differences, but I get MemoryError :(\n",
    "\n",
    "# #Basic feature set\n",
    "\n",
    "#         precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.99      0.99      0.99      8154\n",
    "#           1       0.94      0.96      0.95       923\n",
    "\n",
    "# avg / total       0.99      0.99      0.99      9077\n",
    "\n",
    "# # Add distance to last .!?\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       1.00      0.99      1.00      7933\n",
    "#           1       0.95      0.96      0.96       900\n",
    "\n",
    "# avg / total       0.99      0.99      0.99      8833\n",
    "\n",
    "# # Add 3-grams (no improvement here, but good results on given test set)\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.99      0.99      0.99      8010\n",
    "#           1       0.95      0.95      0.95       930\n",
    "\n",
    "# avg / total       0.99      0.99      0.99      8940\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "4697 4697 155\n"
     ]
    }
   ],
   "source": [
    "# Read given test data\n",
    "\n",
    "import json\n",
    "json_file='../../../tasks/07-language-as-sequence/run-on-test.json'\n",
    "json_data=open(json_file)\n",
    "data = json.load(json_data)\n",
    "json_data.close()\n",
    "\n",
    "print len(data)\n",
    "\n",
    "test_tokens_given = []\n",
    "test_labels_given = []\n",
    "for sentence in data:\n",
    "    for token in sentence:\n",
    "        test_tokens_given.append(token[0])\n",
    "        if token[1] == True:\n",
    "            test_labels_given.append(1) \n",
    "        else:\n",
    "            test_labels_given.append(0)\n",
    "print len(test_tokens_given), len(test_labels_given), sum(test_labels_given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4697\n"
     ]
    }
   ],
   "source": [
    "# Extract features from given tet data\n",
    "\n",
    "test_data_given = []\n",
    "for i in range(len(test_tokens_given)):\n",
    "    \n",
    "    features = extractFeatures(test_tokens_given, i)\n",
    "    test_data_given.append(features)\n",
    "\n",
    "print len(test_data_given)\n",
    "# vectorize given test data\n",
    "x_test_given = vec.transform(test_data_given).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      4542\n",
      "          1       0.67      0.36      0.47       155\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test model on given test set\n",
    "predicted_labels_given = logreg.predict(x_test_given)\n",
    "target_names = [u'0', u'1']\n",
    "print(classification_report(test_labels_given, predicted_labels_given, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Resuls for given test set (results may be different because of random downsampling)\n",
    "\n",
    "# # Basic feature set\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.98      0.99      0.98      4542\n",
    "#           1       0.58      0.39      0.47       155\n",
    "\n",
    "# avg / total       0.97      0.97      0.97      4697\n",
    "\n",
    "\n",
    "# # Add distance to last .!?\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.98      0.99      0.99      4542\n",
    "#           1       0.59      0.38      0.46       155\n",
    "\n",
    "# avg / total       0.97      0.97      0.97      4697\n",
    "\n",
    "# # Add 3-grams\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.98      0.99      0.99      4542\n",
    "#           1       0.64      0.41      0.50       155\n",
    "\n",
    "# avg / total       0.97      0.97      0.97      4697\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
