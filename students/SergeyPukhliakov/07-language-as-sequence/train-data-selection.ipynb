{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie_lines.txt') as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_line(line):\n",
    "    sep = ' +++$+++ '    \n",
    "    ind = line.rfind(sep) + len(sep)\n",
    "    return (line[ind:]).strip().replace(u'\\xa0', u' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for l in content:\n",
    "    l = strip_line(l)\n",
    "    if (len(l)<30) or '...' in l or ' . ' in l or '<u>' in l: continue\n",
    "    if '--' in l or '\\t\\t' in l : continue\n",
    "    sents = sent_tokenize(l)\n",
    "    if len(sents) > 4 : continue\n",
    "    result.append(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = random.sample(result, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tech_crunch = pd.read_csv('./techcrunch_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posts = tech_crunch['content'].sample(600).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_posts = []\n",
    "for p in posts:\n",
    "    p = str(p)\n",
    "    ind = p.find('.', 2000)\n",
    "    cropped_posts.append(p[:ind].replace(u'\\xa0', u' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_sents = [sent_tokenize(p.strip()) for p in cropped_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_short_span(sentences):\n",
    "    start_ind = -1;\n",
    "    for i in range(len(sentences)):\n",
    "        if len(sentences[i]) <= 40:\n",
    "            start_ind = i\n",
    "            break\n",
    "    if start_ind == -1 or start_ind == len(sentences) - 1:\n",
    "        return sentences[:1]\n",
    "    end_ind = -1\n",
    "    for i in range(start_ind, len(sentences)):\n",
    "        if len(sentences[i]) > 40:\n",
    "            end_ind = i - 1\n",
    "            break\n",
    "    if end_ind == -1 or end_ind == start_ind:\n",
    "        return sentences[start_ind:(start_ind + 2)]\n",
    "    return sentences[start_ind:end_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_results = [get_short_span(p) for p in posts_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Some text...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some\n",
      "text\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_first_cap(token):\n",
    "    if token.upper() == token:\n",
    "        return token\n",
    "    if random.randint(0,1) == 0:\n",
    "        return token[0].lower() + token[1:]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_seq(sents):\n",
    "    result = []\n",
    "    for i in range(len(sents)):\n",
    "        s = sents[i]        \n",
    "        doc = nlp(s)\n",
    "        tokens = [t.text for t in doc]\n",
    "        if len(sents) == 1:\n",
    "            result = [[t,False] for t in tokens]\n",
    "            break\n",
    "        if i == 0:\n",
    "            result += [[t,False] for t in tokens[:-2]]\n",
    "            result.append([tokens[-2],True])\n",
    "            continue\n",
    "        if i == len(sents) - 1:\n",
    "            result.append([random_first_cap(tokens[0]), False])\n",
    "            result += [[t,False] for t in tokens[1:]]\n",
    "            continue\n",
    "        else:\n",
    "            result.append([random_first_cap(tokens[0]), False])\n",
    "            result += [[t,False] for t in tokens[1:-2]]\n",
    "            result.append([tokens[-2],True])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Where', False],\n",
       " ['are', False],\n",
       " ['you', False],\n",
       " ['going', True],\n",
       " ['you', False],\n",
       " [\"'ll\", False],\n",
       " ['freeze', False],\n",
       " ['out', False],\n",
       " ['there', True],\n",
       " ['You', False],\n",
       " ['do', False],\n",
       " [\"n't\", False],\n",
       " ['even', False],\n",
       " ['have', False],\n",
       " ['a', False],\n",
       " ['coat', False],\n",
       " ['.', False]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "for sents in result+tech_results:\n",
    "    data.append(merge_seq(sents))\n",
    "with open('train_data.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
