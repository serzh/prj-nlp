{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = parse_data(\"train_data.json\")\n",
    "test = parse_data(\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punc = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(data):\n",
    "    predicted = []\n",
    "    true = []\n",
    "    for tokens in data:\n",
    "        for i in range(len(tokens)):\n",
    "            true.append(int(tokens[i][1]))\n",
    "            if i == 0 or tokens[i] in punc :\n",
    "                predicted.append(0)\n",
    "                continue\n",
    "            if i < len(tokens) - 1 and tokens[i + 1] in punc:\n",
    "                predicted.append(0)\n",
    "                continue\n",
    "            if  tokens[i][0].isupper():\n",
    "                predicted.append(1)\n",
    "                continue\n",
    "            predicted.append(0)\n",
    "    return true, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = baseline(train)\n",
    "x1,y1 = baseline(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98     31390\n",
      "          1       0.02      0.01      0.02       799\n",
      "\n",
      "avg / total       0.95      0.96      0.95     32189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(x+x1,y+y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proces_tokens(tokens):\n",
    "    doc = spacy.tokens.doc.Doc(nlp.vocab, words=tokens)\n",
    "    processed = nlp.tagger(doc)\n",
    "    return nlp.parser(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where where ADV WRB\n",
      "are be VERB VBP\n",
      "you -PRON- PRON PRP\n",
      "going go VERB VBG\n",
      "you -PRON- PRON PRP\n",
      "'ll 'll VERB MD\n",
      "freeze freeze VERB VB\n",
      "out out PART RP\n",
      "there there ADV RB\n",
      "you -PRON- PRON PRP\n",
      "do do VERB VBP\n",
      "n't n't ADV RB\n",
      "even even ADV RB\n",
      "have have VERB VB\n",
      "a a DET DT\n",
      "coat coat NOUN NN\n",
      ". . PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for item in proces_tokens([x[0] for x in train[0]]):    \n",
    "    print(item.text, item.lemma_, item.pos_, item.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_punc = ['!','...',',','.',':',';','?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(doc, i):    \n",
    "        word = doc[i]\n",
    "        features = {\n",
    "            'word' : word.text,\n",
    "            'lemma': word.lemma_,\n",
    "            'postag': word.pos_,\n",
    "            'tag': word.tag_,\n",
    "            'word[-3:]': word.text[-3:],\n",
    "            'word[-2:]': word.text[-2:],\n",
    "            'word.isupper()': word.text.isupper(),\n",
    "            'word.istitle()': word.text.istitle(),\n",
    "            'word.isdigit()': word.text.isdigit(),\n",
    "        }\n",
    "        if i > 0:\n",
    "            word1 = doc[i-1]\n",
    "            features.update({\n",
    "                '-1:word' : word1.text,\n",
    "                '-1:lemma': word1.lemma_,\n",
    "                '-1:postag': word1.pos_,\n",
    "                '-1:tag': word1.tag_,\n",
    "                '-1:word.istitle()': word1.text.istitle(),\n",
    "                '-1:word.isupper()': word1.text.isupper(),\n",
    "                '-1:word.isdigit()': word1.text.isdigit(),\n",
    "                '-1:ngram': word1.lemma_ + '_' + word.lemma_\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = True\n",
    "        if i > 1:\n",
    "            word2 = doc[i-2]\n",
    "            features.update({\n",
    "                '-2:word' : word2.text,\n",
    "                '-2:lemma': word2.lemma_,\n",
    "                '-2:postag': word2.pos_,\n",
    "                '-2:tag': word2.tag_,\n",
    "                '-2:word.istitle()': word2.text.istitle(),\n",
    "                '-2:word.isupper()': word2.text.isupper(),\n",
    "                '-2:word.isdigit()': word2.text.isdigit(),\n",
    "                '-2:ngram': word2.lemma_ + '_' + word1.lemma_ + '_' + word.lemma_\n",
    "            })\n",
    "        if i < len(doc)-1:\n",
    "            word1 = doc[i+1]\n",
    "            features.update({\n",
    "                '+1:word' : word1.text,\n",
    "                '+1:lemma': word1.lemma_,\n",
    "                '+1:postag': word1.pos_,\n",
    "                '+1:tag': word1.tag_,\n",
    "                '+1:word.istitle()': word1.text.istitle(),\n",
    "                '+1:word.isupper()': word1.text.isupper(),\n",
    "                '+1:word.isdigit()': word1.text.isdigit(),\n",
    "                '+1:ngram': word.lemma_ + '_' + word1.lemma_\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True\n",
    "        if i < len(doc)-2:\n",
    "            word2 = doc[i+2]\n",
    "            features.update({\n",
    "                '+2:word' : word2.text,\n",
    "                '+2:lemma': word2.lemma_,\n",
    "                '+2:postag': word2.pos_,\n",
    "                '+2:tag': word2.tag_,\n",
    "                '+2:word.istitle()': word2.text.istitle(),\n",
    "                '+2:word.isupper()': word2.text.isupper(),\n",
    "                '+2:word.isdigit()': word2.text.isdigit(),\n",
    "                '+2:ngram': word2.lemma_ + '_' + word1.lemma_ + '_' + word.lemma_\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+1:lemma': 'be',\n",
       " '+1:ngram': 'where_be',\n",
       " '+1:postag': 'VERB',\n",
       " '+1:tag': 'VBP',\n",
       " '+1:word': 'are',\n",
       " '+1:word.isdigit()': False,\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+2:lemma': '-PRON-',\n",
       " '+2:ngram': '-PRON-_be_where',\n",
       " '+2:postag': 'PRON',\n",
       " '+2:tag': 'PRP',\n",
       " '+2:word': 'you',\n",
       " '+2:word.isdigit()': False,\n",
       " '+2:word.istitle()': False,\n",
       " '+2:word.isupper()': False,\n",
       " 'BOS': True,\n",
       " 'lemma': 'where',\n",
       " 'postag': 'ADV',\n",
       " 'tag': 'WRB',\n",
       " 'word': 'Where',\n",
       " 'word.isdigit()': False,\n",
       " 'word.istitle()': True,\n",
       " 'word.isupper()': False,\n",
       " 'word[-2:]': 're',\n",
       " 'word[-3:]': 'ere'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [x[0] for x in train[0]]   \n",
    "doc = proces_tokens(tokens)\n",
    "word2features(doc,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are you going you 'll freeze out there you do n't even have a coat . \n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):    \n",
    "    tokens = [x[0] for x in sent]   \n",
    "    doc = proces_tokens(tokens)\n",
    "    return [word2features(doc,i) for i in range(len(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "    return [x[1] for x in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2features(data):\n",
    "    results = []\n",
    "    for s in data:\n",
    "        results += (sent2features(s))\n",
    "    return results\n",
    "\n",
    "def data2labels(data):\n",
    "    results = []\n",
    "    for s in data:\n",
    "        results+=(sent2labels(s))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(data2features(train))\n",
    "y_train = data2labels(train)\n",
    "\n",
    "X_test = vec.transform(data2features(test))\n",
    "y_test = data2labels(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train = logistic.predict(X_train)\n",
    "predicted_test = logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00     26848\n",
      "       True       1.00      0.92      0.96       644\n",
      "\n",
      "avg / total       1.00      1.00      1.00     27492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_train, predicted_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      1.00      0.99      4542\n",
      "       True       0.80      0.38      0.52       155\n",
      "\n",
      "avg / total       0.97      0.98      0.97      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test, predicted_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
