{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building word-vector representations with word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore word2vec using a very small text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our text\n",
    "text = [\"Homer lives in a modest home in the suburbs .\",\n",
    "        \"Homer lives with his wife Marge and his children .\",\n",
    "        \"Homer loves beer .\",\n",
    "        \"Homer works a factory job .\",\n",
    "        \"Homer loves his wife and his children .\",\n",
    "        \"Homer loves Marge and children .\",\n",
    "        \"Homer loves Marge .\",\n",
    "        \"Homer drinks beer .\",\n",
    "        \"Homer loves to grill .\"]\n",
    "sentences = [sentence.split() for sentence in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model:\n",
    "* *size* - size of the vector (default = 100)\n",
    "* *window* - size of the context (default = 5)\n",
    "* *min_count* - the minimum word frequency (default = 5)\n",
    "* *iter* - number of iterations (default = 5)\n",
    "* *sample* â€“ threshold for configuring which higher-frequency words are randomly downsampled (default = 0.001, useful range is (0, 1e-5))\n",
    "* *sg* - algorithm; if 1, skip-gram is employed; otherwise, CBOW (default = 0)\n",
    "\n",
    "See more at https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = gensim.models.Word2Vec(size=10, window=3, min_count=1, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 23 tokens:\n",
      "Homer, lives, in, a, modest, home, the, suburbs, ., with, his, wife, Marge, and, children, loves, beer, works, factory, job, drinks, to, grill\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "simple_model.build_vocab(sentences)\n",
    "print(\"The vocabulary contains {} tokens:\".format(len(simple_model.wv.vocab)))\n",
    "print(\", \".join(simple_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(919, 5700)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "simple_model.train(sentences, total_examples=simple_model.corpus_count, epochs=simple_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01457807, -0.02688933, -0.00825159, -0.03283849, -0.03954028,\n",
       "       -0.04387942,  0.03364788,  0.00306001,  0.04000552,  0.02014847],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.wv[\"Homer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('his', 0.6855198740959167),\n",
       " ('job', 0.6089805960655212),\n",
       " ('Homer', 0.4203595817089081),\n",
       " ('works', 0.4064020812511444),\n",
       " ('Marge', 0.35335204005241394)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.wv.most_similar(positive=[\"beer\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('children', 0.7555125951766968),\n",
       " ('loves', 0.6118846535682678),\n",
       " ('to', 0.6085261106491089),\n",
       " ('wife', 0.4653707444667816),\n",
       " ('factory', 0.44557395577430725)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.wv.most_similar(positive=[\"Marge\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wife', 0.6681273579597473),\n",
       " ('children', 0.610073447227478),\n",
       " ('loves', 0.5734997391700745),\n",
       " ('home', 0.4732385575771332),\n",
       " ('.', 0.41256240010261536)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.wv.most_similar(positive=[\"Marge\"], negative=[\"job\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loves', 0.622331976890564),\n",
       " ('to', 0.5720157027244568),\n",
       " ('modest', 0.5595673322677612),\n",
       " ('factory', 0.5169535875320435),\n",
       " ('.', 0.5089230537414551)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.wv.most_similar(positive=[\"Marge\", \"children\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beer'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.wv.doesnt_match([\"beer\", \"wife\", \"Marge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4653707304517156"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.wv.similarity(\"wife\", \"Marge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3174228330138831"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.wv.similarity(\"loves\", \"lives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'Homer', 9)\n",
      "(b'.', 9)\n",
      "(b'loves', 5)\n",
      "(b'Homer_loves', 5)\n",
      "(b'his', 4)\n",
      "(b'Marge', 3)\n",
      "(b'and', 3)\n",
      "(b'children', 3)\n",
      "(b'children_.', 3)\n",
      "(b'lives', 2)\n",
      "(b'Homer_lives', 2)\n",
      "(b'in', 2)\n",
      "(b'a', 2)\n",
      "(b'wife', 2)\n",
      "(b'his_wife', 2)\n",
      "(b'Marge_and', 2)\n",
      "(b'and_his', 2)\n",
      "(b'his_children', 2)\n",
      "(b'beer', 2)\n",
      "(b'beer_.', 2)\n",
      "(b'loves_Marge', 2)\n"
     ]
    }
   ],
   "source": [
    "# We can try detecting multi-word expressions too\n",
    "bigram_transformer = gensim.models.Phrases(sentences)\n",
    "for i in sorted(bigram_transformer.vocab.items(), key=lambda x: x[1], reverse=True):\n",
    "    if i[1] > 1:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more at https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.html#gensim.models.Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore word2vec on a larger text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyText(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with open(self.filename, \"r\") as f:\n",
    "            text = nlp(f.read())\n",
    "            for sentence in text.sents:\n",
    "                words = [\"<S>\"]\n",
    "                for word in sentence:\n",
    "                    if word.pos_ != \"SPACE\":\n",
    "                        lemma = word.lemma_\n",
    "                        if lemma == \"-PRON-\":\n",
    "                            lemma = word.text.lower()\n",
    "                        pos = word.pos_\n",
    "                        if pos == \"PROPN\":\n",
    "                            pos = \"NOUN\"\n",
    "                        words.append(lemma + \"_\" + pos)\n",
    "                words.append(\"</S>\")\n",
    "                if len(words) > 2:\n",
    "                    yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = MyText(\"alice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(alice, size=75, min_count=10, iter=30)\n",
    "# model = gensim.models.Word2Vec(size=75, min_count=10, iter=30)\n",
    "# model.build_vocab(alice)\n",
    "# model.train(alice, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 668 tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"The vocabulary contains {} tokens.\".format(len(model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11413185,  0.5949863 ,  0.49487814,  0.5742768 , -1.0822297 ,\n",
       "       -0.02153508,  0.48581457, -0.71189874,  2.2513523 , -0.0772558 ,\n",
       "       -0.08615175,  0.30193847,  0.3162758 , -2.6292167 , -0.19756871,\n",
       "       -0.8341728 , -0.66789365, -0.23562734,  0.04346377,  1.25502   ,\n",
       "       -0.96912   , -0.8927309 , -0.0882756 , -1.0984857 , -0.6692846 ,\n",
       "       -0.5493002 ,  1.1952738 , -0.35345873,  1.7447567 ,  0.34245366,\n",
       "       -0.4555024 ,  0.30958712, -0.5876762 ,  0.46231976,  0.5932624 ,\n",
       "        0.7631199 ,  1.523663  , -0.39136896, -0.14941442, -0.7581803 ,\n",
       "        1.2540447 ,  0.71262   ,  0.02885384,  2.0752213 , -0.3627157 ,\n",
       "       -0.6159076 , -1.1462373 ,  0.84264815,  2.0659876 , -0.49066183,\n",
       "        0.22708146,  1.329406  ,  0.12656336,  1.4548057 , -0.87437177,\n",
       "        0.27867097,  1.8750645 ,  1.0129884 , -0.32941452, -0.5606945 ,\n",
       "        0.4193016 , -0.8982725 , -0.35410503, -1.5410829 , -0.38870347,\n",
       "       -0.6902184 , -1.4409807 , -0.5451168 , -1.8176272 , -0.7703929 ,\n",
       "        0.34193698, -0.73632604, -0.96864164, -0.81636256, -1.5545594 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"queen_NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king_NOUN', 0.8195973634719849),\n",
       " ('rabbit_NOUN', 0.7940890789031982),\n",
       " ('knight_NOUN', 0.7650502920150757),\n",
       " ('rose_NOUN', 0.6606079936027527),\n",
       " ('sheep_NOUN', 0.6368069648742676),\n",
       " ('gryphon_NOUN', 0.6276119351387024),\n",
       " ('lion_NOUN', 0.6246041655540466),\n",
       " ('unicorn_NOUN', 0.6139159202575684),\n",
       " ('hatter_NOUN', 0.5785979628562927),\n",
       " ('duchess_NOUN', 0.5742928981781006),\n",
       " ('dormouse_NOUN', 0.5626205801963806),\n",
       " ('kitten_NOUN', 0.5557862520217896),\n",
       " ('caterpillar_NOUN', 0.5460533499717712),\n",
       " ('gnat_NOUN', 0.5233383178710938),\n",
       " ('dodo_NOUN', 0.5172272324562073)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"queen_NOUN\"], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alice_NOUN'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match([\"queen_NOUN\", \"king_NOUN\", \"duchess_NOUN\", \"alice_NOUN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go_VERB'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match([\"say_VERB\", \"tell_VERB\", \"reply_VERB\", \"go_VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hair_NOUN', 0.8607650399208069),\n",
       " ('shoulder_NOUN', 0.810667097568512),\n",
       " ('neck_NOUN', 0.8089621067047119),\n",
       " ('hand_NOUN', 0.8060130476951599),\n",
       " ('tail_NOUN', 0.8041206002235413),\n",
       " ('face_NOUN', 0.8006356358528137),\n",
       " ('mouth_NOUN', 0.7944957613945007),\n",
       " ('own_ADJ', 0.7834600210189819),\n",
       " ('paw_NOUN', 0.7756462097167969),\n",
       " ('ear_NOUN', 0.7673256397247314)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"head_NOUN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crab_NOUN', 0.8377188444137573),\n",
       " ('carpenter_NOUN', 0.7569010257720947),\n",
       " ('present_NOUN', 0.7338333129882812),\n",
       " ('old_ADJ', 0.7285784482955933),\n",
       " ('seven_NUM', 0.7272090315818787),\n",
       " ('soup_NOUN', 0.7241595387458801),\n",
       " ('walrus_NOUN', 0.7061634063720703),\n",
       " ('man_NOUN', 0.7022372484207153),\n",
       " ('dinner_NOUN', 0.7014530897140503),\n",
       " ('night_NOUN', 0.6954205632209778)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"oyster_NOUN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she_PRON', 0.6610369086265564),\n",
       " ('afterwards_ADV', 0.6138221025466919),\n",
       " ('herself_PRON', 0.5516687631607056),\n",
       " ('puzzled_ADJ', 0.546956479549408),\n",
       " ('much_ADV', 0.5182530879974365),\n",
       " ('question_NOUN', 0.505143404006958),\n",
       " ('timidly_ADV', 0.49763089418411255),\n",
       " ('politely_ADV', 0.4887434244155884),\n",
       " ('dormouse_NOUN', 0.4806196391582489),\n",
       " ('knight_NOUN', 0.46858036518096924)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"alice_NOUN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('put_VERB', 0.6499727368354797),\n",
       " ('come_VERB', 0.6376410722732544),\n",
       " ('keep_VERB', 0.6284362077713013),\n",
       " ('walk_VERB', 0.6099972724914551),\n",
       " ('move_VERB', 0.5888571739196777),\n",
       " ('send_VERB', 0.5882790684700012),\n",
       " ('get_VERB', 0.5447977185249329),\n",
       " ('turn_VERB', 0.5364412069320679),\n",
       " ('pass_VERB', 0.5315772891044617),\n",
       " ('settle_VERB', 0.520257294178009)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"go_VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('remark_VERB', 0.7727510929107666),\n",
       " ('reply_VERB', 0.7562205195426941),\n",
       " ('add_VERB', 0.719392716884613),\n",
       " ('repeat_VERB', 0.6001617908477783),\n",
       " ('interrupt_VERB', 0.5917848944664001),\n",
       " ('answer_VERB', 0.5445111989974976),\n",
       " ('whisper_VERB', 0.5423060059547424),\n",
       " ('think_VERB', 0.5071455240249634),\n",
       " ('explain_VERB', 0.5045211911201477),\n",
       " ('exclaim_VERB', 0.5042113065719604)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"say_VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('white_NOUN', 0.823891282081604),\n",
       " ('scream_VERB', 0.618787407875061),\n",
       " ('remark_VERB', 0.6065846085548401),\n",
       " ('angrily_ADV', 0.6012203097343445),\n",
       " ('interrupt_VERB', 0.5686538219451904),\n",
       " ('unicorn_NOUN', 0.5209848880767822),\n",
       " ('hatter_NOUN', 0.5172007083892822),\n",
       " ('who_NOUN', 0.5063095092773438),\n",
       " ('hastily_ADV', 0.5001121759414673),\n",
       " ('rose_NOUN', 0.4956131875514984)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"red_NOUN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sudden_ADJ', 0.6625736951828003),\n",
       " ('long_ADJ', 0.6109942197799683),\n",
       " ('tree_NOUN', 0.6106938123703003),\n",
       " ('yard_NOUN', 0.5962684154510498),\n",
       " ('bottle_NOUN', 0.5749298334121704),\n",
       " ('pair_NOUN', 0.5558291673660278),\n",
       " ('quite_ADJ', 0.5445135831832886),\n",
       " ('large_ADJ', 0.530839741230011),\n",
       " ('voice_NOUN', 0.5159767866134644),\n",
       " ('bit_NOUN', 0.5130465030670166)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"little_ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('large_ADJ', 0.7293568849563599),\n",
       " ('grow_VERB', 0.6957991719245911),\n",
       " ('quite_ADV', 0.6751863956451416),\n",
       " ('few_ADJ', 0.6600274443626404),\n",
       " ('pleased_ADJ', 0.6586658358573914),\n",
       " ('idea_NOUN', 0.6539437770843506),\n",
       " ('about_ADV', 0.6469759345054626),\n",
       " ('listen_VERB', 0.6413574814796448),\n",
       " ('flower_NOUN', 0.6317957043647766),\n",
       " ('fan_NOUN', 0.6213661432266235)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"small_ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kitten_NOUN', 0.39184796810150146),\n",
       " ('soldier_NOUN', 0.38951411843299866),\n",
       " ('them_PRON', 0.3883768916130066),\n",
       " ('tear_NOUN', 0.3719978928565979),\n",
       " ('king_NOUN', 0.3448095917701721),\n",
       " ('breath_NOUN', 0.34395867586135864),\n",
       " ('one_NOUN', 0.34117400646209717),\n",
       " ('rabbit_NOUN', 0.33938711881637573),\n",
       " ('fire_NOUN', 0.33344006538391113),\n",
       " ('neck_NOUN', 0.326082706451416)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"queen_NOUN\"], negative=[\"red_NOUN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try some LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['picture', 'conversation', 'use', 'book', 'picture']\n",
      "3570\n",
      "['face', 'cat', 'quarter', 'hour', 'hand']\n",
      "4059\n",
      "['bounding', 'crag', 'agility', 'fawn', 'companion']\n",
      "2200\n"
     ]
    }
   ],
   "source": [
    "def get_content_words(text):\n",
    "    words = []\n",
    "    text = nlp(text)\n",
    "    for sentence in text.sents:\n",
    "        for word in sentence:\n",
    "            if word.tag_ in [\"NN\", \"NNS\"]:\n",
    "                words.append(word.lemma_)\n",
    "    return words\n",
    "\n",
    "with open(\"alice-1.txt\", \"r\") as f:\n",
    "    alice_wonderland = get_content_words(f.read())\n",
    "with open(\"alice-2.txt\", \"r\") as f:\n",
    "    alice_looking_glass = get_content_words(f.read())\n",
    "with open(\"tangled_tale.txt\", \"r\") as f:\n",
    "    tangled_tale = get_content_words(f.read())\n",
    "\n",
    "print(alice_wonderland[10:15])\n",
    "print(len(alice_wonderland))\n",
    "print(alice_looking_glass[10:15])\n",
    "print(len(alice_looking_glass))\n",
    "print(tangled_tale[10:15])\n",
    "print(len(tangled_tale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary([alice_wonderland, alice_looking_glass, tangled_tale])\n",
    "corpus = [dictionary.doc2bow(alice_wonderland),\n",
    "          dictionary.doc2bow(alice_looking_glass),\n",
    "          dictionary.doc2bow(tangled_tale)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 3), (11, 5), (12, 1), (13, 1), (14, 1), (15, 4), (16, 1), (17, 16), (18, 2), (19, 2)]\n",
      "856 946 829\n"
     ]
    }
   ],
   "source": [
    "# Corpus is a list of vectors for documents. The tuples are (term ID, term frequency) pairs.\n",
    "print(corpus[0][:20])\n",
    "print(len(corpus[0]), len(corpus[1]), len(corpus[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.013*\"man\" + 0.008*\"day\" + 0.008*\"lady\" + 0.008*\"room\" + 0.008*\"aunt\"'),\n",
       " (1,\n",
       "  '0.019*\"thing\" + 0.019*\"time\" + 0.015*\"way\" + 0.015*\"head\" + 0.012*\"hand\"')]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4.6038855e-05), (1, 0.00070622)]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_term_topics(\"rose\", minimum_probability=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4.5719455e-05), (1, 0.00023752295)]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_term_topics(\"bush\", minimum_probability=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0042249565), (1, 1.679964e-05)]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_term_topics(\"liquid\", minimum_probability=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.006183312), (1, 0.018578794)]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_term_topics(\"time\", minimum_probability=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_words = get_content_words(\"\"\"\n",
    "'Get to your places!' shouted the Queen in a voice of thunder, and people began running about in all directions, tumbling up against each other; however, they got settled down in a minute or two, and the game began. Alice thought she had never seen such a curious croquet-ground in her life; it was all ridges and furrows; the balls were live hedgehogs, the mallets live flamingoes, and the soldiers had to double themselves up and to stand on their hands and feet, to make the arches.\n",
    "The chief difficulty Alice found at first was in managing her flamingo: she succeeded in getting its body tucked away, comfortably enough, under her arm, with its legs hanging down, but generally, just as she had got its neck nicely straightened out, and was going to give the hedgehog a blow with its head, it WOULD twist itself round and look up in her face, with such a puzzled expression that she could not help bursting out laughing: and when she had got its head down, and was going to begin again, it was very provoking to find that the hedgehog had unrolled itself, and was in the act of crawling away: besides all this, there was generally a ridge or furrow in the way wherever she wanted to send the hedgehog to, and, as the doubled-up soldiers were always getting up and walking off to other parts of the ground, Alice soon came to the conclusion that it was a very difficult game indeed.\n",
    "The players all played at once without waiting for turns, quarrelling all the while, and fighting for the hedgehogs; and in a very short time the Queen was in a furious passion, and went stamping about, and shouting 'Off with his head!' or 'Off with her head!' about once in a minute.\"\"\")\n",
    "\n",
    "tangled_tale_words = get_content_words(\"\"\"\n",
    "When a solid is immersed in a liquid, it is well known that it displaces a portion of the liquid equal to itself in bulk, and that the level of the liquid rises just so much as it would rise if a quantity of liquid had been added to it, equal in bulk to the solid. Lardner says, precisely the same process occurs when a solid is partially immersed: the quantity of liquid displaced, in this case, equalling the portion of the solid which is immersed, and the rise of the level being in proportion.\n",
    "Suppose a solid held above the surface of a liquid and partially immersed: a portion of the liquid is displaced, and the level of the liquid rises. But, by this rise of level, a little bit more of the solid is of course immersed, and so there is a new displacement of a second portion of the liquid, and a consequent rise of level. Again, this second rise of level causes a yet further immersion, and by consequence another displacement of liquid and another rise. It is self-evident that this process must continue till the entire solid is immersed, and that the liquid will then begin to immerse whatever holds the solid, which, being connected with it, must for the time be considered a part of it. If you hold a stick, six feet long, with its end in a tumbler of water, and wait long enough, you must eventually be immersed. The question as to the source from which the water is supplied - which belongs to a high branch of mathematics, and is therefore beyond our present scope - does not apply to the sea. Let us therefore take the familiar instance of a man standing at the edge of the sea, at ebb-tide, with a solid in his hand, which he partially immerses: he remains steadfast and unmoved, and we all know that he must be drowned. The multitudes who daily perish in this manner to attest a philosophical truth, and whose bodies the unreasoning wave casts sullenly upon our thankless shores, have a truer claim to be called the martyrs of science than a Galileo or a Kepler. To use Kossuth's eloquent phrase, they are the unnamed demigods of the nineteenth century.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['place', 'queen', 'voice', 'thunder', 'people', 'direction', 'minute', 'game', 'croquet', 'ground']\n",
      "54\n",
      "['solid', 'liquid', 'portion', 'liquid', 'bulk', 'level', 'liquid', 'quantity', 'liquid', 'bulk']\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "print(alice_words[0:10])\n",
    "print(len(alice_words))\n",
    "print(tangled_tale_words[0:10])\n",
    "print(len(tangled_tale_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary([alice_words, tangled_tale_words])\n",
    "corpus = [dictionary.doc2bow(alice_words), dictionary.doc2bow(tangled_tale_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.058*\"hedgehog\" + 0.047*\"head\" + 0.026*\"ground\" + 0.026*\"flamingo\" + 0.026*\"furrow\"'),\n",
       " (1,\n",
       "  '0.093*\"liquid\" + 0.053*\"level\" + 0.053*\"rise\" + 0.044*\"solid\" + 0.036*\"portion\"')]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8322172), (1, 0.16778275)]\n",
      "[(0, 0.251143), (1, 0.748857)]\n"
     ]
    }
   ],
   "source": [
    "new_text = \"I like hedgehogs because they are so cute! Little hedgehogs live in the forest.\"\n",
    "print(lda[dictionary.doc2bow(get_content_words(new_text))])\n",
    "\n",
    "new_text = \"I like doing chemistry because I get to work with liquids.\"\n",
    "print(lda[dictionary.doc2bow(get_content_words(new_text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.01092342\n",
      "1 0.008426889\n"
     ]
    }
   ],
   "source": [
    "for index in range(lda.num_topics):\n",
    "    print(index, lda.expElogbeta[index][dictionary.token2id[\"time\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.052629057\n",
      "1 0.0012433603\n"
     ]
    }
   ],
   "source": [
    "for index in range(lda.num_topics):\n",
    "    print(index, lda.expElogbeta[index][dictionary.token2id[\"hedgehog\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def color_words(model, doc):\n",
    "\n",
    "    # make into bag of words\n",
    "    doc = model.id2word.doc2bow(doc)\n",
    "    # get word_topics\n",
    "    doc_topics, word_topics, phi_values = model.get_document_topics(doc, per_word_topics=True)\n",
    "\n",
    "    # color-topic matching\n",
    "    topic_colors = { 1:'red', 0:'blue'}\n",
    "    \n",
    "    # set up fig to plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    # a sort of hack to make sure the words are well spaced out.\n",
    "    word_pos = 1/len(doc)\n",
    "    \n",
    "    # use matplotlib to plot words\n",
    "    for word, topics in word_topics:\n",
    "        ax.text(word_pos, 0.8, model.id2word[word],\n",
    "                horizontalalignment='center',\n",
    "                verticalalignment='center',\n",
    "                fontsize=20, color=topic_colors[topics[0]],  # choose just the most likely topic\n",
    "                transform=ax.transAxes)\n",
    "        word_pos += 0.2 # to move the word for the next iter\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_text = \"Flamingoes play with hedgehogs. Their game is reaching a new level. They are good players.\"\n",
    "bow_water = get_content_words(new_text)\n",
    "color_words(lda, bow_water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.104*\"liquid\" + 0.058*\"level\" + 0.058*\"rise\" + 0.049*\"solid\" + 0.040*\"portion\" + 0.021*\"quantity\" + 0.021*\"bulk\" + 0.021*\"water\" + 0.021*\"displacement\" + 0.021*\"process\"'),\n",
       " (1,\n",
       "  '0.013*\"liquid\" + 0.013*\"rise\" + 0.013*\"solid\" + 0.013*\"level\" + 0.012*\"portion\" + 0.012*\"process\" + 0.012*\"sea\" + 0.012*\"displacement\" + 0.012*\"water\" + 0.012*\"quantity\"'),\n",
       " (2,\n",
       "  '0.066*\"hedgehog\" + 0.054*\"head\" + 0.029*\"minute\" + 0.029*\"flamingo\" + 0.029*\"soldier\" + 0.029*\"furrow\" + 0.029*\"ridge\" + 0.029*\"queen\" + 0.029*\"game\" + 0.029*\"ground\"')]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
